{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header>\n",
    "\n",
    "<div class=\"dc-edge-to-edge@sm dc-edge-to-edge--align-center\">\n",
    "\n",
    "<div class=\"dc-edge-to-edge__item\">\n",
    "\n",
    "<div class=\"dc-chapter__title-wrapper\">\n",
    "\n",
    "#### Regression\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"dc-edge-to-edge__item dc-chapter__progress\">\n",
    "\n",
    "<div class=\"course-outline__progress\"><span class=\"course-outline__percentage\">8%</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</header>\n",
    "\n",
    "<div class=\"modal--chapter-text\">\n",
    "\n",
    "In the previous chapter, you made use of image and political datasets to predict binary as well as multiclass outcomes. But what if your problem requires a continuous outcome? Regression, which is the focus of this chapter, is best suited to solving such problems. You will learn about fundamental concepts in regression and apply them to predict the life expectancy in a given country using Gapminder data.\n",
    "\n",
    "</div>\n",
    "\n",
    "•\tIntroduction to regression\n",
    "50 XP\n",
    "\n",
    "•\tWhich of the following is a regression problem?\n",
    "50 XP\n",
    "\n",
    "•\tImporting data for supervised learning\n",
    "100 XP\n",
    "\n",
    "•\tExploring the Gapminder data\n",
    "50 XP\n",
    "\n",
    "•\tThe basics of linear regression\n",
    "50 XP\n",
    "\n",
    "•\tFit & predict for regression\n",
    "100 XP\n",
    "\n",
    "•\tTrain/test split for regression\n",
    "100 XP\n",
    "\n",
    "•\tCross-validation\n",
    "50 XP\n",
    "\n",
    "•\t5-fold cross-validation\n",
    "100 XP\n",
    "\n",
    "•\tK-Fold CV comparison\n",
    "100 XP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which of the following is a regression problem?\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div>\n",
    "\n",
    "Andy introduced regression to you using the Boston housing dataset. But regression models can be used in a variety of contexts to solve a variety of different problems.\n",
    "\n",
    "Given below are four example applications of machine learning. Your job is to pick the one that is _best_ framed as a **regression** problem.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"exercise-area \">\n",
    "\n",
    "<div class=\"dc-u-fx dc-u-fx-dc dc-u-fx-aic dc-u-h-100pc dc-u-m-auto dc-u-of-auto\" style=\"max-width: 900px;\">\n",
    "\n",
    "<section class=\"dc-u-bgc-white dc-u-brad-all dc-u-b dc-u-bc-grey-lighter dc-u-m-auto dc-u-w-100pc\">\n",
    "\n",
    "<div class=\"dc-u-bgc-geyser dc-u-fx dc-u-fx-jcsb dc-u-fx-aic\" style=\"min-height: 36px;\">\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "<div class=\"dc-tag dc-tag--hue dc-u-color-grey-dark dc-u-bgc-white dc-u-mr-8\">50 XP</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"dc-u-p-16\">\n",
    "\n",
    "##### Possible Answers\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge__item\"><label class=\"dc-input-radio dc-u-cursor-pointer\" for=\"0\"><input class=\"dc-input-radio__input\" type=\"radio\" id=\"0\" name=\"multiple-choice-options-55929\" value=\"0\" checked=\"\"><span class=\"dc-input-radio__indicator\"></span><span class=\"dc-input-radio__text\">\n",
    "\n",
    "    <div class=\"\">\n",
    "\n",
    "    <div>An e-commerce company using labeled customer data to predict whether or not a customer will purchase a particular item.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </span></label></div>\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item dc-u-fx-aic dc-u-ifx\">press<label for=\"0\" class=\"dc-u-cursor-pointer dc-u-bgc-primary-dark dc-u-ml-8 dc-u-brad-all dc-u-bs-lg dc-u-wh-24 dc-u-color-white dc-u-d-ib dc-u-ta-center\">1</label></div>\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge__item\"><label class=\"dc-input-radio dc-u-cursor-pointer\" for=\"1\"><input class=\"dc-input-radio__input\" type=\"radio\" id=\"1\" name=\"multiple-choice-options-55929\" value=\"1\"><span class=\"dc-input-radio__indicator\"></span><span class=\"dc-input-radio__text\">\n",
    "\n",
    "    <div class=\"\">\n",
    "\n",
    "    <div>A healthcare company using data about cancer tumors (such as their geometric measurements) to predict whether a new tumor is benign or malignant.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </span></label></div>\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item dc-u-fx-aic dc-u-ifx\">press<label for=\"1\" class=\"dc-u-cursor-pointer dc-u-bgc-primary-dark dc-u-ml-8 dc-u-brad-all dc-u-bs-lg dc-u-wh-24 dc-u-color-white dc-u-d-ib dc-u-ta-center\">2</label></div>\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge__item\"><label class=\"dc-input-radio dc-u-cursor-pointer\" for=\"2\"><input class=\"dc-input-radio__input\" type=\"radio\" id=\"2\" name=\"multiple-choice-options-55929\" value=\"2\"><span class=\"dc-input-radio__indicator\"></span><span class=\"dc-input-radio__text\">\n",
    "\n",
    "    <div class=\"\">\n",
    "\n",
    "    <div>A restaurant using review data to ascribe positive or negative sentiment to a given review.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </span></label></div>\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item dc-u-fx-aic dc-u-ifx\">press<label for=\"2\" class=\"dc-u-cursor-pointer dc-u-bgc-primary-dark dc-u-ml-8 dc-u-brad-all dc-u-bs-lg dc-u-wh-24 dc-u-color-white dc-u-d-ib dc-u-ta-center\">3</label></div>\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge__item\"><label class=\"dc-input-radio dc-u-cursor-pointer\" for=\"3\"><input class=\"dc-input-radio__input\" type=\"radio\" id=\"3\" name=\"multiple-choice-options-55929\" value=\"3\"><span class=\"dc-input-radio__indicator\"></span><span class=\"dc-input-radio__text\">\n",
    "\n",
    "    <div class=\"\">\n",
    "\n",
    "    <div>A bike share company using time and weather data to predict the number of bikes being rented at any given hour.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </span></label></div>\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item dc-u-fx-aic dc-u-ifx\">press<label for=\"3\" class=\"dc-u-cursor-pointer dc-u-bgc-primary-dark dc-u-ml-8 dc-u-brad-all dc-u-bs-lg dc-u-wh-24 dc-u-color-white dc-u-d-ib dc-u-ta-center\">4</label></div>\n",
    "\n",
    "<button aria-label=\"button\" class=\"dc-btn dc-btn--green dc-btn--sm dc-pmce-submit-button\" type=\"button\" data-cy=\"submit-button\"><span>\n",
    "\n",
    "<div class=\"dc-btn__content\">Submit Answer</div>\n",
    "\n",
    "</span></button>\n",
    "\n",
    "<section class=\"dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<nav class=\"dc-u-fx dc-u-fx-fdr dc-sct-feedback__nav\"><button aria-label=\"button\" class=\"dc-btn dc-btn--unstyled dc-sct-feedback__hintandsolution dc-u-trsn-none is-shown dc-u-pv-none no-wrap dc-u-ph-8\" type=\"button\"><span>\n",
    "\n",
    "<div class=\"dc-btn__content dc-u-fx dc-u-fx-aic dc-u-fx-jcc\">Take Hint (-15xp)</div>\n",
    "\n",
    "</span></button></nav>\n",
    "\n",
    "</section>\n",
    "\n",
    "</div>\n",
    "\n",
    "</section>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"exercise-footer\">\n",
    "\n",
    "*   [](https://www.datacamp.com/courses/1939/chapters/5232/continue)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"dc-edge-to-edge__item\"><label class=\"dc-input-radio dc-u-cursor-pointer\" for=\"3\"><span class=\"dc-input-radio__indicator\"></span><span class=\"dc-input-radio__text\">\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div>A bike share company using time and weather data to predict the number of bikes being rented at any given hour.</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</span></label></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"listview__section\" data-onboarding=\"assignment\">\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div class=\"exercise--assignment exercise--typography\">\n",
    "\n",
    "# Importing data for supervised learning\n",
    "\n",
    "<div>\n",
    "\n",
    "<div>\n",
    "\n",
    "In this chapter, you will work with [Gapminder](https://www.gapminder.org/data/) data that we have consolidated into one CSV file available in the workspace as `'gapminder.csv'`. Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country's GDP, fertility rate, and population. As in Chapter 1, the dataset has been preprocessed.\n",
    "\n",
    "Since the target variable here is quantitative, this is a regression problem. To begin, you will fit a linear regression with just one feature: `'fertility'`, which is the average number of children a woman in a given country gives birth to. In later exercises, you will use all the features to build regression models.\n",
    "\n",
    "Before that, however, you need to import the data and get it into the form needed by scikit-learn. This involves creating feature and target variable arrays. Furthermore, since you are going to use only one feature to begin with, you need to do some reshaping using NumPy's `.reshape()` method. Don't worry too much about this reshaping right now, but it is something you will have to do occasionally when working with scikit-learn so it is useful to practice.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__section\" data-onboarding=\"instructions\" style=\"min-height: calc(100% - 33px);\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div role=\"button\" class=\"listview__header\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "<span class=\"tag tag--xp\">100 XP</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div data-onboarding=\"instructions\" class=\"exercise--instructions exercise--typography\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"exercise--instructions__content\">\n",
    "\n",
    "*   Import `numpy` and `pandas` as their standard aliases.\n",
    "*   Read the file `'gapminder.csv'` into a DataFrame `df` using the `read_csv()` function.\n",
    "*   Create array `X` for the `'fertility'` feature and array `y` for the `'life'` target variable.\n",
    "*   Reshape the arrays by using the `.reshape()` method and passing in `-1` and `1`.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"campus-dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-hint\" currentitem=\"false\" style=\"display: inline-block;\">[<span>Take Hint (-30 XP)</span>]</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of y before reshaping: (139,)\n",
      "Dimensions of X before reshaping: (139,)\n",
      "Dimensions of y after reshaping: (139, 1)\n",
      "Dimensions of X after reshaping: (139, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame: df\n",
    "df = pd.read_csv(\"./data/gapminder.csv\") \n",
    "\n",
    "# Create arrays for features and target variable\n",
    "y = df['life'].values\n",
    "X = df['fertility'].values\n",
    "\n",
    "# Print the dimensions of X and y before reshaping\n",
    "print(\"Dimensions of y before reshaping: {}\".format(y.shape))\n",
    "print(\"Dimensions of X before reshaping: {}\".format(X.shape))\n",
    "\n",
    "# Reshape X and y\n",
    "y = y.reshape(-1,1)\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "# Print the dimensions of X and y after reshaping\n",
    "print(\"Dimensions of y after reshaping: {}\".format(y.shape))\n",
    "print(\"Dimensions of X after reshaping: {}\".format(X.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"listview__section\" data-onboarding=\"assignment\">\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div class=\"exercise--assignment exercise--typography\">\n",
    "\n",
    "# Exploring the Gapminder data\n",
    "\n",
    "<div>\n",
    "\n",
    "<div>\n",
    "\n",
    "As always, it is important to explore your data before building models. On the right, we have constructed a heatmap showing the correlation between the different features of the Gapminder dataset, which has been pre-loaded into a DataFrame as `df` and is available for exploration in the IPython Shell. Cells that are in green show positive correlation, while cells that are in red show negative correlation. Take a moment to explore this: Which features are positively correlated with `life`, and which ones are negatively correlated? Does this match your intuition?\n",
    "\n",
    "Then, in the IPython Shell, explore the DataFrame using pandas methods such as `.info()`, `.describe()`, `.head()`.\n",
    "\n",
    "In case you are curious, the heatmap was generated using [Seaborn's heatmap function](http://seaborn.pydata.org/generated/seaborn.heatmap.html) and the following line of code, where `df.corr()` computes the pairwise correlation between columns:\n",
    "\n",
    "`sns.heatmap(df.corr(), square=True, cmap='RdYlGn')`\n",
    "\n",
    "Once you have a feel for the data, consider the statements below and select the one that is **not** true. After this, Hugo will explain the mechanics of linear regression in the next video and you will be on your way building regression models!\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__section\" data-onboarding=\"instructions\" style=\"min-height: calc(100% - 33px);\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div role=\"button\" class=\"listview__header\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "<span class=\"tag tag--xp\">50 XP</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"exercise--instructions\">\n",
    "\n",
    "<div class=\"exercise--instructions-title\">\n",
    "\n",
    "##### Possible Answers\n",
    "\n",
    "</div>\n",
    "\n",
    "<div data-onboarding=\"instructions\" class=\"exercise--typography\">\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge multiple-choice__options\">\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item\"><label for=\"inp_0\" class=\"dc-input-radio\" data-cy=\"mce-option\"><input id=\"inp_0\" data-cy=\"multiple-choice-input-0\" type=\"radio\" class=\"dc-input-radio__input\" value=\"1\" checked=\"\"><span class=\"dc-input-radio__indicator\"></span>\n",
    "\n",
    "    <div>\n",
    "\n",
    "    <div class=\"dc-input-radio__text\">The DataFrame has `139` samples (or rows) and `9` columns.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </label></div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge multiple-choice__options\">\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item\"><label for=\"inp_1\" class=\"dc-input-radio\" data-cy=\"mce-option\"><input id=\"inp_1\" data-cy=\"multiple-choice-input-1\" type=\"radio\" class=\"dc-input-radio__input\" value=\"2\"><span class=\"dc-input-radio__indicator\"></span>\n",
    "\n",
    "    <div>\n",
    "\n",
    "    <div class=\"dc-input-radio__text\">`life` and `fertility` are negatively correlated.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </label></div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge multiple-choice__options\">\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item\"><label for=\"inp_2\" class=\"dc-input-radio\" data-cy=\"mce-option\"><input id=\"inp_2\" data-cy=\"multiple-choice-input-2\" type=\"radio\" class=\"dc-input-radio__input\" value=\"3\"><span class=\"dc-input-radio__indicator\"></span>\n",
    "\n",
    "    <div>\n",
    "\n",
    "    <div class=\"dc-input-radio__text\">The mean of `life` is `69.602878`.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </label></div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge multiple-choice__options\">\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item\"><label for=\"inp_3\" class=\"dc-input-radio\" data-cy=\"mce-option\"><input id=\"inp_3\" data-cy=\"multiple-choice-input-3\" type=\"radio\" class=\"dc-input-radio__input\" value=\"4\"><span class=\"dc-input-radio__indicator\"></span>\n",
    "\n",
    "    <div>\n",
    "\n",
    "    <div class=\"dc-input-radio__text\">`fertility` is of type `int64`.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </label></div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "*   <div class=\"dc-edge-to-edge multiple-choice__options\">\n",
    "\n",
    "    <div class=\"dc-edge-to-edge__item\"><label for=\"inp_4\" class=\"dc-input-radio\" data-cy=\"mce-option\"><input id=\"inp_4\" data-cy=\"multiple-choice-input-4\" type=\"radio\" class=\"dc-input-radio__input\" value=\"5\"><span class=\"dc-input-radio__indicator\"></span>\n",
    "\n",
    "    <div>\n",
    "\n",
    "    <div class=\"dc-input-radio__text\">`GDP` and `life` are positively correlated.</div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    </label></div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "<div class=\"multiple-choice__actions\">\n",
    "\n",
    "<div class=\"exercise--buttons\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-submit-button\" currentitem=\"false\" style=\"float: right; top: -10px;\"><button aria-label=\"button\" class=\"dc-btn dc-btn--green dc-btn--sm\" type=\"button\" id=\"mc-submit\" data-test-id=\"submit-solution-button\" data-cy=\"submit-button\"><span>\n",
    "\n",
    "<div class=\"dc-btn__content\">Submit Answer</div>\n",
    "\n",
    "</span></button></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"campus-dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-hint\" currentitem=\"false\" style=\"display: inline-block;\">[<span>Take Hint (-15 XP)</span>](javascript:void(0))</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/svg+xml;base64,<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Created with matplotlib (http://matplotlib.org/) -->
<svg height="258pt" version="1.1" viewBox="0 0 202 258" width="202pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
 <defs>
  <style type="text/css">
*{stroke-linecap:butt;stroke-linejoin:round;}
  </style>
 </defs>
 <g id="figure_1">
  <g id="patch_1">
   <path d="M 0 258 
L 202 258 
L 202 0 
L 0 0 
z
" style="fill:#ffffff;"/>
  </g>
  <g id="axes_1">
   <g id="patch_2">
    <path d="M 83.867813 148.693519 
L 157.452662 148.693519 
L 157.452662 75.108669 
L 83.867813 75.108669 
z
" style="fill:#eaeaf2;"/>
   </g>
   <g id="matplotlib.axis_1">
    <g id="xtick_1">
     <g id="line2d_1"/>
     <g id="text_1">
      <!-- population -->
      <defs>
       <path d="M 51.421875 26.65625 
Q 51.421875 20.65625 50.4375 15.578125 
Q 49.46875 10.5 47.1875 6.828125 
Q 44.921875 3.171875 41.1875 1.09375 
Q 37.453125 -0.984375 31.984375 -0.984375 
Q 26.3125 -0.984375 22.0625 1.171875 
Q 17.828125 3.328125 15.578125 8.203125 
L 15.328125 8.203125 
Q 15.375 8.109375 15.40625 7.328125 
Q 15.4375 6.546875 15.453125 5.375 
Q 15.484375 4.203125 15.5 2.75 
Q 15.53125 1.3125 15.53125 -0.09375 
L 15.53125 -20.75 
L 6.734375 -20.75 
L 6.734375 42.046875 
Q 6.734375 43.953125 6.703125 45.703125 
Q 6.6875 47.46875 6.640625 48.90625 
Q 6.59375 50.34375 6.546875 51.359375 
Q 6.5 52.390625 6.453125 52.828125 
L 14.9375 52.828125 
Q 14.984375 52.6875 15.0625 51.796875 
Q 15.140625 50.921875 15.203125 49.671875 
Q 15.28125 48.4375 15.359375 47.015625 
Q 15.4375 45.609375 15.4375 44.34375 
L 15.625 44.34375 
Q 16.84375 46.875 18.40625 48.65625 
Q 19.96875 50.4375 21.96875 51.578125 
Q 23.96875 52.734375 26.4375 53.25 
Q 28.90625 53.765625 31.984375 53.765625 
Q 37.453125 53.765625 41.1875 51.8125 
Q 44.921875 49.859375 47.1875 46.3125 
Q 49.46875 42.78125 50.4375 37.765625 
Q 51.421875 32.765625 51.421875 26.65625 
z
M 42.1875 26.46875 
Q 42.1875 31.34375 41.59375 35.15625 
Q 41.015625 38.96875 39.578125 41.59375 
Q 38.140625 44.234375 35.734375 45.59375 
Q 33.34375 46.96875 29.734375 46.96875 
Q 26.8125 46.96875 24.21875 46.140625 
Q 21.625 45.3125 19.703125 42.96875 
Q 17.78125 40.625 16.65625 36.5 
Q 15.53125 32.375 15.53125 25.78125 
Q 15.53125 20.171875 16.453125 16.28125 
Q 17.390625 12.40625 19.171875 10.015625 
Q 20.953125 7.625 23.578125 6.5625 
Q 26.21875 5.515625 29.640625 5.515625 
Q 33.296875 5.515625 35.71875 6.921875 
Q 38.140625 8.34375 39.578125 11.03125 
Q 41.015625 13.71875 41.59375 17.59375 
Q 42.1875 21.484375 42.1875 26.46875 
z
" id="LiberationSans-70"/>
       <path d="M 51.421875 26.46875 
Q 51.421875 12.59375 45.3125 5.796875 
Q 39.203125 -0.984375 27.59375 -0.984375 
Q 22.078125 -0.984375 17.71875 0.671875 
Q 13.375 2.34375 10.375 5.765625 
Q 7.375 9.1875 5.78125 14.328125 
Q 4.203125 19.484375 4.203125 26.46875 
Q 4.203125 53.8125 27.875 53.8125 
Q 34.03125 53.8125 38.5 52.09375 
Q 42.96875 50.390625 45.828125 46.96875 
Q 48.6875 43.5625 50.046875 38.421875 
Q 51.421875 33.296875 51.421875 26.46875 
z
M 42.1875 26.46875 
Q 42.1875 32.625 41.234375 36.625 
Q 40.28125 40.625 38.453125 43.015625 
Q 36.625 45.40625 33.984375 46.359375 
Q 31.34375 47.3125 28.03125 47.3125 
Q 24.65625 47.3125 21.9375 46.3125 
Q 19.234375 45.3125 17.328125 42.890625 
Q 15.4375 40.484375 14.421875 36.46875 
Q 13.421875 32.46875 13.421875 26.46875 
Q 13.421875 20.3125 14.5 16.28125 
Q 15.578125 12.25 17.453125 9.859375 
Q 19.34375 7.46875 21.90625 6.484375 
Q 24.46875 5.515625 27.484375 5.515625 
Q 30.859375 5.515625 33.59375 6.46875 
Q 36.328125 7.421875 38.234375 9.8125 
Q 40.140625 12.203125 41.15625 16.25 
Q 42.1875 20.3125 42.1875 26.46875 
z
" id="LiberationSans-6f"/>
       <path d="M 15.328125 52.828125 
L 15.328125 19.34375 
Q 15.328125 15.484375 15.890625 12.890625 
Q 16.453125 10.296875 17.71875 8.703125 
Q 19 7.125 21.0625 6.46875 
Q 23.140625 5.8125 26.21875 5.8125 
Q 29.34375 5.8125 31.859375 6.90625 
Q 34.375 8.015625 36.15625 10.078125 
Q 37.9375 12.15625 38.90625 15.203125 
Q 39.890625 18.265625 39.890625 22.21875 
L 39.890625 52.828125 
L 48.6875 52.828125 
L 48.6875 11.28125 
Q 48.6875 9.625 48.703125 7.78125 
Q 48.734375 5.953125 48.78125 4.3125 
Q 48.828125 2.6875 48.875 1.515625 
Q 48.921875 0.34375 48.96875 0 
L 40.671875 0 
Q 40.625 0.25 40.578125 1.3125 
Q 40.53125 2.390625 40.453125 3.78125 
Q 40.375 5.171875 40.328125 6.609375 
Q 40.28125 8.0625 40.28125 9.03125 
L 40.140625 9.03125 
Q 38.875 6.734375 37.359375 4.875 
Q 35.84375 3.03125 33.84375 1.734375 
Q 31.84375 0.4375 29.25 -0.265625 
Q 26.65625 -0.984375 23.25 -0.984375 
Q 18.84375 -0.984375 15.671875 0.09375 
Q 12.5 1.171875 10.453125 3.421875 
Q 8.40625 5.671875 7.453125 9.1875 
Q 6.5 12.703125 6.5 17.625 
L 6.5 52.828125 
z
" id="LiberationSans-75"/>
       <path d="M 6.734375 0 
L 6.734375 72.46875 
L 15.53125 72.46875 
L 15.53125 0 
z
" id="LiberationSans-6c"/>
       <path d="M 20.21875 -0.984375 
Q 12.25 -0.984375 8.25 3.21875 
Q 4.25 7.421875 4.25 14.75 
Q 4.25 19.96875 6.21875 23.3125 
Q 8.203125 26.65625 11.390625 28.5625 
Q 14.59375 30.46875 18.6875 31.203125 
Q 22.796875 31.9375 27.046875 32.03125 
L 38.921875 32.234375 
L 38.921875 35.109375 
Q 38.921875 38.375 38.234375 40.671875 
Q 37.546875 42.96875 36.125 44.375 
Q 34.71875 45.796875 32.59375 46.453125 
Q 30.46875 47.125 27.59375 47.125 
Q 25.046875 47.125 23 46.75 
Q 20.953125 46.390625 19.4375 45.4375 
Q 17.921875 44.484375 16.984375 42.84375 
Q 16.0625 41.21875 15.765625 38.71875 
L 6.59375 39.546875 
Q 7.078125 42.671875 8.4375 45.28125 
Q 9.8125 47.90625 12.328125 49.796875 
Q 14.84375 51.703125 18.625 52.75 
Q 22.40625 53.8125 27.78125 53.8125 
Q 37.75 53.8125 42.765625 49.234375 
Q 47.796875 44.671875 47.796875 36.03125 
L 47.796875 13.28125 
Q 47.796875 9.375 48.828125 7.390625 
Q 49.859375 5.421875 52.734375 5.421875 
Q 53.46875 5.421875 54.203125 5.515625 
Q 54.9375 5.609375 55.609375 5.765625 
L 55.609375 0.296875 
Q 53.953125 -0.09375 52.3125 -0.28125 
Q 50.6875 -0.484375 48.828125 -0.484375 
Q 46.34375 -0.484375 44.5625 0.171875 
Q 42.78125 0.828125 41.65625 2.171875 
Q 40.53125 3.515625 39.9375 5.484375 
Q 39.359375 7.46875 39.203125 10.109375 
L 38.921875 10.109375 
Q 37.5 7.5625 35.8125 5.515625 
Q 34.125 3.46875 31.875 2.03125 
Q 29.640625 0.59375 26.78125 -0.1875 
Q 23.921875 -0.984375 20.21875 -0.984375 
z
M 22.21875 5.609375 
Q 26.421875 5.609375 29.5625 7.140625 
Q 32.71875 8.6875 34.78125 11.078125 
Q 36.859375 13.484375 37.890625 16.3125 
Q 38.921875 19.140625 38.921875 21.734375 
L 38.921875 26.078125 
L 29.296875 25.875 
Q 26.078125 25.828125 23.171875 25.40625 
Q 20.265625 25 18.0625 23.78125 
Q 15.875 22.5625 14.578125 20.359375 
Q 13.28125 18.171875 13.28125 14.59375 
Q 13.28125 10.296875 15.59375 7.953125 
Q 17.921875 5.609375 22.21875 5.609375 
z
" id="LiberationSans-61"/>
       <path d="M 27.046875 0.390625 
Q 25.046875 -0.140625 22.96875 -0.453125 
Q 20.90625 -0.78125 18.171875 -0.78125 
Q 7.625 -0.78125 7.625 11.1875 
L 7.625 46.4375 
L 1.515625 46.4375 
L 1.515625 52.828125 
L 7.953125 52.828125 
L 10.546875 64.65625 
L 16.40625 64.65625 
L 16.40625 52.828125 
L 26.171875 52.828125 
L 26.171875 46.4375 
L 16.40625 46.4375 
L 16.40625 13.09375 
Q 16.40625 9.28125 17.640625 7.734375 
Q 18.890625 6.203125 21.96875 6.203125 
Q 23.25 6.203125 24.4375 6.390625 
Q 25.640625 6.59375 27.046875 6.890625 
z
" id="LiberationSans-74"/>
       <path d="M 6.6875 64.0625 
L 6.6875 72.46875 
L 15.484375 72.46875 
L 15.484375 64.0625 
z
M 6.6875 0 
L 6.6875 52.828125 
L 15.484375 52.828125 
L 15.484375 0 
z
" id="LiberationSans-69"/>
       <path d="M 40.28125 0 
L 40.28125 33.5 
Q 40.28125 37.359375 39.71875 39.9375 
Q 39.15625 42.53125 37.890625 44.109375 
Q 36.625 45.703125 34.546875 46.359375 
Q 32.46875 47.015625 29.390625 47.015625 
Q 26.265625 47.015625 23.75 45.921875 
Q 21.234375 44.828125 19.453125 42.75 
Q 17.671875 40.671875 16.6875 37.625 
Q 15.71875 34.578125 15.71875 30.609375 
L 15.71875 0 
L 6.9375 0 
L 6.9375 41.546875 
Q 6.9375 43.21875 6.90625 45.046875 
Q 6.890625 46.875 6.828125 48.5 
Q 6.78125 50.140625 6.734375 51.3125 
Q 6.6875 52.484375 6.640625 52.828125 
L 14.9375 52.828125 
Q 14.984375 52.59375 15.03125 51.515625 
Q 15.09375 50.4375 15.15625 49.046875 
Q 15.234375 47.65625 15.28125 46.21875 
Q 15.328125 44.78125 15.328125 43.796875 
L 15.484375 43.796875 
Q 16.75 46.09375 18.265625 47.953125 
Q 19.78125 49.8125 21.78125 51.09375 
Q 23.78125 52.390625 26.359375 53.09375 
Q 28.953125 53.8125 32.375 53.8125 
Q 36.765625 53.8125 39.9375 52.734375 
Q 43.109375 51.65625 45.15625 49.40625 
Q 47.21875 47.171875 48.171875 43.625 
Q 49.125 40.09375 49.125 35.203125 
L 49.125 0 
z
" id="LiberationSans-6e"/>
      </defs>
      <g style="fill:#262626;" transform="translate(90.541797 201.841956)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-70"/>
       <use x="55.615234" xlink:href="#LiberationSans-6f"/>
       <use x="111.230469" xlink:href="#LiberationSans-70"/>
       <use x="166.845703" xlink:href="#LiberationSans-75"/>
       <use x="222.460938" xlink:href="#LiberationSans-6c"/>
       <use x="244.677734" xlink:href="#LiberationSans-61"/>
       <use x="300.292969" xlink:href="#LiberationSans-74"/>
       <use x="328.076172" xlink:href="#LiberationSans-69"/>
       <use x="350.292969" xlink:href="#LiberationSans-6f"/>
       <use x="405.908203" xlink:href="#LiberationSans-6e"/>
      </g>
     </g>
    </g>
    <g id="xtick_2">
     <g id="line2d_2"/>
     <g id="text_2">
      <!-- fertility -->
      <defs>
       <path d="M 17.625 46.4375 
L 17.625 0 
L 8.84375 0 
L 8.84375 46.4375 
L 1.421875 46.4375 
L 1.421875 52.828125 
L 8.84375 52.828125 
L 8.84375 58.796875 
Q 8.84375 61.671875 9.375 64.140625 
Q 9.90625 66.609375 11.34375 68.4375 
Q 12.796875 70.265625 15.28125 71.3125 
Q 17.78125 72.359375 21.734375 72.359375 
Q 23.296875 72.359375 24.96875 72.21875 
Q 26.65625 72.078125 27.9375 71.78125 
L 27.9375 65.09375 
Q 27.09375 65.234375 26 65.359375 
Q 24.90625 65.484375 24.03125 65.484375 
Q 22.078125 65.484375 20.828125 64.9375 
Q 19.578125 64.40625 18.875 63.40625 
Q 18.171875 62.40625 17.890625 60.9375 
Q 17.625 59.46875 17.625 57.5625 
L 17.625 52.828125 
L 27.9375 52.828125 
L 27.9375 46.4375 
z
" id="LiberationSans-66"/>
       <path d="M 13.484375 24.5625 
Q 13.484375 20.40625 14.328125 16.90625 
Q 15.1875 13.421875 16.96875 10.90625 
Q 18.75 8.40625 21.53125 7 
Q 24.3125 5.609375 28.21875 5.609375 
Q 33.9375 5.609375 37.375 7.90625 
Q 40.828125 10.203125 42.046875 13.71875 
L 49.75 11.53125 
Q 48.921875 9.328125 47.4375 7.109375 
Q 45.953125 4.890625 43.453125 3.09375 
Q 40.96875 1.3125 37.234375 0.15625 
Q 33.5 -0.984375 28.21875 -0.984375 
Q 16.5 -0.984375 10.375 6 
Q 4.25 12.984375 4.25 26.765625 
Q 4.25 34.1875 6.09375 39.328125 
Q 7.953125 44.484375 11.171875 47.703125 
Q 14.40625 50.921875 18.703125 52.359375 
Q 23 53.8125 27.875 53.8125 
Q 34.515625 53.8125 38.984375 51.65625 
Q 43.453125 49.515625 46.15625 45.71875 
Q 48.875 41.9375 50.015625 36.8125 
Q 51.171875 31.6875 51.171875 25.734375 
L 51.171875 24.5625 
z
M 42.09375 31.296875 
Q 41.359375 39.65625 37.84375 43.484375 
Q 34.328125 47.3125 27.734375 47.3125 
Q 25.53125 47.3125 23.109375 46.609375 
Q 20.703125 45.90625 18.65625 44.09375 
Q 16.609375 42.28125 15.1875 39.171875 
Q 13.765625 36.078125 13.578125 31.296875 
z
" id="LiberationSans-65"/>
       <path d="M 6.9375 0 
L 6.9375 40.53125 
Q 6.9375 42.1875 6.90625 43.921875 
Q 6.890625 45.65625 6.828125 47.265625 
Q 6.78125 48.875 6.734375 50.28125 
Q 6.6875 51.703125 6.640625 52.828125 
L 14.9375 52.828125 
Q 14.984375 51.703125 15.0625 50.265625 
Q 15.140625 48.828125 15.203125 47.3125 
Q 15.28125 45.796875 15.296875 44.40625 
Q 15.328125 43.015625 15.328125 42.046875 
L 15.53125 42.046875 
Q 16.453125 45.0625 17.5 47.28125 
Q 18.5625 49.515625 19.96875 50.953125 
Q 21.390625 52.390625 23.34375 53.09375 
Q 25.296875 53.8125 28.078125 53.8125 
Q 29.15625 53.8125 30.125 53.640625 
Q 31.109375 53.46875 31.640625 53.328125 
L 31.640625 45.265625 
Q 30.765625 45.515625 29.59375 45.625 
Q 28.421875 45.75 26.953125 45.75 
Q 23.921875 45.75 21.796875 44.375 
Q 19.671875 43.015625 18.328125 40.59375 
Q 17 38.1875 16.359375 34.84375 
Q 15.71875 31.5 15.71875 27.546875 
L 15.71875 0 
z
" id="LiberationSans-72"/>
       <path d="M 29.5 0 
Q 27.640625 -4.78125 25.703125 -8.609375 
Q 23.78125 -12.453125 21.390625 -15.15625 
Q 19 -17.875 16.0625 -19.3125 
Q 13.140625 -20.75 9.328125 -20.75 
Q 7.671875 -20.75 6.25 -20.65625 
Q 4.828125 -20.5625 3.265625 -20.21875 
L 3.265625 -13.625 
Q 4.203125 -13.765625 5.375 -13.84375 
Q 6.546875 -13.921875 7.375 -13.921875 
Q 11.234375 -13.921875 14.546875 -11.03125 
Q 17.875 -8.15625 20.359375 -1.859375 
L 21.1875 0.25 
L 0.25 52.828125 
L 9.625 52.828125 
L 20.75 23.640625 
Q 21.234375 22.3125 21.984375 20.109375 
Q 22.75 17.921875 23.5 15.71875 
Q 24.265625 13.53125 24.84375 11.765625 
Q 25.4375 10.015625 25.53125 9.578125 
Q 25.6875 10.109375 26.25 11.6875 
Q 26.8125 13.28125 27.515625 15.234375 
Q 28.21875 17.1875 28.953125 19.1875 
Q 29.6875 21.1875 30.171875 22.65625 
L 40.53125 52.828125 
L 49.8125 52.828125 
z
" id="LiberationSans-79"/>
      </defs>
      <g style="fill:#262626;" transform="translate(98.717892 184.584144)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-66"/>
       <use x="27.783203" xlink:href="#LiberationSans-65"/>
       <use x="83.398438" xlink:href="#LiberationSans-72"/>
       <use x="116.699219" xlink:href="#LiberationSans-74"/>
       <use x="144.482422" xlink:href="#LiberationSans-69"/>
       <use x="166.699219" xlink:href="#LiberationSans-6c"/>
       <use x="188.916016" xlink:href="#LiberationSans-69"/>
       <use x="211.132812" xlink:href="#LiberationSans-74"/>
       <use x="238.916016" xlink:href="#LiberationSans-79"/>
      </g>
     </g>
    </g>
    <g id="xtick_3">
     <g id="line2d_3"/>
     <g id="text_3">
      <!-- HIV -->
      <defs>
       <path d="M 54.734375 0 
L 54.734375 31.890625 
L 17.53125 31.890625 
L 17.53125 0 
L 8.203125 0 
L 8.203125 68.796875 
L 17.53125 68.796875 
L 17.53125 39.703125 
L 54.734375 39.703125 
L 54.734375 68.796875 
L 64.0625 68.796875 
L 64.0625 0 
z
" id="LiberationSans-48"/>
       <path d="M 9.234375 0 
L 9.234375 68.796875 
L 18.5625 68.796875 
L 18.5625 0 
z
" id="LiberationSans-49"/>
       <path d="M 38.1875 0 
L 28.515625 0 
L 0.4375 68.796875 
L 10.25 68.796875 
L 29.296875 20.359375 
Q 30.03125 18.171875 30.765625 15.984375 
Q 31.5 13.8125 32.078125 12.109375 
Q 32.765625 10.109375 33.40625 8.203125 
Q 33.984375 10.015625 34.671875 12.015625 
Q 35.25 13.71875 35.953125 15.859375 
Q 36.671875 18.015625 37.5 20.359375 
L 56.453125 68.796875 
L 66.265625 68.796875 
z
" id="LiberationSans-56"/>
      </defs>
      <g style="fill:#262626;" transform="translate(106.893986 172.363831)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-48"/>
       <use x="72.216797" xlink:href="#LiberationSans-49"/>
       <use x="100" xlink:href="#LiberationSans-56"/>
      </g>
     </g>
    </g>
    <g id="xtick_4">
     <g id="line2d_4"/>
     <g id="text_4">
      <!-- CO2 -->
      <defs>
       <path d="M 38.671875 62.203125 
Q 32.8125 62.203125 28.296875 60.25 
Q 23.78125 58.296875 20.71875 54.6875 
Q 17.671875 51.078125 16.109375 46 
Q 14.546875 40.921875 14.546875 34.71875 
Q 14.546875 28.515625 16.234375 23.359375 
Q 17.921875 18.21875 21.0625 14.5 
Q 24.21875 10.796875 28.78125 8.734375 
Q 33.34375 6.6875 39.0625 6.6875 
Q 43.0625 6.6875 46.359375 7.734375 
Q 49.65625 8.796875 52.3125 10.6875 
Q 54.984375 12.59375 57.078125 15.203125 
Q 59.1875 17.828125 60.796875 21 
L 68.40625 17.1875 
Q 66.546875 13.328125 63.8125 10 
Q 61.078125 6.6875 57.390625 4.25 
Q 53.71875 1.8125 49.046875 0.40625 
Q 44.390625 -0.984375 38.625 -0.984375 
Q 30.28125 -0.984375 24 1.671875 
Q 17.71875 4.34375 13.5 9.109375 
Q 9.28125 13.875 7.171875 20.40625 
Q 5.078125 26.953125 5.078125 34.71875 
Q 5.078125 42.78125 7.296875 49.265625 
Q 9.515625 55.765625 13.78125 60.328125 
Q 18.0625 64.890625 24.3125 67.359375 
Q 30.5625 69.828125 38.578125 69.828125 
Q 49.5625 69.828125 56.9375 65.53125 
Q 64.3125 61.234375 67.78125 52.78125 
L 58.9375 49.859375 
Q 57.953125 52.296875 56.296875 54.515625 
Q 54.640625 56.734375 52.140625 58.4375 
Q 49.65625 60.15625 46.3125 61.171875 
Q 42.96875 62.203125 38.671875 62.203125 
z
" id="LiberationSans-43"/>
       <path d="M 73 34.71875 
Q 73 26.65625 70.671875 20.0625 
Q 68.359375 13.484375 63.96875 8.796875 
Q 59.578125 4.109375 53.21875 1.5625 
Q 46.875 -0.984375 38.8125 -0.984375 
Q 30.328125 -0.984375 23.921875 1.671875 
Q 17.53125 4.34375 13.28125 9.109375 
Q 9.03125 13.875 6.875 20.40625 
Q 4.734375 26.953125 4.734375 34.71875 
Q 4.734375 42.78125 6.984375 49.265625 
Q 9.234375 55.765625 13.578125 60.328125 
Q 17.921875 64.890625 24.296875 67.359375 
Q 30.671875 69.828125 38.921875 69.828125 
Q 47.125 69.828125 53.484375 67.328125 
Q 59.859375 64.84375 64.171875 60.25 
Q 68.5 55.671875 70.75 49.171875 
Q 73 42.671875 73 34.71875 
z
M 63.484375 34.71875 
Q 63.484375 40.921875 61.890625 46 
Q 60.296875 51.078125 57.203125 54.6875 
Q 54.109375 58.296875 49.515625 60.25 
Q 44.921875 62.203125 38.921875 62.203125 
Q 32.765625 62.203125 28.125 60.25 
Q 23.484375 58.296875 20.375 54.6875 
Q 17.28125 51.078125 15.734375 46 
Q 14.203125 40.921875 14.203125 34.71875 
Q 14.203125 28.515625 15.796875 23.359375 
Q 17.390625 18.21875 20.484375 14.484375 
Q 23.578125 10.75 28.1875 8.671875 
Q 32.8125 6.59375 38.8125 6.59375 
Q 45.265625 6.59375 49.921875 8.6875 
Q 54.59375 10.796875 57.59375 14.53125 
Q 60.59375 18.265625 62.03125 23.4375 
Q 63.484375 28.609375 63.484375 34.71875 
z
" id="LiberationSans-4f"/>
       <path d="M 5.03125 0 
L 5.03125 6.203125 
Q 7.515625 11.921875 11.109375 16.28125 
Q 14.703125 20.65625 18.65625 24.1875 
Q 22.609375 27.734375 26.484375 30.765625 
Q 30.375 33.796875 33.5 36.8125 
Q 36.625 39.84375 38.546875 43.15625 
Q 40.484375 46.484375 40.484375 50.6875 
Q 40.484375 53.609375 39.59375 55.828125 
Q 38.71875 58.0625 37.0625 59.5625 
Q 35.40625 61.078125 33.078125 61.828125 
Q 30.765625 62.59375 27.9375 62.59375 
Q 25.296875 62.59375 22.96875 61.859375 
Q 20.65625 61.140625 18.84375 59.671875 
Q 17.046875 58.203125 15.890625 56.03125 
Q 14.75 53.859375 14.40625 50.984375 
L 5.421875 51.8125 
Q 5.859375 55.515625 7.46875 58.78125 
Q 9.078125 62.0625 11.90625 64.53125 
Q 14.75 67 18.71875 68.40625 
Q 22.703125 69.828125 27.9375 69.828125 
Q 33.0625 69.828125 37.0625 68.609375 
Q 41.0625 67.390625 43.8125 64.984375 
Q 46.578125 62.59375 48.046875 59.078125 
Q 49.515625 55.5625 49.515625 50.984375 
Q 49.515625 47.515625 48.265625 44.390625 
Q 47.015625 41.265625 44.9375 38.421875 
Q 42.875 35.59375 40.140625 32.953125 
Q 37.40625 30.328125 34.421875 27.8125 
Q 31.453125 25.296875 28.421875 22.828125 
Q 25.390625 20.359375 22.71875 17.859375 
Q 20.0625 15.375 17.96875 12.8125 
Q 15.875 10.25 14.703125 7.46875 
L 50.59375 7.46875 
L 50.59375 0 
z
" id="LiberationSans-32"/>
      </defs>
      <g style="fill:#262626;" transform="translate(115.070081 176.254456)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-43"/>
       <use x="72.216797" xlink:href="#LiberationSans-4f"/>
       <use x="150" xlink:href="#LiberationSans-32"/>
      </g>
     </g>
    </g>
    <g id="xtick_5">
     <g id="line2d_5"/>
     <g id="text_5">
      <!-- BMI_male -->
      <defs>
       <path d="M 61.421875 19.390625 
Q 61.421875 14.15625 59.375 10.46875 
Q 57.328125 6.78125 53.859375 4.46875 
Q 50.390625 2.15625 45.796875 1.078125 
Q 41.21875 0 36.140625 0 
L 8.203125 0 
L 8.203125 68.796875 
L 33.203125 68.796875 
Q 38.921875 68.796875 43.40625 67.84375 
Q 47.90625 66.890625 51.03125 64.84375 
Q 54.15625 62.796875 55.78125 59.640625 
Q 57.421875 56.5 57.421875 52.09375 
Q 57.421875 49.21875 56.609375 46.671875 
Q 55.8125 44.140625 54.171875 42.0625 
Q 52.546875 39.984375 50.078125 38.5 
Q 47.609375 37.015625 44.34375 36.28125 
Q 48.484375 35.796875 51.6875 34.375 
Q 54.890625 32.953125 57.03125 30.75 
Q 59.1875 28.5625 60.296875 25.65625 
Q 61.421875 22.75 61.421875 19.390625 
z
M 48.046875 50.984375 
Q 48.046875 56.546875 44.234375 58.9375 
Q 40.4375 61.328125 33.203125 61.328125 
L 17.53125 61.328125 
L 17.53125 39.546875 
L 33.203125 39.546875 
Q 37.3125 39.546875 40.140625 40.34375 
Q 42.96875 41.15625 44.71875 42.640625 
Q 46.484375 44.140625 47.265625 46.234375 
Q 48.046875 48.34375 48.046875 50.984375 
z
M 52 20.125 
Q 52 23.390625 50.796875 25.6875 
Q 49.609375 27.984375 47.359375 29.4375 
Q 45.125 30.90625 41.96875 31.59375 
Q 38.8125 32.28125 34.90625 32.28125 
L 17.53125 32.28125 
L 17.53125 7.46875 
L 35.640625 7.46875 
Q 39.203125 7.46875 42.203125 8.046875 
Q 45.21875 8.640625 47.390625 10.109375 
Q 49.5625 11.578125 50.78125 14.015625 
Q 52 16.453125 52 20.125 
z
" id="LiberationSans-42"/>
       <path d="M 66.703125 0 
L 66.703125 45.90625 
Q 66.703125 48.390625 66.75 50.96875 
Q 66.796875 53.5625 66.890625 55.71875 
Q 67 58.203125 67.140625 60.546875 
Q 66.453125 58.0625 65.71875 55.609375 
Q 65.09375 53.515625 64.328125 51.140625 
Q 63.578125 48.78125 62.84375 46.875 
L 45.0625 0 
L 38.53125 0 
L 20.515625 46.875 
Q 20.21875 47.609375 19.890625 48.578125 
Q 19.578125 49.5625 19.203125 50.65625 
Q 18.84375 51.765625 18.46875 52.90625 
Q 18.109375 54.046875 17.78125 55.171875 
Q 16.9375 57.765625 16.15625 60.546875 
Q 16.21875 57.8125 16.3125 55.125 
Q 16.40625 52.828125 16.453125 50.3125 
Q 16.5 47.796875 16.5 45.90625 
L 16.5 0 
L 8.203125 0 
L 8.203125 68.796875 
L 20.453125 68.796875 
L 38.765625 21.09375 
Q 39.109375 20.125 39.59375 18.578125 
Q 40.09375 17.046875 40.53125 15.421875 
Q 40.96875 13.8125 41.328125 12.375 
Q 41.703125 10.9375 41.84375 10.15625 
Q 42 10.9375 42.390625 12.40625 
Q 42.78125 13.875 43.28125 15.484375 
Q 43.796875 17.09375 44.28125 18.609375 
Q 44.78125 20.125 45.171875 21.09375 
L 63.140625 68.796875 
L 75.09375 68.796875 
L 75.09375 0 
z
" id="LiberationSans-4d"/>
       <path d="M -1.515625 -19.875 
L -1.515625 -13.53125 
L 56.734375 -13.53125 
L 56.734375 -19.875 
z
" id="LiberationSans-5f"/>
       <path d="M 37.5 0 
L 37.5 33.5 
Q 37.5 37.359375 37.015625 39.9375 
Q 36.53125 42.53125 35.375 44.109375 
Q 34.234375 45.703125 32.375 46.359375 
Q 30.515625 47.015625 27.828125 47.015625 
Q 25.046875 47.015625 22.796875 45.921875 
Q 20.5625 44.828125 18.96875 42.75 
Q 17.390625 40.671875 16.53125 37.625 
Q 15.671875 34.578125 15.671875 30.609375 
L 15.671875 0 
L 6.9375 0 
L 6.9375 41.546875 
Q 6.9375 43.21875 6.90625 45.046875 
Q 6.890625 46.875 6.828125 48.5 
Q 6.78125 50.140625 6.734375 51.3125 
Q 6.6875 52.484375 6.640625 52.828125 
L 14.9375 52.828125 
Q 14.984375 52.59375 15.03125 51.515625 
Q 15.09375 50.4375 15.15625 49.046875 
Q 15.234375 47.65625 15.28125 46.21875 
Q 15.328125 44.78125 15.328125 43.796875 
L 15.484375 43.796875 
Q 16.65625 46.09375 18.015625 47.953125 
Q 19.390625 49.8125 21.21875 51.09375 
Q 23.046875 52.390625 25.40625 53.09375 
Q 27.78125 53.8125 30.90625 53.8125 
Q 36.921875 53.8125 40.40625 51.421875 
Q 43.890625 49.03125 45.265625 43.796875 
L 45.40625 43.796875 
Q 46.578125 46.09375 48.046875 47.953125 
Q 49.515625 49.8125 51.46875 51.09375 
Q 53.421875 52.390625 55.859375 53.09375 
Q 58.296875 53.8125 61.421875 53.8125 
Q 65.4375 53.8125 68.328125 52.734375 
Q 71.234375 51.65625 73.09375 49.40625 
Q 74.953125 47.171875 75.828125 43.625 
Q 76.703125 40.09375 76.703125 35.203125 
L 76.703125 0 
L 68.015625 0 
L 68.015625 33.5 
Q 68.015625 37.359375 67.53125 39.9375 
Q 67.046875 42.53125 65.890625 44.109375 
Q 64.75 45.703125 62.890625 46.359375 
Q 61.03125 47.015625 58.34375 47.015625 
Q 55.5625 47.015625 53.3125 45.96875 
Q 51.078125 44.921875 49.484375 42.875 
Q 47.90625 40.828125 47.046875 37.75 
Q 46.1875 34.671875 46.1875 30.609375 
L 46.1875 0 
z
" id="LiberationSans-6d"/>
      </defs>
      <g style="fill:#262626;" transform="translate(123.246175 200.706019)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-42"/>
       <use x="66.699219" xlink:href="#LiberationSans-4d"/>
       <use x="150" xlink:href="#LiberationSans-49"/>
       <use x="177.783203" xlink:href="#LiberationSans-5f"/>
       <use x="233.398438" xlink:href="#LiberationSans-6d"/>
       <use x="316.699219" xlink:href="#LiberationSans-61"/>
       <use x="372.314453" xlink:href="#LiberationSans-6c"/>
       <use x="394.53125" xlink:href="#LiberationSans-65"/>
      </g>
     </g>
    </g>
    <g id="xtick_6">
     <g id="line2d_6"/>
     <g id="text_6">
      <!-- GDP -->
      <defs>
       <path d="M 5.03125 34.71875 
Q 5.03125 42.78125 7.203125 49.265625 
Q 9.375 55.765625 13.6875 60.328125 
Q 18.015625 64.890625 24.40625 67.359375 
Q 30.8125 69.828125 39.265625 69.828125 
Q 45.609375 69.828125 50.390625 68.703125 
Q 55.171875 67.578125 58.6875 65.46875 
Q 62.203125 63.375 64.640625 60.375 
Q 67.09375 57.375 68.796875 53.609375 
L 59.90625 50.984375 
Q 58.640625 53.515625 56.859375 55.59375 
Q 55.078125 57.671875 52.53125 59.125 
Q 50 60.59375 46.65625 61.390625 
Q 43.3125 62.203125 39.015625 62.203125 
Q 32.765625 62.203125 28.140625 60.25 
Q 23.53125 58.296875 20.5 54.6875 
Q 17.484375 51.078125 15.984375 46 
Q 14.5 40.921875 14.5 34.71875 
Q 14.5 28.515625 16.109375 23.359375 
Q 17.71875 18.21875 20.890625 14.484375 
Q 24.078125 10.75 28.78125 8.671875 
Q 33.5 6.59375 39.703125 6.59375 
Q 43.75 6.59375 47.171875 7.265625 
Q 50.59375 7.953125 53.34375 9.046875 
Q 56.109375 10.15625 58.203125 11.5 
Q 60.296875 12.84375 61.71875 14.203125 
L 61.71875 26.609375 
L 41.15625 26.609375 
L 41.15625 34.421875 
L 70.3125 34.421875 
L 70.3125 10.6875 
Q 68.015625 8.34375 64.90625 6.21875 
Q 61.8125 4.109375 57.953125 2.484375 
Q 54.109375 0.875 49.515625 -0.046875 
Q 44.921875 -0.984375 39.703125 -0.984375 
Q 31 -0.984375 24.5 1.671875 
Q 18.015625 4.34375 13.6875 9.109375 
Q 9.375 13.875 7.203125 20.40625 
Q 5.03125 26.953125 5.03125 34.71875 
z
" id="LiberationSans-47"/>
       <path d="M 67.4375 35.109375 
Q 67.4375 26.515625 64.84375 19.9375 
Q 62.25 13.375 57.75 8.953125 
Q 53.265625 4.546875 47.140625 2.265625 
Q 41.015625 0 33.9375 0 
L 8.203125 0 
L 8.203125 68.796875 
L 30.953125 68.796875 
Q 38.921875 68.796875 45.625 66.8125 
Q 52.34375 64.84375 57.171875 60.71875 
Q 62.015625 56.59375 64.71875 50.234375 
Q 67.4375 43.890625 67.4375 35.109375 
z
M 58.0625 35.109375 
Q 58.0625 42.09375 56.046875 47.046875 
Q 54.046875 52 50.4375 55.171875 
Q 46.828125 58.34375 41.796875 59.828125 
Q 36.765625 61.328125 30.765625 61.328125 
L 17.53125 61.328125 
L 17.53125 7.46875 
L 32.859375 7.46875 
Q 38.28125 7.46875 42.890625 9.21875 
Q 47.515625 10.984375 50.875 14.453125 
Q 54.25 17.921875 56.15625 23.09375 
Q 58.0625 28.265625 58.0625 35.109375 
z
" id="LiberationSans-44"/>
       <path d="M 61.421875 48.09375 
Q 61.421875 43.609375 59.9375 39.71875 
Q 58.453125 35.84375 55.5 32.984375 
Q 52.546875 30.125 48.09375 28.46875 
Q 43.65625 26.8125 37.75 26.8125 
L 17.53125 26.8125 
L 17.53125 0 
L 8.203125 0 
L 8.203125 68.796875 
L 37.15625 68.796875 
Q 43.265625 68.796875 47.796875 67.3125 
Q 52.34375 65.828125 55.375 63.109375 
Q 58.40625 60.40625 59.90625 56.5625 
Q 61.421875 52.734375 61.421875 48.09375 
z
M 52.046875 48 
Q 52.046875 54.546875 48.046875 57.9375 
Q 44.046875 61.328125 36.03125 61.328125 
L 17.53125 61.328125 
L 17.53125 34.1875 
L 36.421875 34.1875 
Q 44.484375 34.1875 48.265625 37.75 
Q 52.046875 41.3125 52.046875 48 
z
" id="LiberationSans-50"/>
      </defs>
      <g style="fill:#262626;" transform="translate(131.422269 177.363831)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-47"/>
       <use x="77.783203" xlink:href="#LiberationSans-44"/>
       <use x="150" xlink:href="#LiberationSans-50"/>
      </g>
     </g>
    </g>
    <g id="xtick_7">
     <g id="line2d_7"/>
     <g id="text_7">
      <!-- BMI_female -->
      <g style="fill:#262626;" transform="translate(139.598364 209.045081)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-42"/>
       <use x="66.699219" xlink:href="#LiberationSans-4d"/>
       <use x="150" xlink:href="#LiberationSans-49"/>
       <use x="177.783203" xlink:href="#LiberationSans-5f"/>
       <use x="233.398438" xlink:href="#LiberationSans-66"/>
       <use x="261.181641" xlink:href="#LiberationSans-65"/>
       <use x="316.796875" xlink:href="#LiberationSans-6d"/>
       <use x="400.097656" xlink:href="#LiberationSans-61"/>
       <use x="455.712891" xlink:href="#LiberationSans-6c"/>
       <use x="477.929688" xlink:href="#LiberationSans-65"/>
      </g>
     </g>
    </g>
    <g id="xtick_8">
     <g id="line2d_8"/>
     <g id="text_8">
      <!-- life -->
      <g style="fill:#262626;" transform="translate(147.774458 168.476331)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-6c"/>
       <use x="22.216797" xlink:href="#LiberationSans-69"/>
       <use x="44.433594" xlink:href="#LiberationSans-66"/>
       <use x="72.216797" xlink:href="#LiberationSans-65"/>
      </g>
     </g>
    </g>
    <g id="xtick_9">
     <g id="line2d_9"/>
     <g id="text_9">
      <!-- child_mortality -->
      <defs>
       <path d="M 13.421875 26.65625 
Q 13.421875 22.125 14.078125 18.3125 
Q 14.75 14.5 16.3125 11.734375 
Q 17.875 8.984375 20.4375 7.46875 
Q 23 5.953125 26.765625 5.953125 
Q 31.453125 5.953125 34.59375 8.484375 
Q 37.75 11.03125 38.484375 16.3125 
L 47.359375 15.71875 
Q 46.921875 12.453125 45.453125 9.421875 
Q 44 6.390625 41.484375 4.09375 
Q 38.96875 1.8125 35.34375 0.40625 
Q 31.734375 -0.984375 27 -0.984375 
Q 20.796875 -0.984375 16.453125 1.109375 
Q 12.109375 3.21875 9.390625 6.90625 
Q 6.6875 10.59375 5.46875 15.59375 
Q 4.25 20.609375 4.25 26.46875 
Q 4.25 31.78125 5.125 35.859375 
Q 6 39.9375 7.59375 42.984375 
Q 9.1875 46.046875 11.328125 48.125 
Q 13.484375 50.203125 15.984375 51.4375 
Q 18.5 52.6875 21.28125 53.25 
Q 24.078125 53.8125 26.90625 53.8125 
Q 31.34375 53.8125 34.8125 52.59375 
Q 38.28125 51.375 40.796875 49.25 
Q 43.3125 47.125 44.875 44.234375 
Q 46.4375 41.359375 47.078125 38.03125 
L 38.03125 37.359375 
Q 37.359375 41.75 34.5625 44.328125 
Q 31.78125 46.921875 26.65625 46.921875 
Q 22.90625 46.921875 20.390625 45.671875 
Q 17.875 44.4375 16.3125 41.921875 
Q 14.75 39.40625 14.078125 35.59375 
Q 13.421875 31.78125 13.421875 26.65625 
z
" id="LiberationSans-63"/>
       <path d="M 15.484375 43.796875 
Q 16.9375 46.484375 18.640625 48.359375 
Q 20.359375 50.25 22.40625 51.46875 
Q 24.46875 52.6875 26.90625 53.25 
Q 29.34375 53.8125 32.375 53.8125 
Q 37.453125 53.8125 40.703125 52.4375 
Q 43.953125 51.078125 45.828125 48.609375 
Q 47.703125 46.140625 48.40625 42.71875 
Q 49.125 39.3125 49.125 35.203125 
L 49.125 0 
L 40.28125 0 
L 40.28125 33.5 
Q 40.28125 36.859375 39.859375 39.390625 
Q 39.453125 41.9375 38.28125 43.625 
Q 37.109375 45.3125 34.953125 46.15625 
Q 32.8125 47.015625 29.390625 47.015625 
Q 26.265625 47.015625 23.75 45.890625 
Q 21.234375 44.78125 19.453125 42.71875 
Q 17.671875 40.671875 16.6875 37.734375 
Q 15.71875 34.8125 15.71875 31.15625 
L 15.71875 0 
L 6.9375 0 
L 6.9375 72.46875 
L 15.71875 72.46875 
L 15.71875 53.609375 
Q 15.71875 52 15.671875 50.390625 
Q 15.625 48.78125 15.546875 47.40625 
Q 15.484375 46.046875 15.421875 45.09375 
Q 15.375 44.140625 15.328125 43.796875 
z
" id="LiberationSans-68"/>
       <path d="M 40.09375 8.5 
Q 37.640625 3.421875 33.609375 1.21875 
Q 29.59375 -0.984375 23.640625 -0.984375 
Q 13.625 -0.984375 8.90625 5.75 
Q 4.203125 12.5 4.203125 26.171875 
Q 4.203125 53.8125 23.640625 53.8125 
Q 29.640625 53.8125 33.640625 51.609375 
Q 37.640625 49.421875 40.09375 44.625 
L 40.1875 44.625 
Q 40.1875 45.125 40.15625 46.171875 
Q 40.140625 47.21875 40.109375 48.359375 
Q 40.09375 49.515625 40.09375 50.53125 
Q 40.09375 51.5625 40.09375 52 
L 40.09375 72.46875 
L 48.875 72.46875 
L 48.875 10.890625 
Q 48.875 8.984375 48.890625 7.21875 
Q 48.921875 5.46875 48.96875 4 
Q 49.03125 2.546875 49.078125 1.484375 
Q 49.125 0.4375 49.171875 0 
L 40.765625 0 
Q 40.671875 0.484375 40.59375 1.390625 
Q 40.53125 2.296875 40.453125 3.46875 
Q 40.375 4.640625 40.328125 5.9375 
Q 40.28125 7.234375 40.28125 8.5 
z
M 13.421875 26.46875 
Q 13.421875 21 14.109375 17.09375 
Q 14.796875 13.1875 16.3125 10.671875 
Q 17.828125 8.15625 20.171875 6.984375 
Q 22.515625 5.8125 25.875 5.8125 
Q 29.34375 5.8125 32 6.9375 
Q 34.671875 8.0625 36.453125 10.578125 
Q 38.234375 13.09375 39.15625 17.140625 
Q 40.09375 21.1875 40.09375 27.046875 
Q 40.09375 32.671875 39.15625 36.546875 
Q 38.234375 40.4375 36.421875 42.828125 
Q 34.625 45.21875 32 46.265625 
Q 29.390625 47.3125 25.984375 47.3125 
Q 22.75 47.3125 20.40625 46.1875 
Q 18.0625 45.0625 16.5 42.578125 
Q 14.9375 40.09375 14.171875 36.109375 
Q 13.421875 32.125 13.421875 26.46875 
z
" id="LiberationSans-64"/>
      </defs>
      <g style="fill:#262626;" transform="translate(155.950553 219.601331)rotate(-90)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-63"/>
       <use x="50" xlink:href="#LiberationSans-68"/>
       <use x="105.615234" xlink:href="#LiberationSans-69"/>
       <use x="127.832031" xlink:href="#LiberationSans-6c"/>
       <use x="150.048828" xlink:href="#LiberationSans-64"/>
       <use x="205.664062" xlink:href="#LiberationSans-5f"/>
       <use x="261.279297" xlink:href="#LiberationSans-6d"/>
       <use x="344.580078" xlink:href="#LiberationSans-6f"/>
       <use x="400.195312" xlink:href="#LiberationSans-72"/>
       <use x="433.496094" xlink:href="#LiberationSans-74"/>
       <use x="461.279297" xlink:href="#LiberationSans-61"/>
       <use x="516.894531" xlink:href="#LiberationSans-6c"/>
       <use x="539.111328" xlink:href="#LiberationSans-69"/>
       <use x="561.328125" xlink:href="#LiberationSans-74"/>
       <use x="589.111328" xlink:href="#LiberationSans-79"/>
      </g>
     </g>
    </g>
   </g>
   <g id="matplotlib.axis_2">
    <g id="ytick_1">
     <g id="line2d_10"/>
     <g id="text_10">
      <!-- child_mortality -->
      <g style="fill:#262626;" transform="translate(12.96 148.228909)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-63"/>
       <use x="50" xlink:href="#LiberationSans-68"/>
       <use x="105.615234" xlink:href="#LiberationSans-69"/>
       <use x="127.832031" xlink:href="#LiberationSans-6c"/>
       <use x="150.048828" xlink:href="#LiberationSans-64"/>
       <use x="205.664062" xlink:href="#LiberationSans-5f"/>
       <use x="261.279297" xlink:href="#LiberationSans-6d"/>
       <use x="344.580078" xlink:href="#LiberationSans-6f"/>
       <use x="400.195312" xlink:href="#LiberationSans-72"/>
       <use x="433.496094" xlink:href="#LiberationSans-74"/>
       <use x="461.279297" xlink:href="#LiberationSans-61"/>
       <use x="516.894531" xlink:href="#LiberationSans-6c"/>
       <use x="539.111328" xlink:href="#LiberationSans-69"/>
       <use x="561.328125" xlink:href="#LiberationSans-74"/>
       <use x="589.111328" xlink:href="#LiberationSans-79"/>
      </g>
     </g>
    </g>
    <g id="ytick_2">
     <g id="line2d_11"/>
     <g id="text_11">
      <!-- life -->
      <g style="fill:#262626;" transform="translate(64.085 140.052815)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-6c"/>
       <use x="22.216797" xlink:href="#LiberationSans-69"/>
       <use x="44.433594" xlink:href="#LiberationSans-66"/>
       <use x="72.216797" xlink:href="#LiberationSans-65"/>
      </g>
     </g>
    </g>
    <g id="ytick_3">
     <g id="line2d_12"/>
     <g id="text_12">
      <!-- BMI_female -->
      <g style="fill:#262626;" transform="translate(23.51625 131.87672)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-42"/>
       <use x="66.699219" xlink:href="#LiberationSans-4d"/>
       <use x="150" xlink:href="#LiberationSans-49"/>
       <use x="177.783203" xlink:href="#LiberationSans-5f"/>
       <use x="233.398438" xlink:href="#LiberationSans-66"/>
       <use x="261.181641" xlink:href="#LiberationSans-65"/>
       <use x="316.796875" xlink:href="#LiberationSans-6d"/>
       <use x="400.097656" xlink:href="#LiberationSans-61"/>
       <use x="455.712891" xlink:href="#LiberationSans-6c"/>
       <use x="477.929688" xlink:href="#LiberationSans-65"/>
      </g>
     </g>
    </g>
    <g id="ytick_4">
     <g id="line2d_13"/>
     <g id="text_13">
      <!-- GDP -->
      <g style="fill:#262626;" transform="translate(55.1975 123.700626)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-47"/>
       <use x="77.783203" xlink:href="#LiberationSans-44"/>
       <use x="150" xlink:href="#LiberationSans-50"/>
      </g>
     </g>
    </g>
    <g id="ytick_5">
     <g id="line2d_14"/>
     <g id="text_14">
      <!-- BMI_male -->
      <g style="fill:#262626;" transform="translate(31.855313 115.524531)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-42"/>
       <use x="66.699219" xlink:href="#LiberationSans-4d"/>
       <use x="150" xlink:href="#LiberationSans-49"/>
       <use x="177.783203" xlink:href="#LiberationSans-5f"/>
       <use x="233.398438" xlink:href="#LiberationSans-6d"/>
       <use x="316.699219" xlink:href="#LiberationSans-61"/>
       <use x="372.314453" xlink:href="#LiberationSans-6c"/>
       <use x="394.53125" xlink:href="#LiberationSans-65"/>
      </g>
     </g>
    </g>
    <g id="ytick_6">
     <g id="line2d_15"/>
     <g id="text_15">
      <!-- CO2 -->
      <g style="fill:#262626;" transform="translate(56.306875 107.348437)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-43"/>
       <use x="72.216797" xlink:href="#LiberationSans-4f"/>
       <use x="150" xlink:href="#LiberationSans-32"/>
      </g>
     </g>
    </g>
    <g id="ytick_7">
     <g id="line2d_16"/>
     <g id="text_16">
      <!-- HIV -->
      <g style="fill:#262626;" transform="translate(60.1975 99.172342)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-48"/>
       <use x="72.216797" xlink:href="#LiberationSans-49"/>
       <use x="100" xlink:href="#LiberationSans-56"/>
      </g>
     </g>
    </g>
    <g id="ytick_8">
     <g id="line2d_17"/>
     <g id="text_17">
      <!-- fertility -->
      <g style="fill:#262626;" transform="translate(47.977188 90.996248)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-66"/>
       <use x="27.783203" xlink:href="#LiberationSans-65"/>
       <use x="83.398438" xlink:href="#LiberationSans-72"/>
       <use x="116.699219" xlink:href="#LiberationSans-74"/>
       <use x="144.482422" xlink:href="#LiberationSans-69"/>
       <use x="166.699219" xlink:href="#LiberationSans-6c"/>
       <use x="188.916016" xlink:href="#LiberationSans-69"/>
       <use x="211.132812" xlink:href="#LiberationSans-74"/>
       <use x="238.916016" xlink:href="#LiberationSans-79"/>
      </g>
     </g>
    </g>
    <g id="ytick_9">
     <g id="line2d_18"/>
     <g id="text_18">
      <!-- population -->
      <g style="fill:#262626;" transform="translate(30.719375 82.820153)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-70"/>
       <use x="55.615234" xlink:href="#LiberationSans-6f"/>
       <use x="111.230469" xlink:href="#LiberationSans-70"/>
       <use x="166.845703" xlink:href="#LiberationSans-75"/>
       <use x="222.460938" xlink:href="#LiberationSans-6c"/>
       <use x="244.677734" xlink:href="#LiberationSans-61"/>
       <use x="300.292969" xlink:href="#LiberationSans-74"/>
       <use x="328.076172" xlink:href="#LiberationSans-69"/>
       <use x="350.292969" xlink:href="#LiberationSans-6f"/>
       <use x="405.908203" xlink:href="#LiberationSans-6e"/>
      </g>
     </g>
    </g>
   </g>
   <g id="QuadMesh_1">
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 148.693519 
L 92.043907 148.693519 
L 92.043907 140.517424 
L 83.867813 140.517424 
L 83.867813 148.693519 
" style="fill:#fdfebc;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 148.693519 
L 100.220001 148.693519 
L 100.220001 140.517424 
L 92.043907 140.517424 
L 92.043907 148.693519 
" style="fill:#0c7f43;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 148.693519 
L 108.396096 148.693519 
L 108.396096 140.517424 
L 100.220001 140.517424 
L 100.220001 148.693519 
" style="fill:#a7d96b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 148.693519 
L 116.57219 148.693519 
L 116.57219 140.517424 
L 108.396096 140.517424 
L 108.396096 148.693519 
" style="fill:#fa9b58;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 148.693519 
L 124.748285 148.693519 
L 124.748285 140.517424 
L 116.57219 140.517424 
L 116.57219 148.693519 
" style="fill:#de402e;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 148.693519 
L 132.924379 148.693519 
L 132.924379 140.517424 
L 124.748285 140.517424 
L 124.748285 148.693519 
" style="fill:#f67f4b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 148.693519 
L 141.100474 148.693519 
L 141.100474 140.517424 
L 132.924379 140.517424 
L 132.924379 148.693519 
" style="fill:#6bbf64;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 148.693519 
L 149.276568 148.693519 
L 149.276568 140.517424 
L 141.100474 140.517424 
L 141.100474 148.693519 
" style="fill:#c41e27;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 148.693519 
L 157.452662 148.693519 
L 157.452662 140.517424 
L 149.276568 140.517424 
L 149.276568 148.693519 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 140.517424 
L 92.043907 140.517424 
L 92.043907 132.34133 
L 83.867813 132.34133 
L 83.867813 140.517424 
" style="fill:#fffebe;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 140.517424 
L 100.220001 140.517424 
L 100.220001 132.34133 
L 92.043907 132.34133 
L 92.043907 140.517424 
" style="fill:#d93429;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 140.517424 
L 108.396096 140.517424 
L 108.396096 132.34133 
L 100.220001 132.34133 
L 100.220001 140.517424 
" style="fill:#f36b42;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 140.517424 
L 116.57219 140.517424 
L 116.57219 132.34133 
L 108.396096 132.34133 
L 108.396096 140.517424 
" style="fill:#8ccd67;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 140.517424 
L 124.748285 140.517424 
L 124.748285 132.34133 
L 116.57219 132.34133 
L 116.57219 140.517424 
" style="fill:#33a456;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 140.517424 
L 132.924379 140.517424 
L 132.924379 132.34133 
L 124.748285 132.34133 
L 124.748285 140.517424 
" style="fill:#5ab760;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 140.517424 
L 141.100474 140.517424 
L 141.100474 132.34133 
L 132.924379 132.34133 
L 132.924379 140.517424 
" style="fill:#eb5a3a;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 140.517424 
L 149.276568 140.517424 
L 149.276568 132.34133 
L 141.100474 132.34133 
L 141.100474 140.517424 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 140.517424 
L 157.452662 140.517424 
L 157.452662 132.34133 
L 149.276568 132.34133 
L 149.276568 140.517424 
" style="fill:#c41e27;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 132.34133 
L 92.043907 132.34133 
L 92.043907 124.165235 
L 83.867813 124.165235 
L 83.867813 132.34133 
" style="fill:#feea9b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 132.34133 
L 100.220001 132.34133 
L 100.220001 124.165235 
L 92.043907 124.165235 
L 92.043907 132.34133 
" style="fill:#7dc765;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 132.34133 
L 108.396096 132.34133 
L 108.396096 124.165235 
L 100.220001 124.165235 
L 100.220001 132.34133 
" style="fill:#a7d96b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 132.34133 
L 116.57219 132.34133 
L 116.57219 124.165235 
L 108.396096 124.165235 
L 108.396096 132.34133 
" style="fill:#fecc7b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 132.34133 
L 124.748285 132.34133 
L 124.748285 124.165235 
L 116.57219 124.165235 
L 116.57219 132.34133 
" style="fill:#fb9d59;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 132.34133 
L 132.924379 132.34133 
L 132.924379 124.165235 
L 124.748285 124.165235 
L 124.748285 132.34133 
" style="fill:#fdad60;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 132.34133 
L 141.100474 132.34133 
L 141.100474 124.165235 
L 132.924379 124.165235 
L 132.924379 132.34133 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 132.34133 
L 149.276568 132.34133 
L 149.276568 124.165235 
L 141.100474 124.165235 
L 141.100474 132.34133 
" style="fill:#eb5a3a;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 132.34133 
L 157.452662 132.34133 
L 157.452662 124.165235 
L 149.276568 124.165235 
L 149.276568 132.34133 
" style="fill:#6bbf64;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 124.165235 
L 92.043907 124.165235 
L 92.043907 115.989141 
L 83.867813 115.989141 
L 83.867813 124.165235 
" style="fill:#fff8b4;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 124.165235 
L 100.220001 124.165235 
L 100.220001 115.989141 
L 92.043907 115.989141 
L 92.043907 124.165235 
" style="fill:#f88c51;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 124.165235 
L 108.396096 124.165235 
L 108.396096 115.989141 
L 100.220001 115.989141 
L 100.220001 124.165235 
" style="fill:#fede89;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 124.165235 
L 116.57219 124.165235 
L 116.57219 115.989141 
L 108.396096 115.989141 
L 108.396096 124.165235 
" style="fill:#16914d;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 124.165235 
L 124.748285 124.165235 
L 124.748285 115.989141 
L 116.57219 115.989141 
L 116.57219 124.165235 
" style="fill:#5ab760;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 124.165235 
L 132.924379 124.165235 
L 132.924379 115.989141 
L 124.748285 115.989141 
L 124.748285 124.165235 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 124.165235 
L 141.100474 124.165235 
L 141.100474 115.989141 
L 132.924379 115.989141 
L 132.924379 124.165235 
" style="fill:#fdad60;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 124.165235 
L 149.276568 124.165235 
L 149.276568 115.989141 
L 141.100474 115.989141 
L 141.100474 124.165235 
" style="fill:#5ab760;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 124.165235 
L 157.452662 124.165235 
L 157.452662 115.989141 
L 149.276568 115.989141 
L 149.276568 124.165235 
" style="fill:#f67f4b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 115.989141 
L 92.043907 115.989141 
L 92.043907 107.813047 
L 83.867813 107.813047 
L 83.867813 115.989141 
" style="fill:#feea9b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 115.989141 
L 100.220001 115.989141 
L 100.220001 107.813047 
L 92.043907 107.813047 
L 92.043907 115.989141 
" style="fill:#e34933;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 115.989141 
L 108.396096 115.989141 
L 108.396096 107.813047 
L 100.220001 107.813047 
L 100.220001 115.989141 
" style="fill:#fdc574;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 115.989141 
L 116.57219 115.989141 
L 116.57219 107.813047 
L 108.396096 107.813047 
L 108.396096 115.989141 
" style="fill:#6bbf64;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 115.989141 
L 124.748285 115.989141 
L 124.748285 107.813047 
L 116.57219 107.813047 
L 116.57219 115.989141 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 115.989141 
L 132.924379 115.989141 
L 132.924379 107.813047 
L 124.748285 107.813047 
L 124.748285 115.989141 
" style="fill:#5ab760;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 115.989141 
L 141.100474 115.989141 
L 141.100474 107.813047 
L 132.924379 107.813047 
L 132.924379 115.989141 
" style="fill:#fb9d59;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 115.989141 
L 149.276568 115.989141 
L 149.276568 107.813047 
L 141.100474 107.813047 
L 141.100474 115.989141 
" style="fill:#33a456;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 115.989141 
L 157.452662 115.989141 
L 157.452662 107.813047 
L 149.276568 107.813047 
L 149.276568 115.989141 
" style="fill:#de402e;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 107.813047 
L 92.043907 107.813047 
L 92.043907 99.636952 
L 83.867813 99.636952 
L 83.867813 107.813047 
" style="fill:#fffdbc;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 107.813047 
L 100.220001 107.813047 
L 100.220001 99.636952 
L 92.043907 99.636952 
L 92.043907 107.813047 
" style="fill:#fba05b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 107.813047 
L 108.396096 107.813047 
L 108.396096 99.636952 
L 100.220001 99.636952 
L 100.220001 107.813047 
" style="fill:#fee593;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 107.813047 
L 116.57219 107.813047 
L 116.57219 99.636952 
L 108.396096 99.636952 
L 108.396096 107.813047 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 107.813047 
L 124.748285 107.813047 
L 124.748285 99.636952 
L 116.57219 99.636952 
L 116.57219 107.813047 
" style="fill:#6bbf64;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 107.813047 
L 132.924379 107.813047 
L 132.924379 99.636952 
L 124.748285 99.636952 
L 124.748285 107.813047 
" style="fill:#16914d;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 107.813047 
L 141.100474 107.813047 
L 141.100474 99.636952 
L 132.924379 99.636952 
L 132.924379 107.813047 
" style="fill:#fecc7b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 107.813047 
L 149.276568 107.813047 
L 149.276568 99.636952 
L 141.100474 99.636952 
L 141.100474 107.813047 
" style="fill:#8ccd67;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 107.813047 
L 157.452662 107.813047 
L 157.452662 99.636952 
L 149.276568 99.636952 
L 149.276568 107.813047 
" style="fill:#fa9b58;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 99.636952 
L 92.043907 99.636952 
L 92.043907 91.460858 
L 83.867813 91.460858 
L 83.867813 99.636952 
" style="fill:#fff5ae;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 99.636952 
L 100.220001 99.636952 
L 100.220001 91.460858 
L 92.043907 91.460858 
L 92.043907 99.636952 
" style="fill:#c1e57b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 99.636952 
L 108.396096 99.636952 
L 108.396096 91.460858 
L 100.220001 91.460858 
L 100.220001 99.636952 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 99.636952 
L 116.57219 99.636952 
L 116.57219 91.460858 
L 108.396096 91.460858 
L 108.396096 99.636952 
" style="fill:#fee593;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 99.636952 
L 124.748285 99.636952 
L 124.748285 91.460858 
L 116.57219 91.460858 
L 116.57219 99.636952 
" style="fill:#fdc574;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 99.636952 
L 132.924379 99.636952 
L 132.924379 91.460858 
L 124.748285 91.460858 
L 124.748285 99.636952 
" style="fill:#fede89;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 99.636952 
L 141.100474 99.636952 
L 141.100474 91.460858 
L 132.924379 91.460858 
L 132.924379 99.636952 
" style="fill:#a7d96b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 99.636952 
L 149.276568 99.636952 
L 149.276568 91.460858 
L 141.100474 91.460858 
L 141.100474 99.636952 
" style="fill:#f36b42;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 99.636952 
L 157.452662 99.636952 
L 157.452662 91.460858 
L 149.276568 91.460858 
L 149.276568 99.636952 
" style="fill:#a7d96b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 91.460858 
L 92.043907 91.460858 
L 92.043907 83.284763 
L 83.867813 83.284763 
L 83.867813 91.460858 
" style="fill:#fff6b0;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 91.460858 
L 100.220001 91.460858 
L 100.220001 83.284763 
L 92.043907 83.284763 
L 92.043907 91.460858 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 91.460858 
L 108.396096 91.460858 
L 108.396096 83.284763 
L 100.220001 83.284763 
L 100.220001 91.460858 
" style="fill:#c1e57b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 91.460858 
L 116.57219 91.460858 
L 116.57219 83.284763 
L 108.396096 83.284763 
L 108.396096 91.460858 
" style="fill:#fba05b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 91.460858 
L 124.748285 91.460858 
L 124.748285 83.284763 
L 116.57219 83.284763 
L 116.57219 91.460858 
" style="fill:#e34933;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 91.460858 
L 132.924379 91.460858 
L 132.924379 83.284763 
L 124.748285 83.284763 
L 124.748285 91.460858 
" style="fill:#f88c51;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 91.460858 
L 141.100474 91.460858 
L 141.100474 83.284763 
L 132.924379 83.284763 
L 132.924379 91.460858 
" style="fill:#7dc765;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 91.460858 
L 149.276568 91.460858 
L 149.276568 83.284763 
L 141.100474 83.284763 
L 141.100474 91.460858 
" style="fill:#d93429;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 91.460858 
L 157.452662 91.460858 
L 157.452662 83.284763 
L 149.276568 83.284763 
L 149.276568 91.460858 
" style="fill:#0c7f43;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 83.867813 83.284763 
L 92.043907 83.284763 
L 92.043907 75.108669 
L 83.867813 75.108669 
L 83.867813 83.284763 
" style="fill:#006837;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 92.043907 83.284763 
L 100.220001 83.284763 
L 100.220001 75.108669 
L 92.043907 75.108669 
L 92.043907 83.284763 
" style="fill:#fff6b0;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 100.220001 83.284763 
L 108.396096 83.284763 
L 108.396096 75.108669 
L 100.220001 75.108669 
L 100.220001 83.284763 
" style="fill:#fff5ae;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 108.396096 83.284763 
L 116.57219 83.284763 
L 116.57219 75.108669 
L 108.396096 75.108669 
L 108.396096 83.284763 
" style="fill:#fffdbc;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 116.57219 83.284763 
L 124.748285 83.284763 
L 124.748285 75.108669 
L 116.57219 75.108669 
L 116.57219 83.284763 
" style="fill:#feea9b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 124.748285 83.284763 
L 132.924379 83.284763 
L 132.924379 75.108669 
L 124.748285 75.108669 
L 124.748285 83.284763 
" style="fill:#fff8b4;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 132.924379 83.284763 
L 141.100474 83.284763 
L 141.100474 75.108669 
L 132.924379 75.108669 
L 132.924379 83.284763 
" style="fill:#feea9b;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 141.100474 83.284763 
L 149.276568 83.284763 
L 149.276568 75.108669 
L 141.100474 75.108669 
L 141.100474 83.284763 
" style="fill:#fffebe;"/>
    <path clip-path="url(#pfc422ec6b9)" d="M 149.276568 83.284763 
L 157.452662 83.284763 
L 157.452662 75.108669 
L 149.276568 75.108669 
L 149.276568 83.284763 
" style="fill:#fdfebc;"/>
   </g>
  </g>
  <g id="axes_2">
   <g id="patch_3">
    <path clip-path="url(#p827af6e9c4)" d="M 162.051716 210.842187 
L 162.051716 210.06921 
L 162.051716 13.732977 
L 162.051716 12.96 
L 171.945825 12.96 
L 171.945825 13.732977 
L 171.945825 210.06921 
L 171.945825 210.842187 
z
" style="fill:#eaeaf2;stroke:#eaeaf2;stroke-linejoin:miter;stroke-width:0.01;"/>
   </g>
   <g id="matplotlib.axis_3"/>
   <g id="matplotlib.axis_4">
    <g id="ytick_10">
     <g id="line2d_19"/>
     <g id="text_19">
      <!-- −0.8 -->
      <defs>
       <path d="M 4.9375 29.6875 
L 4.9375 36.8125 
L 53.515625 36.8125 
L 53.515625 29.6875 
z
" id="LiberationSans-2212"/>
       <path d="M 51.703125 34.421875 
Q 51.703125 24.515625 49.828125 17.75 
Q 47.953125 10.984375 44.703125 6.8125 
Q 41.453125 2.640625 37.0625 0.828125 
Q 32.671875 -0.984375 27.6875 -0.984375 
Q 22.65625 -0.984375 18.3125 0.828125 
Q 13.96875 2.640625 10.765625 6.78125 
Q 7.5625 10.9375 5.734375 17.703125 
Q 3.90625 24.46875 3.90625 34.421875 
Q 3.90625 44.828125 5.734375 51.640625 
Q 7.5625 58.453125 10.78125 62.5 
Q 14.015625 66.546875 18.40625 68.1875 
Q 22.796875 69.828125 27.984375 69.828125 
Q 32.90625 69.828125 37.21875 68.1875 
Q 41.546875 66.546875 44.765625 62.5 
Q 48 58.453125 49.84375 51.640625 
Q 51.703125 44.828125 51.703125 34.421875 
z
M 42.78125 34.421875 
Q 42.78125 42.625 41.796875 48.0625 
Q 40.828125 53.515625 38.921875 56.765625 
Q 37.015625 60.015625 34.25 61.359375 
Q 31.5 62.703125 27.984375 62.703125 
Q 24.265625 62.703125 21.4375 61.328125 
Q 18.609375 59.96875 16.671875 56.71875 
Q 14.75 53.46875 13.765625 48.015625 
Q 12.796875 42.578125 12.796875 34.421875 
Q 12.796875 26.515625 13.796875 21.09375 
Q 14.796875 15.671875 16.71875 12.375 
Q 18.65625 9.078125 21.4375 7.640625 
Q 24.21875 6.203125 27.78125 6.203125 
Q 31.25 6.203125 34.03125 7.640625 
Q 36.8125 9.078125 38.734375 12.375 
Q 40.671875 15.671875 41.71875 21.09375 
Q 42.78125 26.515625 42.78125 34.421875 
z
" id="LiberationSans-30"/>
       <path d="M 9.125 0 
L 9.125 10.6875 
L 18.65625 10.6875 
L 18.65625 0 
z
" id="LiberationSans-2e"/>
       <path d="M 51.265625 19.1875 
Q 51.265625 14.796875 49.875 11.109375 
Q 48.484375 7.421875 45.625 4.734375 
Q 42.78125 2.046875 38.328125 0.53125 
Q 33.890625 -0.984375 27.828125 -0.984375 
Q 21.78125 -0.984375 17.359375 0.53125 
Q 12.9375 2.046875 10.03125 4.703125 
Q 7.125 7.375 5.734375 11.0625 
Q 4.34375 14.75 4.34375 19.09375 
Q 4.34375 22.859375 5.484375 25.78125 
Q 6.640625 28.71875 8.5625 30.828125 
Q 10.5 32.953125 12.96875 34.25 
Q 15.4375 35.546875 18.0625 35.984375 
L 18.0625 36.1875 
Q 15.1875 36.859375 12.90625 38.375 
Q 10.640625 39.890625 9.09375 42.015625 
Q 7.5625 44.140625 6.75 46.71875 
Q 5.953125 49.3125 5.953125 52.203125 
Q 5.953125 55.8125 7.34375 59 
Q 8.734375 62.203125 11.46875 64.625 
Q 14.203125 67.046875 18.25 68.4375 
Q 22.3125 69.828125 27.640625 69.828125 
Q 33.25 69.828125 37.375 68.40625 
Q 41.5 67 44.203125 64.578125 
Q 46.921875 62.15625 48.234375 58.9375 
Q 49.5625 55.71875 49.5625 52.09375 
Q 49.5625 49.265625 48.75 46.671875 
Q 47.953125 44.09375 46.40625 41.96875 
Q 44.875 39.84375 42.59375 38.34375 
Q 40.328125 36.859375 37.359375 36.28125 
L 37.359375 36.078125 
Q 40.328125 35.59375 42.859375 34.296875 
Q 45.40625 33.015625 47.265625 30.890625 
Q 49.125 28.765625 50.1875 25.828125 
Q 51.265625 22.90625 51.265625 19.1875 
z
M 40.4375 51.609375 
Q 40.4375 54.203125 39.765625 56.34375 
Q 39.109375 58.5 37.59375 60.03125 
Q 36.078125 61.578125 33.640625 62.421875 
Q 31.203125 63.28125 27.640625 63.28125 
Q 24.171875 63.28125 21.78125 62.421875 
Q 19.390625 61.578125 17.84375 60.03125 
Q 16.3125 58.5 15.625 56.34375 
Q 14.9375 54.203125 14.9375 51.609375 
Q 14.9375 49.5625 15.46875 47.40625 
Q 16.015625 45.265625 17.421875 43.5 
Q 18.84375 41.75 21.328125 40.625 
Q 23.828125 39.5 27.734375 39.5 
Q 31.890625 39.5 34.40625 40.625 
Q 36.921875 41.75 38.25 43.5 
Q 39.59375 45.265625 40.015625 47.40625 
Q 40.4375 49.5625 40.4375 51.609375 
z
M 42.140625 20.015625 
Q 42.140625 22.515625 41.453125 24.828125 
Q 40.765625 27.15625 39.109375 28.9375 
Q 37.453125 30.71875 34.640625 31.8125 
Q 31.84375 32.90625 27.640625 32.90625 
Q 23.78125 32.90625 21.0625 31.8125 
Q 18.359375 30.71875 16.671875 28.90625 
Q 14.984375 27.09375 14.203125 24.71875 
Q 13.421875 22.359375 13.421875 19.828125 
Q 13.421875 16.65625 14.203125 14.03125 
Q 14.984375 11.421875 16.6875 9.546875 
Q 18.40625 7.671875 21.1875 6.640625 
Q 23.96875 5.609375 27.9375 5.609375 
Q 31.9375 5.609375 34.671875 6.640625 
Q 37.40625 7.671875 39.0625 9.546875 
Q 40.71875 11.421875 41.421875 14.078125 
Q 42.140625 16.75 42.140625 20.015625 
z
" id="LiberationSans-38"/>
      </defs>
      <g style="fill:#262626;" transform="translate(178.945825 194.677406)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-2212"/>
       <use x="58.398438" xlink:href="#LiberationSans-30"/>
       <use x="114.013672" xlink:href="#LiberationSans-2e"/>
       <use x="141.796875" xlink:href="#LiberationSans-38"/>
      </g>
     </g>
    </g>
    <g id="ytick_11">
     <g id="line2d_20"/>
     <g id="text_20">
      <!-- −0.4 -->
      <defs>
       <path d="M 43.015625 15.578125 
L 43.015625 0 
L 34.71875 0 
L 34.71875 15.578125 
L 2.296875 15.578125 
L 2.296875 22.40625 
L 33.796875 68.796875 
L 43.015625 68.796875 
L 43.015625 22.515625 
L 52.6875 22.515625 
L 52.6875 15.578125 
z
M 34.71875 58.890625 
Q 34.625 58.640625 34.234375 57.9375 
Q 33.84375 57.234375 33.34375 56.34375 
Q 32.859375 55.46875 32.34375 54.5625 
Q 31.84375 53.65625 31.453125 53.078125 
L 13.8125 27.09375 
Q 13.578125 26.703125 13.109375 26.0625 
Q 12.640625 25.4375 12.15625 24.78125 
Q 11.671875 24.125 11.171875 23.484375 
Q 10.6875 22.859375 10.40625 22.515625 
L 34.71875 22.515625 
z
" id="LiberationSans-34"/>
      </defs>
      <g style="fill:#262626;" transform="translate(178.945825 155.100969)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-2212"/>
       <use x="58.398438" xlink:href="#LiberationSans-30"/>
       <use x="114.013672" xlink:href="#LiberationSans-2e"/>
       <use x="141.796875" xlink:href="#LiberationSans-34"/>
      </g>
     </g>
    </g>
    <g id="ytick_12">
     <g id="line2d_21"/>
     <g id="text_21">
      <!-- 0.0 -->
      <g style="fill:#262626;" transform="translate(178.945825 115.524531)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-30"/>
       <use x="55.615234" xlink:href="#LiberationSans-2e"/>
       <use x="83.398438" xlink:href="#LiberationSans-30"/>
      </g>
     </g>
    </g>
    <g id="ytick_13">
     <g id="line2d_22"/>
     <g id="text_22">
      <!-- 0.4 -->
      <g style="fill:#262626;" transform="translate(178.945825 75.948094)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-30"/>
       <use x="55.615234" xlink:href="#LiberationSans-2e"/>
       <use x="83.398438" xlink:href="#LiberationSans-34"/>
      </g>
     </g>
    </g>
    <g id="ytick_14">
     <g id="line2d_23"/>
     <g id="text_23">
      <!-- 0.8 -->
      <g style="fill:#262626;" transform="translate(178.945825 36.371656)scale(0.1 -0.1)">
       <use xlink:href="#LiberationSans-30"/>
       <use x="55.615234" xlink:href="#LiberationSans-2e"/>
       <use x="83.398438" xlink:href="#LiberationSans-38"/>
      </g>
     </g>
    </g>
   </g>
   <image height="198" id="imaged8a9271dc3" transform="scale(1 -1)translate(0 -198)" width="10" x="162" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAoAAADGCAYAAADrGBFyAAAABHNCSVQICAgIfAhkiAAAATlJREFUWIXtmFEKwzAMQ+0k3VF29d1xbZxd4RVkTMr2LfRsS21h/mnvZeA3WiOyFOHodehRhu7dq9A7nKewFK3RZA5mOZw6Oi4FReNl8IztYNHwZfh5qLBvkIzDQt5IJmFG6Gg4a9oekydjCVvLHV8PSoaj+XnouyfhPDAZo881vyP8Dt9w5DNSIfxgV57HfYfzyGdMSEYeYacNd/2MT0LLZ/R2qNFYqC/FkyIMCyhcULio41yXGk2FeOsZcEa8NV5mrhOi5Vtjx4QI5cnot75iQvRCf5vZCKNC6pgwIz4P7iMX7nAeil516LqtJ9PZOAOjoeOkM56sjhnooGg8I0fLk8FZczS0TEDri4uTmTTrL1QmoLEjn7EQjUtBsy5E3ziPvrj0jUvRizrqhUEjTEBfdei6rf/nkQh/8k8Uw68nkEoAAAAASUVORK5CYII=" y="-13"/>
   <g id="patch_4">
    <path d="M 162.051716 210.842187 
L 162.051716 210.06921 
L 162.051716 13.732977 
L 162.051716 12.96 
L 171.945825 12.96 
L 171.945825 13.732977 
L 171.945825 210.06921 
L 171.945825 210.842187 
z
" style="fill:none;"/>
   </g>
  </g>
 </g>
 <defs>
  <clipPath id="pfc422ec6b9">
   <rect height="73.58485" width="73.58485" x="83.867813" y="75.108669"/>
  </clipPath>
  <clipPath id="p827af6e9c4">
   <rect height="197.882187" width="9.894109" x="162.051716" y="12.96"/>
  </clipPath>
 </defs>
</svg>
)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139 entries, 0 to 138\n",
      "Data columns (total 9 columns):\n",
      "population         139 non-null float64\n",
      "fertility          139 non-null float64\n",
      "HIV                139 non-null float64\n",
      "CO2                139 non-null float64\n",
      "BMI_male           139 non-null float64\n",
      "GDP                139 non-null int64\n",
      "BMI_female         139 non-null float64\n",
      "life               139 non-null float64\n",
      "child_mortality    139 non-null float64\n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 9.9 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.60287769784175"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "np.mean(df.life)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"dc-completed__body\">\n",
    "\n",
    "<div class=\"dc-completed__status\">\n",
    "\n",
    "<div class=\"dc-exercise-completed all-xp\">+50 XP</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"dc-completed__message\">\n",
    "\n",
    "<div>\n",
    "\n",
    "Good job! As seen by using `df.info()`, `fertility`, along with all the other columns, is of type `float64`, **not** `int64`.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"dc-completed__continue\">\n",
    "\n",
    "Press enter to\n",
    "\n",
    "<div tabindex=\"-1\">[Continue](/courses/supervised-learning-with-scikit-learn/regression-2?ex=5)</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"listview__section\" data-onboarding=\"assignment\">\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div class=\"exercise--assignment exercise--typography\">\n",
    "\n",
    "# Fit & predict for regression\n",
    "\n",
    "<div>\n",
    "\n",
    "<div>\n",
    "\n",
    "Now, you will fit a linear regression and predict life expectancy using just one feature. You saw Andy do this earlier using the `'RM'` feature of the Boston housing dataset. In this exercise, you will use the `'fertility'` feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is `'life'`. The array for the target variable has been pre-loaded as `y` and the array for `'fertility'` has been pre-loaded as `X_fertility`.\n",
    "\n",
    "A scatter plot with `'fertility'` on the x-axis and `'life'` on the y-axis has been generated. As you can see, there is a strongly negative correlation, so a linear regression should be able to capture this trend. Your job is to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. You will also compute and print the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.231em, 1001.23em, 2.407em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 1.178em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.154em, 1000.75em, 4.169em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: MathJax_Math-italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span><span style=\"position: absolute; top: -4.378em; left: 0.751em;\"><span class=\"mn\" id=\"MathJax-Span-5\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></span><msup><mi>R</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-1\">R^2</script> score using sckit-learn's `.score()` method.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__section\" data-onboarding=\"instructions\" style=\"min-height: calc(100% - 33px);\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div role=\"button\" class=\"listview__header\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "<span class=\"tag tag--xp\">100 XP</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div data-onboarding=\"instructions\" class=\"exercise--instructions exercise--typography\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"exercise--instructions__content\">\n",
    "\n",
    "*   Import `LinearRegression` from `sklearn.linear_model`.\n",
    "*   Create a `LinearRegression` regressor called `reg`.\n",
    "*   Set up the prediction space to range from the minimum to the maximum of `X_fertility`. This has been done for you.\n",
    "*   Fit the regressor to the data (`X_fertility` and `y`) and compute its predictions using the `.predict()` method and the `prediction_space` array.\n",
    "*   Compute and print the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-6\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.231em, 1001.23em, 2.407em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-7\"><span class=\"msubsup\" id=\"MathJax-Span-8\"><span style=\"display: inline-block; position: relative; width: 1.178em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.154em, 1000.75em, 4.169em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-family: MathJax_Math-italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span><span style=\"position: absolute; top: -4.378em; left: 0.751em;\"><span class=\"mn\" id=\"MathJax-Span-10\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></span><msup><mi>R</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-2\">R^2</script> score using the `.score()` method.\n",
    "*   Overlay the plot with your linear regression line. This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"campus-dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-hint\" currentitem=\"true\" style=\"display: inline-block;\">\n",
    "\n",
    "<div class=\"__react_component_tooltip place-right type-dark tooltip top\" data-id=\"tooltip\" style=\"left: 180px; top: 669px;\">\n",
    "\n",
    "<div class=\"tooltip-inner\">Ctrl+H</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "[<span>Take Hint (-30 XP)</span>](javascript:void(0))</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6192442167740035\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set default Seaborn style\n",
    "sns.set()\n",
    "\n",
    "X_fertility = np.array([[ 2.73],[ 6.43],[ 2.24],[ 1.4 ],[ 1.96],[ 1.41],[ 1.99],[ 1.89],[ 2.38],[ 1.83],[ 1.42],[ 1.82],[ 2.91],[ 5.27],[ 2.51],[ 3.48],[ 2.86],[ 1.9 ],[ 1.43],[ 6.04],[ 6.48],[ 3.05],[ 5.17],[ 1.68],[ 6.81],[ 1.89],[ 2.43],[ 5.05],[ 5.1 ],[ 1.91],[ 4.91],[ 1.43],[ 1.5 ],[ 1.89],[ 3.76],[ 2.73],[ 2.95],[ 2.32],[ 5.31],[ 5.16],[ 1.62],[ 2.74],[ 1.85],[ 1.97],[ 4.28],[ 5.8 ],[ 1.79],[ 1.37],[ 4.19],[ 1.46],[ 4.12],[ 5.34],[ 5.25],[ 2.74],[ 3.5 ],[ 3.27],[ 1.33],[ 2.12],[ 2.64],[ 2.48],[ 1.88],[ 2.  ],[ 2.92],[ 1.39],[ 2.39],[ 1.34],[ 2.51],[ 4.76],[ 1.5 ],[ 1.57],[ 3.34],[ 5.19],[ 1.42],[ 1.63],[ 4.79],[ 5.78],[ 2.05],[ 2.38],[ 6.82],[ 1.38],[ 4.94],[ 1.58],[ 2.35],[ 1.49],[ 2.37],[ 2.44],[ 5.54],[ 2.05],[ 2.9 ],[ 1.77],[ 2.12],[ 2.72],[ 7.59],[ 6.02],[ 1.96],[ 2.89],[ 3.58],[ 2.61],[ 4.07],[ 3.06],[ 2.58],[ 3.26],[ 1.33],[ 1.36],[ 2.2 ],[ 1.34],[ 1.49],[ 5.06],[ 5.11],[ 1.41],[ 5.13],[ 1.28],[ 1.31],[ 1.43],[ 7.06],[ 2.54],[ 1.42],[ 2.32],[ 4.79],[ 2.41],[ 3.7 ],[ 1.92],[ 1.47],[ 3.7 ],[ 5.54],[ 1.48],[ 4.88],[ 1.8 ],[ 2.04],[ 2.15],[ 6.34],[ 1.38],[ 1.87],[ 2.07],[ 2.11],[ 2.46],[ 1.86],[ 5.88],[ 3.85]])\n",
    "plt.scatter(X_fertility, df.life)\n",
    "plt.xlabel('Fertility')\n",
    "plt.ylabel('Life Expectancy')\n",
    "\n",
    "# Create the regressor: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Create the prediction space\n",
    "prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n",
    "\n",
    "# Fit the model to the data\n",
    "reg.fit(X_fertility, y)\n",
    "\n",
    "\n",
    "# Compute predictions over the prediction space: y_pred\n",
    "y_pred = reg.predict(prediction_space)\n",
    "\n",
    "# Print R^2 \n",
    "print(reg.score(X_fertility, y))\n",
    "\n",
    "# Plot regression line\n",
    "plt.plot(prediction_space, y_pred, color='black', linewidth=3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"listview__section\" data-onboarding=\"assignment\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div aria-hidden=\"true\" role=\"button\" class=\"listview__header\" style=\"position: absolute; top: 0px; width: calc(100% - 23px);\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div class=\"exercise--assignment exercise--typography\">\n",
    "\n",
    "# Train/test split for regression\n",
    "\n",
    "<div>\n",
    "\n",
    "<div>\n",
    "\n",
    "As you learned in Chapter 1, train and test sets are vital to ensure that your supervised learning model is able to generalize well to new data. This was true for classification models, and is equally true for linear regression models.\n",
    "\n",
    "In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over **all** features. In addition to computing the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-11\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.231em, 1001.23em, 2.407em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-12\"><span class=\"msubsup\" id=\"MathJax-Span-13\"><span style=\"display: inline-block; position: relative; width: 1.178em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.154em, 1000.75em, 4.169em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-14\" style=\"font-family: MathJax_Math-italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span><span style=\"position: absolute; top: -4.378em; left: 0.751em;\"><span class=\"mn\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></span><msup><mi>R</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-3\">R^2</script> score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. The feature array `X` and target variable array `y` have been pre-loaded for you from the DataFrame `df`.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__section\" data-onboarding=\"instructions\" style=\"min-height: calc(100% - 33px);\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div role=\"button\" class=\"listview__header\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "<span class=\"tag tag--xp\">100 XP</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div data-onboarding=\"instructions\" class=\"exercise--instructions exercise--typography\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"exercise--instructions__content\">\n",
    "\n",
    "*   Import `LinearRegression` from `sklearn.linear_model`, `mean_squared_error` from `sklearn.metrics`, and `train_test_split` from `sklearn.model_selection`.\n",
    "*   Using `X` and `y`, create training and test sets such that 30% is used for testing and 70% for training. Use a random state of `42`.\n",
    "*   Create a linear regression regressor called `reg_all`, fit it to the training set, and evaluate it on the test set.\n",
    "*   Compute and print the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-16\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.231em, 1001.23em, 2.407em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"msubsup\" id=\"MathJax-Span-18\"><span style=\"display: inline-block; position: relative; width: 1.178em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.154em, 1000.75em, 4.169em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-19\" style=\"font-family: MathJax_Math-italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span><span style=\"position: absolute; top: -4.378em; left: 0.751em;\"><span class=\"mn\" id=\"MathJax-Span-20\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></span><msup><mi>R</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-4\">R^2</script> score using the `.score()` method on the test set.\n",
    "*   Compute and print the RMSE. To do this, first compute the Mean Squared Error using the `mean_squared_error()` function with the arguments `y_test` and `y_pred`, and then take its square root using `np.sqrt()`.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"campus-dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-hint\" currentitem=\"true\" style=\"display: inline-block;\">\n",
    "\n",
    "<div class=\"__react_component_tooltip place-right type-dark tooltip top\" data-id=\"tooltip\" style=\"left: 180px; top: 702px;\">\n",
    "\n",
    "<div class=\"tooltip-inner\">Ctrl+H</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "[<span>Take Hint (-30 XP)</span>](javascript:void(0))</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7298987360907498\n",
      "Root Mean Squared Error: 4.194027914110239\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "# Create the regressor: reg_all\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "reg_all.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data: y_pred\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"listview__section\" data-onboarding=\"assignment\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div aria-hidden=\"true\" role=\"button\" class=\"listview__header\" style=\"position: absolute; top: 0px; width: calc(100% - 23px);\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div class=\"exercise--assignment exercise--typography\">\n",
    "\n",
    "# 5-fold cross-validation\n",
    "\n",
    "<div>\n",
    "\n",
    "<div>\n",
    "\n",
    "Cross-validation is a vital step in evaluating a model. It maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data.\n",
    "\n",
    "In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn's `cross_val_score()` function uses <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-41\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.231em, 1001.23em, 2.407em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"msubsup\" id=\"MathJax-Span-43\"><span style=\"display: inline-block; position: relative; width: 1.178em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.154em, 1000.75em, 4.169em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: MathJax_Math-italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span><span style=\"position: absolute; top: -4.378em; left: 0.751em;\"><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></span><msup><mi>R</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-9\">R^2</script> as the metric of choice for regression. Since you are performing 5-fold cross-validation, the function will return 5 scores. Your job is to compute these 5 scores and then take their average.\n",
    "\n",
    "The DataFrame has been loaded as `df` and split into the feature/target variable arrays `X` and `y`. The modules `pandas` and `numpy` have been imported as `pd` and `np`, respectively.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__section\" data-onboarding=\"instructions\" style=\"min-height: calc(100% - 33px);\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div role=\"button\" class=\"listview__header\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "<span class=\"tag tag--xp\">100 XP</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div data-onboarding=\"instructions\" class=\"exercise--instructions exercise--typography\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"exercise--instructions__content\">\n",
    "\n",
    "*   Import `LinearRegression` from `sklearn.linear_model` and `cross_val_score` from `sklearn.model_selection`.\n",
    "*   Create a linear regression regressor called `reg`.\n",
    "*   Use the `cross_val_score()` function to perform 5-fold cross-validation on `X` and `y`.\n",
    "*   Compute and print the average cross-validation score. You can use NumPy's `mean()` function to compute the average.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"campus-dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-hint\" currentitem=\"true\" style=\"display: inline-block;\">\n",
    "\n",
    "<div class=\"__react_component_tooltip place-right type-dark tooltip top\" data-id=\"tooltip\" style=\"left: 180px; top: 495px;\">\n",
    "\n",
    "<div class=\"tooltip-inner\">Ctrl+H</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "[<span>Take Hint (-30 XP)</span>](javascript:void(0))</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71001079 0.75007717 0.55271526 0.547501   0.52410561]\n",
      "Average 5-Fold CV Score: 0.6168819644425119\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression object: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(reg, X, y, cv=5)\n",
    "\n",
    "# Print the 5-fold cross-validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"listview__section\" data-onboarding=\"assignment\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div aria-hidden=\"true\" role=\"button\" class=\"listview__header\" style=\"position: absolute; top: 0px; width: calc(100% - 23px);\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div class=\"exercise--assignment exercise--typography\">\n",
    "\n",
    "# K-Fold CV comparison\n",
    "\n",
    "<div>\n",
    "\n",
    "<div>\n",
    "\n",
    "Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset.\n",
    "\n",
    "In the IPython Shell, you can use `%timeit` to see how long each 3-fold CV takes compared to 10-fold CV by executing the following `cv=3` and `cv=10`:\n",
    "\n",
    "    %timeit cross_val_score(reg, X, y, cv = ____)\n",
    "\n",
    "`pandas` and `numpy` are available in the workspace as `pd` and `np`. The DataFrame has been loaded as `df` and the feature/target variable arrays `X` and `y` have been created.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__section\" data-onboarding=\"instructions\" style=\"min-height: calc(100% - 33px);\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div role=\"button\" class=\"listview__header\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "<span class=\"tag tag--xp\">100 XP</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div data-onboarding=\"instructions\" class=\"exercise--instructions exercise--typography\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"exercise--instructions__content\">\n",
    "\n",
    "*   Import `LinearRegression` from `sklearn.linear_model` and `cross_val_score` from `sklearn.model_selection`.\n",
    "*   Create a linear regression regressor called `reg`.\n",
    "*   Perform 3-fold CV and then 10-fold CV. Compare the resulting mean scores.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"campus-dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-hint\" currentitem=\"true\" style=\"display: inline-block;\">\n",
    "\n",
    "<div class=\"__react_component_tooltip place-right type-dark tooltip top\" data-id=\"tooltip\" style=\"left: 180px; top: 360px;\">\n",
    "\n",
    "<div class=\"tooltip-inner\">Ctrl+H</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "[<span>Take Hint (-30 XP)</span>](javascript:void(0))</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6294715754653507\n",
      "0.5883937741571186\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Create a linear regression object: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Perform 3-fold CV\n",
    "cvscores_3 = cross_val_score(reg, X, y, cv=3)\n",
    "print(np.mean(cvscores_3))\n",
    "\n",
    "# Perform 10-fold CV\n",
    "cvscores_10 = cross_val_score(reg, X, y, cv=10)\n",
    "print(np.mean(cvscores_10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"listview__section\" data-onboarding=\"assignment\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div aria-hidden=\"true\" role=\"button\" class=\"listview__header\" style=\"position: absolute; top: 0px; width: calc(100% - 23px);\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div class=\"exercise--assignment exercise--typography\">\n",
    "\n",
    "# Regularization I: Lasso\n",
    "\n",
    "<div>\n",
    "\n",
    "<div>\n",
    "\n",
    "In the video, you saw how Lasso selected out the `'RM'` feature as being the most important for predicting Boston house prices, while shrinking the coefficients of certain other features to 0\\. Its ability to perform feature selection in this way becomes even more useful when you are dealing with data involving thousands of features.\n",
    "\n",
    "In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining.\n",
    "\n",
    "The feature and target variable arrays have been pre-loaded as `X` and `y`.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__section\" data-onboarding=\"instructions\" style=\"min-height: calc(100% - 33px);\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div role=\"button\" class=\"listview__header\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "<span class=\"tag tag--xp\">100 XP</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div data-onboarding=\"instructions\" class=\"exercise--instructions exercise--typography\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"exercise--instructions__content\">\n",
    "\n",
    "*   Import `Lasso` from `sklearn.linear_model`.\n",
    "*   Instantiate a Lasso regressor with an alpha of `0.4` and specify `normalize=True`.\n",
    "*   Fit the regressor to the data and compute the coefficients using the `coef_` attribute.\n",
    "*   Plot the coefficients on the y-axis and column names on the x-axis. This has been done for you, so hit 'Submit Answer' to view the plot!\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"campus-dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-hint\" currentitem=\"true\" style=\"display: inline-block;\">\n",
    "\n",
    "<div class=\"__react_component_tooltip place-right type-dark tooltip top\" data-id=\"tooltip\" style=\"left: 180px; top: 471px;\">\n",
    "\n",
    "<div class=\"tooltip-inner\">Ctrl+H</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "[<span>Take Hint (-30 XP)</span>](javascript:void(0))</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.51388042]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (8,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-b40318a8d87c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Plot the coefficients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlasso_coef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmargins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                       mplDeprecation)\n\u001b[0;32m   3362\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1865\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1867\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1526\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1528\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1529\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 242\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (8,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "df_columns = ['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n",
    "       'BMI_female', 'child_mortality']\n",
    "# Instantiate a lasso regressor: lasso\n",
    "lasso = Lasso(alpha=0.4, normalize=True)\n",
    "\n",
    "# Fit the regressor to the data\n",
    "lasso.fit(X,y,)\n",
    "\n",
    "# Compute and print the coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "print(lasso_coef)\n",
    "\n",
    "# Plot the coefficients\n",
    "plt.plot(range(len(df_columns)), lasso_coef)\n",
    "plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\n",
    "plt.margins(0.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"listview__section\" data-onboarding=\"assignment\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div aria-hidden=\"true\" role=\"button\" class=\"listview__header\" style=\"position: absolute; top: 0px; width: calc(100% - 23px);\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div class=\"exercise--assignment exercise--typography\">\n",
    "\n",
    "# Regularization II: Ridge\n",
    "\n",
    "<div>\n",
    "\n",
    "<div>\n",
    "\n",
    "Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice.\n",
    "\n",
    "Recall that lasso performs regularization by adding to the loss function a penalty term of the _absolute_ value of each coefficient multiplied by some alpha. This is also known as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.445em, 1001.18em, 2.46em, -999.997em); top: -2.294em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: MathJax_Math-italic;\">L</span><span class=\"mn\" id=\"MathJax-Span-4\" style=\"font-family: MathJax_Main;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.3em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mn>1</mn></math></span><mi>L</mi><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-1\">L1</script> regularization because the regularization term is the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-5\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.445em, 1001.18em, 2.46em, -999.997em); top: -2.294em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: MathJax_Math-italic;\">L</span><span class=\"mn\" id=\"MathJax-Span-8\" style=\"font-family: MathJax_Main;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.3em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mn>1</mn></math></span><mi>L</mi><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-2\">L1</script> norm of the coefficients. This is not the only way to regularize, however.\n",
    "\n",
    "If instead you took the sum of the _squared_ values of the coefficients multiplied by some alpha - like in Ridge regression - you would be computing the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-9\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.445em, 1001.18em, 2.46em, -999.997em); top: -2.294em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: MathJax_Math-italic;\">L</span><span class=\"mn\" id=\"MathJax-Span-12\" style=\"font-family: MathJax_Main;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.3em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mn>2</mn></math></span><mi>L</mi><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-3\">L2</script> norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.231em, 1001.23em, 2.407em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"msubsup\" id=\"MathJax-Span-15\"><span style=\"display: inline-block; position: relative; width: 1.178em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.154em, 1000.75em, 4.169em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-family: MathJax_Math-italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span><span style=\"position: absolute; top: -4.378em; left: 0.751em;\"><span class=\"mn\" id=\"MathJax-Span-17\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></span><msup><mi>R</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-4\">R^2</script> scores for each, using this function that we have defined for you, which plots the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-18\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.231em, 1001.23em, 2.407em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-19\"><span class=\"msubsup\" id=\"MathJax-Span-20\"><span style=\"display: inline-block; position: relative; width: 1.178em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.154em, 1000.75em, 4.169em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-family: MathJax_Math-italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span><span style=\"position: absolute; top: -4.378em; left: 0.751em;\"><span class=\"mn\" id=\"MathJax-Span-22\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></span><msup><mi>R</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-5\">R^2</script> score as well as standard error for each alpha:\n",
    "\n",
    "    def display_plot(cv_scores, cv_scores_std):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.plot(alpha_space, cv_scores)\n",
    "\n",
    "        std_error = cv_scores_std / np.sqrt(10)\n",
    "\n",
    "        ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
    "        ax.set_ylabel('CV Score +/- Std Error')\n",
    "        ax.set_xlabel('Alpha')\n",
    "        ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "        ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "        ax.set_xscale('log')\n",
    "        plt.show()\n",
    "\n",
    "Don't worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-23\" style=\"width: 1.445em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.231em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(1.231em, 1001.23em, 2.407em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-24\"><span class=\"msubsup\" id=\"MathJax-Span-25\"><span style=\"display: inline-block; position: relative; width: 1.178em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.154em, 1000.75em, 4.169em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: MathJax_Math-italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span><span style=\"position: absolute; top: -4.378em; left: 0.751em;\"><span class=\"mn\" id=\"MathJax-Span-27\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></span><msup><mi>R</mi><mn>2</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"></span><script type=\"math/tex\" id=\"MathJax-Element-6\">R^2</script> score varies with different alphas, and to understand the importance of selecting the right value for alpha. You'll learn how to tune alpha in the next chapter.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__section\" data-onboarding=\"instructions\" style=\"min-height: calc(100% - 33px);\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div role=\"button\" class=\"listview__header\">\n",
    "\n",
    "<div class=\"exercise--sidebar-header\">\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "<span class=\"tag tag--xp\">100 XP</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div aria-hidden=\"true\" role=\"button\" class=\"listview__header\" style=\"position: absolute; bottom: 0px; width: calc(100% - 23px);\">\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"listview__content\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"\">\n",
    "\n",
    "<div data-onboarding=\"instructions\" class=\"exercise--instructions exercise--typography\">\n",
    "\n",
    "<div>\n",
    "\n",
    "<div class=\"exercise--instructions__content\">\n",
    "\n",
    "*   Instantiate a Ridge regressor and specify `normalize=True`.\n",
    "*   Inside the `for` loop:\n",
    "    *   Specify the alpha value for the regressor to use.\n",
    "    *   Perform 10-fold cross-validation on the regressor with the specified alpha. The data is available in the arrays `X` and `y`.\n",
    "    *   Append the average and the standard deviation of the computed cross-validated scores. NumPy has been pre-imported for you as `np`.\n",
    "*   Use the `display_plot()` function to visualize the scores and standard deviations.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"campus-dc-sct-feedback\" tabindex=\"-1\">\n",
    "\n",
    "<div data-tip=\"true\" data-for=\"tp-hint\" currentitem=\"true\" style=\"display: inline-block;\">\n",
    "\n",
    "<div class=\"__react_component_tooltip place-right type-dark tooltip top\" data-id=\"tooltip\" style=\"left: 180px; top: 389px;\">\n",
    "\n",
    "<div class=\"tooltip-inner\">Ctrl+H</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "[<span>Take Hint (-30 XP)</span>](javascript:void(0))</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use this function to create a plot    \n",
    "def display_plot(cv_scores, cv_scores_std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(alpha_space, cv_scores)\n",
    "\n",
    "    std_error = cv_scores_std / np.sqrt(10)\n",
    "\n",
    "    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
    "    ax.set_ylabel('CV Score +/- Std Error')\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "    ax.set_xscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "# Setup the array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-4, 0, 50)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
    "\n",
    "# Display the plot\n",
    "display_plot(ridge_scores, ridge_scores_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`In [9]:`\n",
    "\n",
    "<pre>\n",
    "\n",
    "<div class=\"geshifilter\">\n",
    "\n",
    "<div class=\"python geshifilter-python\"><span class=\"kw1\">def</span> <span class=\"kw3\">test</span><span class=\"br0\">(</span>x<span class=\"br0\">)</span>:  \n",
    "\n",
    "   <span class=\"kw1\">return</span> x*<span class=\"nu0\">2</span>  \n",
    "\n",
    "<span class=\"kw1\">print</span><span class=\"br0\">(</span><span class=\"kw3\">inspect</span>.<span class=\"me1\">getsource</span><span class=\"br0\">(</span><span class=\"kw3\">test</span><span class=\"br0\">)</span><span class=\"br0\">)</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</pre>\n",
    "\n",
    "`def test(x): return x*2`\n",
    "\n",
    "`In [10]:`\n",
    "\n",
    "<pre>\n",
    "\n",
    "<div class=\"geshifilter\">\n",
    "\n",
    "<div class=\"python geshifilter-python\"><span class=\"kw1\">print</span><span class=\"br0\">(</span><span class=\"kw3\">inspect</span>.<span class=\"me1\">getsourcefile</span><span class=\"br0\">(</span><span class=\"kw3\">test</span><span class=\"br0\">)</span><span class=\"br0\">)</span></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</pre>\n",
    "\n",
    "`<ipython-input-9-70ac3e17460c>`\n",
    "\n",
    "`In [11]:`\n",
    "\n",
    "<pre><span class=\"geshifilter\">`<span class=\"kw1\">print</span><span class=\"br0\">(</span><span class=\"kw3\">inspect</span>.<span class=\"me1\">getsourcelines</span><span class=\"br0\">(</span><span class=\"kw3\">test</span><span class=\"br0\">)</span><span class=\"br0\">)</span>`</span></pre>\n",
    "\n",
    "`(['def test(x):\\n', ' return x*2\\n'], 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\"><head><link href=\"/favicon.ico\" rel=\"icon\" type=\"image/png\"/><link href=\"/static/css/main.83227c16.css\" rel=\"stylesheet\"/><title data-react-helmet=\"true\">Cross-validation | Python</title><link data-react-helmet=\"true\" href=\"data:image/png;base64,bW9kdWxlLmV4cG9ydHMgPSBfX3dlYnBhY2tfcHVibGljX3BhdGhfXyArICIvc3RhdGljL21lZGlhL2FwcGxlLWljb24uNGZhMTNiMGYucG5nIjs=\" rel=\"apple-touch-icon\"/><link data-react-helmet=\"true\" href=\"data:image/png;base64,bW9kdWxlLmV4cG9ydHMgPSBfX3dlYnBhY2tfcHVibGljX3BhdGhfXyArICIvc3RhdGljL21lZGlhL2FwcGxlLWljb24uNGZhMTNiMGYucG5nIjs=\" rel=\"apple-touch-icon-precomposed\"/><meta charset=\"utf-8\" data-react-helmet=\"true\"/><meta content=\"IE=edge,chrome=1\" data-react-helmet=\"true\" http-equiv=\"X-UA-Compatible\"/><meta content=\"width=device-width, initial-scale=1, maximum-scale=1\" data-react-helmet=\"true\" name=\"viewport\"/><meta content=\"!\" data-react-helmet=\"true\" name=\"fragment\"/><meta content=\"R, Python, Data analysis, interactive, learning\" data-react-helmet=\"true\" name=\"keywords\"/><meta content=\"Here is an example of Cross-validation: .\" data-react-helmet=\"true\" name=\"description\"/><meta content=\"summary\" data-react-helmet=\"true\" name=\"twitter:card\"/><meta content=\"@DataCamp\" data-react-helmet=\"true\" name=\"twitter:site\"/><meta content=\"Cross-validation | Python\" data-react-helmet=\"true\" name=\"twitter:title\"/><meta content=\"Here is an example of Cross-validation: .\" data-react-helmet=\"true\" name=\"twitter:description\"/><meta content=\"@DataCamp\" data-react-helmet=\"true\" name=\"twitter:creator\"/><meta content=\"/public/assets/images/var/twitter_share.png\" data-react-helmet=\"true\" name=\"twitter:image:src\"/><meta content=\"www.datacamp.com\" data-react-helmet=\"true\" name=\"twitter:domain\"/><meta content=\"Cross-validation | Python\" data-react-helmet=\"true\" property=\"og:title\"/><meta content=\"/public/assets/images/var/linkedin_share.png\" data-react-helmet=\"true\" property=\"og:image\"/><meta content=\"892114885437-01a7plbsu1b2vobuhvnckmmanhb58h3a.apps.googleusercontent.com\" data-react-helmet=\"true\" name=\"google-signin-clientid\"/><meta content=\"email profile\" data-react-helmet=\"true\" name=\"google-signin-scope\"/><meta content=\"single_host_origin\" data-react-helmet=\"true\" name=\"google-signin-cookiepolicy\"/><script data-react-helmet=\"true\" src=\"//use.typekit.net/pjc4zfc.js\"></script><script charset=\"UTF-8\" data-react-helmet=\"true\">try{Typekit.load();}catch(e){}</script></head><body><script>window.PRELOADED_STATE = \"[&quot;~#iM&quot;,[&quot;preFetchedData&quot;,[&quot;^0&quot;,[&quot;course&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,1939,&quot;title&quot;,&quot;Supervised Learning with scikit-learn&quot;,&quot;description&quot;,&quot;At the end of day, the value of Data Scientists rests on their ability to describe the world and to make predictions. Machine Learning is the field of teaching machines and computers to learn from existing data to make predictions on new data - will a given tumor be benign or malignant? Which of your customers will take their business elsewhere? Is a particular email spam or not? In this course, you&#39;ll learn how to use Python to perform supervised learning, an essential component of Machine Learning. You&#39;ll learn how to build predictive models, how to tune their parameters and how to tell how well they will perform on unseen data, all the while using real world datasets. You&#39;ll do so using scikit-learn, one of the most popular and user-friendly machine learning libraries for Python.&quot;,&quot;short_description&quot;,&quot;Learn how to build and tune predictive models and evaluate how well they will perform on unseen data.&quot;,&quot;author_field&quot;,null,&quot;author_bio&quot;,null,&quot;author_image&quot;,&quot;placeholder.png&quot;,&quot;nb_of_subscriptions&quot;,82755,&quot;slug&quot;,&quot;supervised-learning-with-scikit-learn&quot;,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1939/shields/thumb/shield_image_course_1939_20180618-12-158gfc9?1529338815&quot;,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1939/shields/thumb_home/shield_image_course_1939_20180618-12-158gfc9?1529338815&quot;,&quot;last_updated_on&quot;,&quot;06/09/2018&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/supervised-learning-with-scikit-learn&quot;,&quot;should_cache&quot;,true,&quot;type&quot;,&quot;datacamp&quot;,&quot;difficulty_level&quot;,1,&quot;state&quot;,&quot;live&quot;,&quot;university&quot;,null,&quot;sharing_links&quot;,[&quot;^ &quot;,&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;],&quot;programming_language&quot;,&quot;python&quot;,&quot;paid&quot;,true,&quot;time_needed&quot;,null,&quot;xp&quot;,4300,&quot;topic_id&quot;,6,&quot;reduced_outline&quot;,null,&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;chapters&quot;,[[&quot;^ &quot;,&quot;id&quot;,5232,&quot;title_meta&quot;,null,&quot;^1&quot;,&quot;Classification&quot;,&quot;^2&quot;,&quot;In this chapter, you will be introduced to classification problems and learn how to solve them using supervised learning techniques. Classification problems are prevalent in a variety of domains, ranging from finance to healthcare. Here, you will have the chance to apply what you are learning to a political dataset, where you classify the party affiliation of United States Congressmen based on their voting records. &quot;,&quot;number&quot;,1,&quot;^8&quot;,&quot;classification&quot;,&quot;nb_exercises&quot;,12,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;17/07/2018&quot;,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_4172/slides/ch1_slides.pdf&quot;,&quot;free_preview&quot;,true,&quot;xp&quot;,900],[&quot;^ &quot;,&quot;id&quot;,5233,&quot;^M&quot;,null,&quot;^1&quot;,&quot;Regression&quot;,&quot;^2&quot;,&quot;In the previous chapter, you made use of image and political datasets to predict binary as well as multiclass outcomes. But what if your problem requires a continuous outcome? Regression, which is the focus of this chapter, is best suited to solving such problems. You will learn about fundamental concepts in regression and apply them to predict the life expectancy in a given country using Gapminder data. &quot;,&quot;^N&quot;,2,&quot;^8&quot;,&quot;regression-2&quot;,&quot;^O&quot;,13,&quot;^P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;17/07/2018&quot;,&quot;^R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_4172/slides/ch2_slides.pdf&quot;,&quot;^S&quot;,null,&quot;xp&quot;,1000],[&quot;^ &quot;,&quot;id&quot;,5234,&quot;^M&quot;,null,&quot;^1&quot;,&quot;Fine-tuning your model&quot;,&quot;^2&quot;,&quot;Having trained your model, your next task is to evaluate its performance. What metrics can you use to gauge how good your model is? So far, you have used accuracy for classification and R-squared for regression. In this chapter, you will learn about some of the other metrics available in  scikit-learn that will allow you to assess your model&#39;s performance in a more nuanced manner. You will then learn to optimize both your classification as well as regression models using hyperparameter tuning. &quot;,&quot;^N&quot;,3,&quot;^8&quot;,&quot;fine-tuning-your-model&quot;,&quot;^O&quot;,15,&quot;^P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;17/07/2018&quot;,&quot;^R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_3894/slides/ch3_slides.pdf&quot;,&quot;^S&quot;,null,&quot;xp&quot;,1200],[&quot;^ &quot;,&quot;id&quot;,5235,&quot;^M&quot;,null,&quot;^1&quot;,&quot;Preprocessing and pipelines&quot;,&quot;^2&quot;,&quot;This chapter will introduce the notion of pipelines and how scikit-learn allows for transformers and estimators to be chained together and used as a single unit. Pre-processing techniques will be then be introduced as a way to enhance model performance and pipelines will be  the glue that ties together concepts in the prior chapters.&quot;,&quot;^N&quot;,4,&quot;^8&quot;,&quot;preprocessing-and-pipelines&quot;,&quot;^O&quot;,14,&quot;^P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;17/07/2018&quot;,&quot;^R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_3327/slides/ch4_slides.pdf&quot;,&quot;^S&quot;,null,&quot;xp&quot;,1200]]]]],&quot;chapter&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,5233,&quot;^M&quot;,null,&quot;^1&quot;,&quot;Regression&quot;,&quot;^2&quot;,&quot;In the previous chapter, you made use of image and political datasets to predict binary as well as multiclass outcomes. But what if your problem requires a continuous outcome? Regression, which is the focus of this chapter, is best suited to solving such problems. You will learn about fundamental concepts in regression and apply them to predict the life expectancy in a given country using Gapminder data. &quot;,&quot;^N&quot;,2,&quot;^8&quot;,&quot;regression-2&quot;,&quot;^O&quot;,13,&quot;^P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;17/07/2018&quot;,&quot;^R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_4172/slides/ch2_slides.pdf&quot;,&quot;^S&quot;,null,&quot;xp&quot;,1000]]],&quot;exercises&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[[&quot;^ &quot;,&quot;id&quot;,55928,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;assignment&quot;,null,&quot;^1&quot;,&quot;Introduction to regression&quot;,&quot;sample_code&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;^N&quot;,1,&quot;sct&quot;,&quot;&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;attachments&quot;,null,&quot;xp&quot;,50,&quot;possible_answers&quot;,[],&quot;feedbacks&quot;,[],&quot;question&quot;,&quot;&quot;,&quot;video_link&quot;,null,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1939_supervised_learning_w_scikit_learn/v8/hls-ch2_1.master.m3u8&quot;,&quot;aspect_ratio&quot;,56.25,&quot;projector_key&quot;,&quot;course_1939_c4f0a9bf726ed46d3a5baff1ba8c64e7&quot;,&quot;language&quot;,&quot;python&quot;,&quot;randomNumber&quot;,0.06627007370500415],[&quot;^ &quot;,&quot;id&quot;,55929,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Andy introduced regression to you using the Boston housing dataset. But regression models can be used in a variety of contexts to solve a variety of different problems.&lt;/p&gt;\\\\n&lt;p&gt;Given below are four example applications of machine learning. Your job is to pick the one that is &lt;em&gt;best&lt;/em&gt; framed as a &lt;strong&gt;regression&lt;/strong&gt; problem.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Which of the following is a regression problem?&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,2,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,&quot;&lt;p&gt;In regression problems, the target variable takes on continuous values (such as the price of a house), unlike in classification problems, where the target variable falls into discrete categories (such as the integers 0 through 9 in the MNIST dataset).&lt;/p&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[&quot;An e-commerce company using labeled customer data to predict whether or not a customer will purchase a particular item.&quot;,&quot;A healthcare company using data about cancer tumors (such as their geometric measurements) to predict whether a new tumor is benign or malignant.&quot;,&quot;A restaurant using review data to ascribe positive or negative sentiment to a given review.&quot;,&quot;[A bike share company using time and weather data to predict the number of bikes being rented at any given hour.]&quot;],&quot;^10&quot;,[&quot;Incorrect. There are only two outcomes here: Either the customer will purchase the item, or they will not. This is a classification task.&quot;,&quot;Incorrect. There are only two outcomes here: Either the tumor is benign, or it is malignant. This is a classification task.&quot;,&quot;Incorrect. The target variable here is the sentiment of a review: It can be either positive or negative. This is not a task suited to regression.&quot;,&quot;Great work! The target variable here - the number of bike rentals at any given hour - is quantitative, so this is best framed as a regression problem.&quot;],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.7121569409537483],[&quot;^ &quot;,&quot;id&quot;,55930,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;In this chapter, you will work with &lt;a href=\\\\&quot;https://www.gapminder.org/data/\\\\&quot;&gt;Gapminder&lt;/a&gt; data that we have consolidated into one CSV file available in the workspace as &lt;code&gt;&#39;gapminder.csv&#39;&lt;/code&gt;. Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country&#39;s GDP, fertility rate, and population. As in Chapter 1, the dataset has been preprocessed.&lt;/p&gt;\\\\n&lt;p&gt;Since the target variable here is quantitative, this is a regression problem. To begin, you will fit a linear regression with just one feature: &lt;code&gt;&#39;fertility&#39;&lt;/code&gt;, which is the average number of children a woman in a given country gives birth to. In later exercises, you will use all the features to build regression models.&lt;/p&gt;\\\\n&lt;p&gt;Before that, however, you need to import the data and get it into the form needed by scikit-learn. This involves creating feature and target variable arrays. Furthermore, since you are going to use only one feature to begin with, you need to do some reshaping using NumPy&#39;s &lt;code&gt;.reshape()&lt;/code&gt; method. Don&#39;t worry too much about this reshaping right now, but it is something you will have to do occasionally when working with scikit-learn so it is useful to practice.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Importing data for supervised learning&quot;,&quot;^U&quot;,&quot;# Import numpy and pandas\\\\nimport numpy as np\\\\nimport pandas as pd\\\\n\\\\n# Read the CSV file into a DataFrame: df\\\\ndf = ____\\\\n\\\\n# Create arrays for features and target variable\\\\ny = ____\\\\nX = ____\\\\n\\\\n# Print the dimensions of X and y before reshaping\\\\nprint(\\\\&quot;Dimensions of y before reshaping: {}\\\\&quot;.format(y.shape))\\\\nprint(\\\\&quot;Dimensions of X before reshaping: {}\\\\&quot;.format(X.shape))\\\\n\\\\n# Reshape X and y\\\\ny = ____\\\\nX = ____\\\\n\\\\n# Print the dimensions of X and y after reshaping\\\\nprint(\\\\&quot;Dimensions of y after reshaping: {}\\\\&quot;.format(y.shape))\\\\nprint(\\\\&quot;Dimensions of X after reshaping: {}\\\\&quot;.format(X.shape))\\\\n&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;pandas&lt;/code&gt; as their standard aliases.&lt;/li&gt;\\\\n&lt;li&gt;Read the file &lt;code&gt;&#39;gapminder.csv&#39;&lt;/code&gt; into a DataFrame &lt;code&gt;df&lt;/code&gt; using the &lt;code&gt;read_csv()&lt;/code&gt; function.&lt;/li&gt;\\\\n&lt;li&gt;Create array &lt;code&gt;X&lt;/code&gt; for the &lt;code&gt;&#39;fertility&#39;&lt;/code&gt; feature  and array &lt;code&gt;y&lt;/code&gt; for the &lt;code&gt;&#39;life&#39;&lt;/code&gt; target variable.&lt;/li&gt;\\\\n&lt;li&gt;Reshape the arrays by using the &lt;code&gt;.reshape()&lt;/code&gt; method and passing in &lt;code&gt;-1&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,3,&quot;sct&quot;,&quot;Ex().test_import(&#39;numpy&#39;)\\\\nEx().test_import(&#39;pandas&#39;)\\\\n\\\\nEx().test_function(&#39;pandas.read_csv&#39;)\\\\nEx().test_object(&#39;df&#39;)\\\\n\\\\nEx().test_object(&#39;y&#39;)\\\\nEx().test_object(&#39;X&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\nEx().test_function(&#39;print&#39;, index=2)\\\\n\\\\nEx().test_function(&#39;y.reshape&#39;)\\\\nEx().test_function(&#39;X.reshape&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=3)\\\\nEx().test_function(&#39;print&#39;, index=4)\\\\n\\\\nsuccess_msg(\\\\&quot;Great work! Notice the differences in shape before and after applying the `.reshape()` method. Getting the feature and target variable arrays into the right format for scikit-learn is an important precursor to model building.\\\\&quot;)\\\\n&quot;,&quot;^W&quot;,&quot;fn = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;\\\\n\\\\nfrom urllib.request import urlretrieve\\\\nurlretrieve(fn, &#39;gapminder.csv&#39;)&quot;,&quot;^X&quot;,&quot;# Import numpy and pandas\\\\nimport numpy as np\\\\nimport pandas as pd\\\\n\\\\n# Read the CSV file into a DataFrame: df\\\\ndf = pd.read_csv(&#39;gapminder.csv&#39;)\\\\n\\\\n# Create arrays for features and target variable\\\\ny = df[&#39;life&#39;].values\\\\nX = df[&#39;fertility&#39;].values\\\\n\\\\n# Print the dimensions of X and y before reshaping\\\\nprint(\\\\&quot;Dimensions of y before reshaping: {}\\\\&quot;.format(y.shape))\\\\nprint(\\\\&quot;Dimensions of X before reshaping: {}\\\\&quot;.format(X.shape))\\\\n\\\\n# Reshape X and y\\\\ny = y.reshape(-1, 1)\\\\nX = X.reshape(-1, 1)\\\\n\\\\n# Print the dimensions of X and y after reshaping\\\\nprint(\\\\&quot;Dimensions of y after reshaping: {}\\\\&quot;.format(y.shape))\\\\nprint(\\\\&quot;Dimensions of X after reshaping: {}\\\\&quot;.format(X.shape))\\\\n&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use &lt;code&gt;import x as y&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; as the alias &lt;code&gt;y&lt;/code&gt;. &lt;/li&gt;\\\\n&lt;li&gt;To read in the CSV file &lt;code&gt;&#39;gapminder.csv&#39;&lt;/code&gt;, pass it in as an argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;To create the arrays, first select the column using &lt;code&gt;df[&#39;column_name&#39;]&lt;/code&gt; and then access its &lt;code&gt;.values&lt;/code&gt; attribute.&lt;/li&gt;\\\\n&lt;li&gt;Use &lt;code&gt;.reshape(-1, 1)&lt;/code&gt; on &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; to reshape them.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.8386097187169055],[&quot;^ &quot;,&quot;id&quot;,101796,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;As always, it is important to explore your data before building models. On the right, we have constructed a heatmap showing the correlation between the different features of the Gapminder dataset, which has been pre-loaded into a DataFrame as &lt;code&gt;df&lt;/code&gt; and is available for exploration in the IPython Shell. Cells that are in green show positive correlation, while cells that are in red show negative correlation. Take a moment to explore this: Which features are positively correlated with &lt;code&gt;life&lt;/code&gt;, and which ones are negatively correlated? Does this match your intuition? &lt;/p&gt;\\\\n&lt;p&gt;Then, in the IPython Shell, explore the DataFrame using pandas methods such as &lt;code&gt;.info()&lt;/code&gt;, &lt;code&gt;.describe()&lt;/code&gt;, &lt;code&gt;.head()&lt;/code&gt;. &lt;/p&gt;\\\\n&lt;p&gt;In case you are curious, the heatmap was generated using &lt;a href=\\\\&quot;http://seaborn.pydata.org/generated/seaborn.heatmap.html\\\\&quot;&gt;Seaborn&#39;s heatmap function&lt;/a&gt; and the following line of code, where &lt;code&gt;df.corr()&lt;/code&gt; computes the pairwise correlation between columns:&lt;/p&gt;\\\\n&lt;p&gt;&lt;code&gt;sns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;)&lt;/code&gt;&lt;/p&gt;\\\\n&lt;p&gt;Once you have a feel for the data, consider the statements below and select the one that is &lt;strong&gt;not&lt;/strong&gt; true. After this, Hugo will explain the mechanics of linear regression in the next video and you will be on your way building regression models!&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Exploring the Gapminder data&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,[&quot;The DataFrame has &lt;code&gt;139&lt;/code&gt; samples (or rows) and &lt;code&gt;9&lt;/code&gt; columns.&quot;,&quot;&lt;code&gt;life&lt;/code&gt; and &lt;code&gt;fertility&lt;/code&gt; are negatively correlated.&quot;,&quot;The mean of &lt;code&gt;life&lt;/code&gt; is &lt;code&gt;69.602878&lt;/code&gt;.&quot;,&quot;&lt;code&gt;fertility&lt;/code&gt; is of type &lt;code&gt;int64&lt;/code&gt;.&quot;,&quot;&lt;code&gt;GDP&lt;/code&gt; and &lt;code&gt;life&lt;/code&gt; are positively correlated.&quot;],&quot;^N&quot;,4,&quot;sct&quot;,&quot;msg1 = \\\\&quot;The DataFrame does indeed have `139` rows and 9 `columns`, as seen by using `df.info()`. Remember, `life` is a column as well, even though we are using it as our target variable.\\\\&quot;\\\\nmsg2 = \\\\&quot;This is a true statement. Look at the heatmap: the cell corresponding to `life` and `fertility` is red, indicating a negative correlation.\\\\&quot;\\\\nmsg3 = \\\\&quot;Using `df.describe()` shows that the mean of `life` is indeed `69.602878`.\\\\&quot;\\\\nmsg_success = \\\\&quot;Good job! As seen by using `df.info()`, `fertility`, along with all the other columns, is of type `float64`, **not** `int64`.\\\\&quot;\\\\nmsg5 = \\\\&quot;This is a true statement. Look at the heatmap: the cell corresponding to `GDP` and `life` is green, indicating a positive correlation.\\\\&quot;\\\\n\\\\ntest_mc(4, [msg1, msg2, msg3, msg_success, msg5]) #Option 4 is correct&quot;,&quot;^W&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nimport seaborn as sns\\\\nfrom urllib.request import urlretrieve\\\\n\\\\nfn = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;\\\\nurlretrieve(fn, &#39;gapminder.csv&#39;)\\\\n\\\\ndf = pd.read_csv(&#39;gapminder.csv&#39;)\\\\n\\\\nsns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;)\\\\n\\\\nplt.xticks(rotation=90) \\\\nplt.yticks(rotation=0)\\\\n\\\\nplt.show()&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;In the IPython Shell, use the &lt;code&gt;.info()&lt;/code&gt;, &lt;code&gt;.describe()&lt;/code&gt;, and &lt;code&gt;.head()&lt;/code&gt; methods on &lt;code&gt;df&lt;/code&gt; to explore the DataFrame. Observe the heatmap as well. Remember, cells in red show negative correlation, while cells in green show positive correlation.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.13925421161774],[&quot;^ &quot;,&quot;id&quot;,55931,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,&quot;The basics of linear regression&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,5,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,null,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^12&quot;,null,&quot;^13&quot;,&quot;//videos.datacamp.com/transcoded/1939_supervised_learning_w_scikit_learn/v7/hls-ch2_2.master.m3u8&quot;,&quot;^14&quot;,56.25,&quot;^15&quot;,&quot;course_1939_c2c0d781d764cf5b8310a165fe711cfe&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.18733165875262814],[&quot;^ &quot;,&quot;id&quot;,55932,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Now, you will fit a linear regression and predict life expectancy using just one feature. You saw Andy do this earlier using the &lt;code&gt;&#39;RM&#39;&lt;/code&gt; feature of the Boston housing dataset. In this exercise, you will use the &lt;code&gt;&#39;fertility&#39;&lt;/code&gt; feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is &lt;code&gt;&#39;life&#39;&lt;/code&gt;. The array for the target variable has been pre-loaded as &lt;code&gt;y&lt;/code&gt; and the array for &lt;code&gt;&#39;fertility&#39;&lt;/code&gt; has been pre-loaded as &lt;code&gt;X_fertility&lt;/code&gt;.&lt;/p&gt;\\\\n&lt;p&gt;A scatter plot with &lt;code&gt;&#39;fertility&#39;&lt;/code&gt; on the x-axis and &lt;code&gt;&#39;life&#39;&lt;/code&gt; on the y-axis has been generated. As you can see, there is a strongly negative correlation, so a linear regression should be able to capture this trend. Your job is to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. You will also compute and print the \\\\\\\\(R^2\\\\\\\\) score using sckit-learn&#39;s &lt;code&gt;.score()&lt;/code&gt; method.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Fit &amp; predict for regression&quot;,&quot;^U&quot;,&quot;# Import LinearRegression\\\\n____\\\\n\\\\n# Create the regressor: reg\\\\nreg = ____\\\\n\\\\n# Create the prediction space\\\\nprediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\\\\n\\\\n# Fit the model to the data\\\\n____\\\\n\\\\n# Compute predictions over the prediction space: y_pred\\\\ny_pred = ____\\\\n\\\\n# Print R^2 \\\\nprint(reg.score(____, ____))\\\\n\\\\n# Plot regression line\\\\nplt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3)\\\\nplt.show()\\\\n&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;LinearRegression&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Create a &lt;code&gt;LinearRegression&lt;/code&gt; regressor called &lt;code&gt;reg&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Set up the prediction space to range from the minimum to the maximum of &lt;code&gt;X_fertility&lt;/code&gt;. This has been done for you.&lt;/li&gt;\\\\n&lt;li&gt;Fit the regressor to the data (&lt;code&gt;X_fertility&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;) and compute its predictions using the &lt;code&gt;.predict()&lt;/code&gt; method and the &lt;code&gt;prediction_space&lt;/code&gt; array.&lt;/li&gt;\\\\n&lt;li&gt;Compute and print the \\\\\\\\(R^2\\\\\\\\) score using the &lt;code&gt;.score()&lt;/code&gt; method.&lt;/li&gt;\\\\n&lt;li&gt;Overlay the plot with your linear regression line. This has been done for you, so hit &#39;Submit Answer&#39; to see the result!&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,6,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.LinearRegression&#39;,\\\\n                 not_imported_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.LinearRegression&#39;)\\\\nEx().test_object(&#39;reg&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;numpy.linspace&#39;)\\\\nEx().test_function(&#39;numpy.linspace.reshape&#39;)\\\\nEx().test_object(&#39;prediction_space&#39;)\\\\n\\\\nEx().test_function(&#39;reg.fit&#39;)\\\\nEx().check_function(&#39;reg.predict&#39;, index=0).check_args(0).has_equal_value(&#39;Did you use `.predict()` to compute the predictions of `reg` over `prediction_space`?&#39;, error_msg=&#39;Did you use `.predict()` compute the predictions of `reg` over `prediction_space`?&#39;)\\\\nEx().test_object(&#39;y_pred&#39;)\\\\n\\\\nEx().test_function(&#39;reg.score&#39;)\\\\nEx().test_function(&#39;print&#39;)\\\\n\\\\nEx().test_function(&#39;matplotlib.pyplot.plot&#39;, incorrect_msg=&#39;The code to plot the regression line has been provided for you. You don\\\\\\\\&#39;t have to change this.&#39;, not_called_msg=&#39;The code to plot the regression line has been provided for you. You don\\\\\\\\&#39;t have to change this.&#39;)\\\\nEx().test_function(&#39;matplotlib.pyplot.show&#39;, incorrect_msg=&#39;The code to plot the regression line has been provided for you. You don\\\\\\\\&#39;t have to change this.&#39;, not_called_msg=&#39;The code to plot the regression line has been provided for you. You don\\\\\\\\&#39;t have to change this.&#39;)\\\\n\\\\nsuccess_msg(\\\\&quot;Fantastic! Notice how the line captures the underlying trend in the data. And the performance is quite decent for this basic regression model with only one feature!\\\\&quot;)\\\\n&quot;,&quot;^W&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\n\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1)\\\\n\\\\n# Reshape to 1-D\\\\ny = y.reshape(-1, 1)\\\\nX_fertility = X[&#39;fertility&#39;].reshape(-1, 1) \\\\n\\\\n_ = plt.scatter(X[&#39;fertility&#39;], y, color=&#39;blue&#39;)\\\\n_ = plt.ylabel(&#39;Life Expectancy&#39;)\\\\n_ = plt.xlabel(&#39;Fertility&#39;)\\\\nplt.show()\\\\n&quot;,&quot;^X&quot;,&quot;# Import LinearRegression\\\\nfrom sklearn.linear_model import LinearRegression\\\\n\\\\n# Create the regressor: reg\\\\nreg = LinearRegression()\\\\n\\\\n# Create the prediction space\\\\nprediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\\\\n\\\\n# Fit the model to the data\\\\nreg.fit(X_fertility, y)\\\\n\\\\n# Compute predictions over the prediction space: y_pred\\\\ny_pred = reg.predict(prediction_space)\\\\n\\\\n# Print R^2 \\\\nprint(reg.score(X_fertility, y))\\\\n\\\\n# Plot regression line\\\\nplt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3)\\\\nplt.show()\\\\n&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;You can import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt; using the command &lt;code&gt;from y import x&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the function &lt;code&gt;LinearRegression()&lt;/code&gt; to create the regressor.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;.fit()&lt;/code&gt; method on &lt;code&gt;reg&lt;/code&gt; with &lt;code&gt;X_fertility&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; as arguments to fit the model.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;.predict()&lt;/code&gt; method on &lt;code&gt;reg&lt;/code&gt; with &lt;code&gt;prediction_space&lt;/code&gt; as the argument to compute the predictions.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;.score()&lt;/code&gt; method with &lt;code&gt;X_fertility&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; as arguments to compute the \\\\\\\\(R^2\\\\\\\\) score.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.28675428230407807],[&quot;^ &quot;,&quot;id&quot;,68998,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;As you learned in Chapter 1, train and test sets are vital to ensure that your supervised learning model is able to generalize well to new data. This was true for classification models, and is equally true for linear regression models. &lt;/p&gt;\\\\n&lt;p&gt;In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over &lt;strong&gt;all&lt;/strong&gt; features. In addition to computing the \\\\\\\\(R^2\\\\\\\\) score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. The feature array &lt;code&gt;X&lt;/code&gt; and target variable array &lt;code&gt;y&lt;/code&gt; have been pre-loaded for you from the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Train/test split for regression&quot;,&quot;^U&quot;,&quot;# Import necessary modules\\\\n____\\\\n____\\\\n____\\\\n\\\\n# Create training and test sets\\\\nX_train, X_test, y_train, y_test = ____(____, ____, test_size = ____, random_state=____)\\\\n\\\\n# Create the regressor: reg_all\\\\nreg_all = ____\\\\n\\\\n# Fit the regressor to the training data\\\\n____\\\\n\\\\n# Predict on the test data: y_pred\\\\ny_pred = ____\\\\n\\\\n# Compute and print R^2 and RMSE\\\\nprint(\\\\&quot;R^2: {}\\\\&quot;.format(reg_all.score(X_test, y_test)))\\\\nrmse = np.sqrt(____)\\\\nprint(\\\\&quot;Root Mean Squared Error: {}\\\\&quot;.format(rmse))\\\\n&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;LinearRegression&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt;, &lt;code&gt;mean_squared_error&lt;/code&gt; from &lt;code&gt;sklearn.metrics&lt;/code&gt;, and &lt;code&gt;train_test_split&lt;/code&gt; from &lt;code&gt;sklearn.model_selection&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Using &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, create training and test sets such that 30% is used for testing and 70% for training. Use a random state of &lt;code&gt;42&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Create a linear regression regressor called &lt;code&gt;reg_all&lt;/code&gt;, fit it to the training set, and evaluate it on the test set.&lt;/li&gt;\\\\n&lt;li&gt;Compute and print the \\\\\\\\(R^2\\\\\\\\) score using the &lt;code&gt;.score()&lt;/code&gt; method on the test set.&lt;/li&gt;\\\\n&lt;li&gt;Compute and print the RMSE. To do this, first compute the Mean Squared Error using the &lt;code&gt;mean_squared_error()&lt;/code&gt; function with the arguments &lt;code&gt;y_test&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt;, and then take its square root using &lt;code&gt;np.sqrt()&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,7,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.LinearRegression&#39;,\\\\n                 not_imported_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;)\\\\n                 \\\\nEx().test_import(&#39;sklearn.metrics.mean_squared_error&#39;,\\\\n                 not_imported_msg=&#39;Did you import `mean_squared_error` from `sklearn.metrics`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `mean_squared_error` from `sklearn.metrics`?&#39;)\\\\n                 \\\\nEx().test_import(&#39;sklearn.model_selection.train_test_split&#39;,\\\\n                 not_imported_msg=&#39;Did you import `train_test_split` from `sklearn.model_selection`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `train_test_split` from `sklearn.model_selection`?&#39;)\\\\n\\\\nEx().test_correct(test_object(&#39;X_train&#39;), test_function(&#39;sklearn.model_selection.train_test_split&#39;, args_not_specified_msg=&#39;Have you specified all required arguments inside `train_test_split()`? After passing in `X` and `y`, you need to specify the `train_size` **or** `test_size` and the `random_state`.&#39;))\\\\nEx().test_correct(test_object(&#39;X_test&#39;), test_function(&#39;sklearn.model_selection.train_test_split&#39;, args_not_specified_msg=&#39;Have you specified all required arguments inside `train_test_split()`? After passing in `X` and `y`, you need to specify the `train_size` **or** `test_size` and the `random_state`.&#39;))\\\\nEx().test_correct(test_object(&#39;y_train&#39;), test_function(&#39;sklearn.model_selection.train_test_split&#39;, args_not_specified_msg=&#39;Have you specified all required arguments inside `train_test_split()`? After passing in `X` and `y`, you need to specify the `train_size` **or** `test_size` and the `random_state`.&#39;))\\\\nEx().test_correct(test_object(&#39;y_test&#39;), test_function(&#39;sklearn.model_selection.train_test_split&#39;, args_not_specified_msg=&#39;Have you specified all required arguments inside `train_test_split()`? After passing in `X` and `y`, you need to specify the `train_size` **or** `test_size` and the `random_state`.&#39;))\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.LinearRegression&#39;)\\\\nEx().test_object(&#39;reg_all&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;reg_all.predict&#39;)\\\\n#Ex().test_object(&#39;y_pred&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\nEx().test_function(&#39;sklearn.metrics.mean_squared_error&#39;)\\\\nEx().test_function(&#39;numpy.sqrt&#39;)\\\\nEx().test_object(&#39;rmse&#39;)\\\\nEx().test_function(&#39;print&#39;, index=2)\\\\n\\\\nsuccess_msg(\\\\&quot;Excellent! Using all features has improved the model score. This makes sense, as the model has more information to learn from. However, there is one potential pitfall to this process. Can you spot it? You&#39;ll learn about this as well how to better validate your models in the next video!\\\\&quot;)\\\\n\\\\n&quot;,&quot;^W&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values\\\\n&quot;,&quot;^X&quot;,&quot;# Import necessary modules\\\\nfrom sklearn.linear_model import LinearRegression\\\\nfrom sklearn.metrics import mean_squared_error\\\\nfrom sklearn.model_selection import train_test_split\\\\n\\\\n# Create training and test sets\\\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\\\\n\\\\n# Create the regressor: reg_all\\\\nreg_all = LinearRegression()\\\\n\\\\n# Fit the regressor to the training data\\\\nreg_all.fit(X_train, y_train)\\\\n\\\\n# Predict on the test data: y_pred\\\\ny_pred = reg_all.predict(X_test)\\\\n\\\\n# Compute and print R^2 and RMSE\\\\nprint(\\\\&quot;R^2: {}\\\\&quot;.format(reg_all.score(X_test, y_test)))\\\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\\\\nprint(\\\\&quot;Root Mean Squared Error: {}\\\\&quot;.format(rmse))\\\\n&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use the command &lt;code&gt;from y import x&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use &lt;code&gt;train_test_split()&lt;/code&gt; to create training and test sets. Pass in the arguments &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, and specify the test set size using &lt;code&gt;test_size&lt;/code&gt; (&lt;strong&gt;or&lt;/strong&gt; the training set size using &lt;code&gt;train_size&lt;/code&gt;) and the random state using &lt;code&gt;random_state&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;To create the regressor, use &lt;code&gt;LinearRegression()&lt;/code&gt;. Then use &lt;code&gt;.fit()&lt;/code&gt; to fit it to &lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt;, and &lt;code&gt;.predict()&lt;/code&gt; to evaluate it over &lt;code&gt;X_test&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;To compute the RMSE, compute the mean squared error inside the provided &lt;code&gt;np.sqrt()&lt;/code&gt; function using the &lt;code&gt;mean_squared_error()&lt;/code&gt; function. &lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.6935809439079497],[&quot;^ &quot;,&quot;id&quot;,68999,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,&quot;Cross-validation&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,8,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,null,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^12&quot;,null,&quot;^13&quot;,&quot;//videos.datacamp.com/transcoded/1939_supervised_learning_w_scikit_learn/v5/hls-ch2_3.master.m3u8&quot;,&quot;^14&quot;,56.25,&quot;^15&quot;,&quot;course_1939_958a0c679de5c7d3b0b0a5acec08ba22&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.7864841993659184],[&quot;^ &quot;,&quot;id&quot;,62360,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Cross-validation is a vital step in evaluating a model. It maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data.&lt;/p&gt;\\\\n&lt;p&gt;In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn&#39;s &lt;code&gt;cross_val_score()&lt;/code&gt; function uses \\\\\\\\(R^2\\\\\\\\) as the metric of choice for regression. Since you are performing 5-fold cross-validation, the function will return 5 scores. Your job is to compute these 5 scores and then take their average.&lt;/p&gt;\\\\n&lt;p&gt;The DataFrame has been loaded as &lt;code&gt;df&lt;/code&gt; and split into the feature/target variable arrays &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. The modules &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; have been imported as &lt;code&gt;pd&lt;/code&gt; and &lt;code&gt;np&lt;/code&gt;, respectively.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;5-fold cross-validation&quot;,&quot;^U&quot;,&quot;# Import the necessary modules\\\\n____\\\\n____\\\\n\\\\n# Create a linear regression object: reg\\\\nreg = ____\\\\n\\\\n# Compute 5-fold cross-validation scores: cv_scores\\\\ncv_scores = ____\\\\n\\\\n# Print the 5-fold cross-validation scores\\\\nprint(____)\\\\n\\\\nprint(\\\\&quot;Average 5-Fold CV Score: {}\\\\&quot;.format(____(____)))\\\\n&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;LinearRegression&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt; and &lt;code&gt;cross_val_score&lt;/code&gt; from &lt;code&gt;sklearn.model_selection&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Create a linear regression regressor called &lt;code&gt;reg&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;cross_val_score()&lt;/code&gt; function to perform 5-fold cross-validation on &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Compute and print the average cross-validation score. You can use NumPy&#39;s &lt;code&gt;mean()&lt;/code&gt; function to compute the average.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,9,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.LinearRegression&#39;,\\\\n                 not_imported_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;)\\\\n\\\\nEx().test_import(&#39;sklearn.model_selection.cross_val_score&#39;,\\\\n                 not_imported_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.LinearRegression&#39;)\\\\nEx().test_object(&#39;reg&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;sklearn.model_selection.cross_val_score&#39;, do_eval=False)\\\\nEx().test_object(&#39;cv_scores&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\nEx().test_function(&#39;print&#39;, index=2)\\\\n\\\\nsuccess_msg(\\\\&quot;Great work! Now that you have cross-validated your model, you can more confidently evaluate its predictions.\\\\&quot;)\\\\n&quot;,&quot;^W&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values&quot;,&quot;^X&quot;,&quot;# Import the necessary modules\\\\nfrom sklearn.linear_model import LinearRegression\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Create a linear regression object: reg\\\\nreg = LinearRegression()\\\\n\\\\n# Compute 5-fold cross-validation scores: cv_scores\\\\ncv_scores = cross_val_score(reg, X, y, cv=5)\\\\n\\\\n# Print the 5-fold cross-validation scores\\\\nprint(cv_scores)\\\\n\\\\n# Print the average 5-fold cross-validation score\\\\nprint(\\\\&quot;Average 5-Fold CV Score: {}\\\\&quot;.format(np.mean(cv_scores)))\\\\n&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use the command &lt;code&gt;from y import x&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;LinearRegression()&lt;/code&gt; function to create the regressor.&lt;/li&gt;\\\\n&lt;li&gt;Inside &lt;code&gt;cross_val_score()&lt;/code&gt;, specify the arguments &lt;code&gt;reg&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;cv=5&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Pass &lt;code&gt;cv_scores&lt;/code&gt; to the first function, and &lt;code&gt;np.mean(cv_scores)&lt;/code&gt; to the &lt;code&gt;format()&lt;/code&gt; function of the second &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.015515917094008236],[&quot;^ &quot;,&quot;id&quot;,69000,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset.&lt;/p&gt;\\\\n&lt;p&gt;In the IPython Shell, you can use &lt;code&gt;%timeit&lt;/code&gt; to see how long each 3-fold CV takes compared to 10-fold CV by executing the following &lt;code&gt;cv=3&lt;/code&gt; and &lt;code&gt;cv=10&lt;/code&gt;:&lt;/p&gt;\\\\n&lt;pre&gt;&lt;code&gt;%timeit cross_val_score(reg, X, y, cv = ____)\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\n&lt;p&gt;&lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; are available in the workspace as &lt;code&gt;pd&lt;/code&gt; and &lt;code&gt;np&lt;/code&gt;. The DataFrame has been loaded as &lt;code&gt;df&lt;/code&gt; and the feature/target variable arrays &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; have been created.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;K-Fold CV comparison&quot;,&quot;^U&quot;,&quot;# Import necessary modules\\\\n____\\\\n____\\\\n\\\\n# Create a linear regression object: reg\\\\nreg = ____\\\\n\\\\n# Perform 3-fold CV\\\\ncvscores_3 = ____\\\\nprint(np.mean(cvscores_3))\\\\n\\\\n# Perform 10-fold CV\\\\ncvscores_10 = ____\\\\nprint(np.mean(cvscores_10))\\\\n&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;LinearRegression&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt; and &lt;code&gt;cross_val_score&lt;/code&gt; from &lt;code&gt;sklearn.model_selection&lt;/code&gt;. &lt;/li&gt;\\\\n&lt;li&gt;Create a linear regression regressor called &lt;code&gt;reg&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Perform 3-fold CV and then 10-fold CV. Compare the resulting mean scores.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,10,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.LinearRegression&#39;,\\\\n                 not_imported_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;)\\\\n                 \\\\nEx().test_import(&#39;sklearn.model_selection.cross_val_score&#39;,\\\\n                 not_imported_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.LinearRegression&#39;, do_eval=False)\\\\nEx().test_object(&#39;reg&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;sklearn.model_selection.cross_val_score&#39;, do_eval=False, index=1)\\\\nEx().test_object(&#39;cvscores_3&#39;)\\\\n\\\\nEx().test_function(&#39;numpy.mean&#39;, index=1)\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\n\\\\nEx().test_function(&#39;sklearn.model_selection.cross_val_score&#39;, do_eval=False, index=2)\\\\nEx().test_object(&#39;cvscores_10&#39;)\\\\nEx().test_function(&#39;numpy.mean&#39;, index=2)\\\\nEx().test_function(&#39;print&#39;, index=2)\\\\n\\\\nsuccess_msg(\\\\&quot;Excellent! Did you use `%timeit` in the IPython Shell to see how much longer it takes 10-fold cross-validation to run compared to 3-fold cross-validation?\\\\&quot;)\\\\n\\\\n&quot;,&quot;^W&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values\\\\n\\\\n# Import necessary modules\\\\nfrom sklearn import linear_model\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Create a linear regression object\\\\nreg = linear_model.LinearRegression()\\\\n\\\\n&quot;,&quot;^X&quot;,&quot;# Import necessary modules\\\\nfrom sklearn.linear_model import LinearRegression\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Create a linear regression object: reg\\\\nreg = LinearRegression()\\\\n\\\\n# Perform 3-fold CV\\\\ncvscores_3 = cross_val_score(reg, X, y, cv = 3)\\\\nprint(np.mean(cvscores_3))\\\\n\\\\n# Perform 10-fold CV\\\\ncvscores_10 = cross_val_score(reg, X, y, cv = 10)\\\\nprint(np.mean(cvscores_10))\\\\n&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use the command &lt;code&gt;from y import x&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;LinearRegression()&lt;/code&gt; function to create the regressor.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;cross_val_score()&lt;/code&gt; function with &lt;code&gt;reg&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; as arguments. For 3-fold CV, specify &lt;code&gt;cv=3&lt;/code&gt;, and for 10-fold CV, specify &lt;code&gt;cv=10&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.3140520886777385],[&quot;^ &quot;,&quot;id&quot;,62363,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^T&quot;,null,&quot;^1&quot;,&quot;Regularized regression&quot;,&quot;^U&quot;,&quot;&quot;,&quot;^V&quot;,null,&quot;^N&quot;,11,&quot;sct&quot;,&quot;&quot;,&quot;^W&quot;,&quot;&quot;,&quot;^X&quot;,&quot;&quot;,&quot;^Y&quot;,null,&quot;^Z&quot;,null,&quot;xp&quot;,50,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^12&quot;,null,&quot;^13&quot;,&quot;//videos.datacamp.com/transcoded/1939_supervised_learning_w_scikit_learn/v4/hls-ch2_4.master.m3u8&quot;,&quot;^14&quot;,56.25,&quot;^15&quot;,&quot;course_1939_f9e49441a9f1a28282ee5ba0c965cf9d&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.760492612055184],[&quot;^ &quot;,&quot;id&quot;,69001,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;In the video, you saw how Lasso selected out the &lt;code&gt;&#39;RM&#39;&lt;/code&gt; feature as being the most important for predicting Boston house prices, while shrinking the coefficients of certain other features to 0. Its ability to perform feature selection in this way becomes even more useful when you are dealing with data involving thousands of features. &lt;/p&gt;\\\\n&lt;p&gt;In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining.&lt;/p&gt;\\\\n&lt;p&gt;The feature and target variable arrays have been pre-loaded as &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Regularization I: Lasso&quot;,&quot;^U&quot;,&quot;# Import Lasso\\\\nfrom sklearn.linear_model import Lasso\\\\n\\\\n# Instantiate a lasso regressor: lasso\\\\nlasso = ____\\\\n\\\\n# Fit the regressor to the data\\\\n____\\\\n\\\\n# Compute and print the coefficients\\\\nlasso_coef = ____\\\\nprint(lasso_coef)\\\\n\\\\n# Plot the coefficients\\\\nplt.plot(range(len(df_columns)), lasso_coef)\\\\nplt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\\\\nplt.margins(0.02)\\\\nplt.show()\\\\n&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;Lasso&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Instantiate a Lasso regressor with an alpha of &lt;code&gt;0.4&lt;/code&gt; and specify &lt;code&gt;normalize=True&lt;/code&gt;. &lt;/li&gt;\\\\n&lt;li&gt;Fit the regressor to the data and compute the coefficients using the &lt;code&gt;coef_&lt;/code&gt; attribute.&lt;/li&gt;\\\\n&lt;li&gt;Plot the coefficients on the y-axis and column names on the x-axis. This has been done for you, so hit &#39;Submit Answer&#39; to view the plot!&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,12,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.Lasso&#39;,\\\\n                 not_imported_msg=&#39;Did you import `Lasso` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `Lasso` from `sklearn.linear_model`?&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.Lasso&#39;)\\\\nEx().test_object(&#39;lasso&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;lasso.fit&#39;)\\\\nEx().test_object_accessed(&#39;lasso.coef_&#39;)\\\\nEx().test_object(&#39;lasso_coef&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\n\\\\nEx().test_function(&#39;matplotlib.pyplot.plot&#39;)\\\\nEx().test_function(&#39;matplotlib.pyplot.xticks&#39;)\\\\nEx().test_function(&#39;matplotlib.pyplot.margins&#39;)\\\\nEx().test_function(&#39;matplotlib.pyplot.show&#39;)\\\\n\\\\nsuccess_msg(\\\\&quot;Great work! According to the lasso algorithm, it seems like `&#39;child_mortality&#39;` is the most important feature when predicting life expectancy.\\\\&quot;)\\\\n\\\\n&quot;,&quot;^W&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values\\\\n\\\\ndf_columns = df.drop(&#39;life&#39;, axis=1).columns\\\\n&quot;,&quot;^X&quot;,&quot;# Import Lasso\\\\nfrom sklearn.linear_model import Lasso\\\\n\\\\n# Instantiate a lasso regressor: lasso\\\\nlasso = Lasso(alpha=0.4, normalize=True)\\\\n\\\\n# Fit the regressor to the data\\\\nlasso.fit(X, y)\\\\n\\\\n# Compute and print the coefficients\\\\nlasso_coef = lasso.coef_\\\\nprint(lasso_coef)\\\\n\\\\n# Plot the coefficients\\\\nplt.plot(range(len(df_columns)), lasso_coef)\\\\nplt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\\\\nplt.margins(0.02)\\\\nplt.show()\\\\n&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use the command &lt;code&gt;from y import x&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;Lasso()&lt;/code&gt; function with &lt;code&gt;alpha=0.4&lt;/code&gt; and &lt;code&gt;normalize=True&lt;/code&gt; to instantiate the regressor.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;.fit()&lt;/code&gt; method of the regressor on &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; to fit it to the data.&lt;/li&gt;\\\\n&lt;li&gt;To compute the coefficients, access the &lt;code&gt;.coef_&lt;/code&gt; attribute of the lasso regressor.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.05695502255452389],[&quot;^ &quot;,&quot;id&quot;,69002,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^T&quot;,&quot;&lt;p&gt;Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice.&lt;/p&gt;\\\\n&lt;p&gt;Recall that lasso performs regularization by adding to the loss function a penalty term of the &lt;em&gt;absolute&lt;/em&gt; value of each coefficient multiplied by some alpha. This is also known as \\\\\\\\(L1\\\\\\\\) regularization because the regularization term is the \\\\\\\\(L1\\\\\\\\) norm of the coefficients. This is not the only way to regularize, however. &lt;/p&gt;\\\\n&lt;p&gt;If instead you took the sum of the &lt;em&gt;squared&lt;/em&gt; values of the coefficients multiplied by some alpha - like in Ridge regression - you would be computing the \\\\\\\\(L2\\\\\\\\) norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated \\\\\\\\(R^2\\\\\\\\) scores for each, using this function that we have defined for you, which plots the \\\\\\\\(R^2\\\\\\\\) score as well as standard error for each alpha:&lt;/p&gt;\\\\n&lt;pre&gt;&lt;code&gt;def display_plot(cv_scores, cv_scores_std):\\\\n    fig = plt.figure()\\\\n    ax = fig.add_subplot(1,1,1)\\\\n    ax.plot(alpha_space, cv_scores)\\\\n\\\\n    std_error = cv_scores_std / np.sqrt(10)\\\\n\\\\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\\\\n    ax.set_ylabel(&#39;CV Score +/- Std Error&#39;)\\\\n    ax.set_xlabel(&#39;Alpha&#39;)\\\\n    ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;)\\\\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\\\\n    ax.set_xscale(&#39;log&#39;)\\\\n    plt.show()\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\n&lt;p&gt;Don&#39;t worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the \\\\\\\\(R^2\\\\\\\\) score varies with different alphas, and to understand the importance of selecting the right value for alpha. You&#39;ll learn how to tune alpha in the next chapter.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Regularization II: Ridge&quot;,&quot;^U&quot;,&quot;# Import necessary modules\\\\nfrom sklearn.linear_model import Ridge\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Setup the array of alphas and lists to store scores\\\\nalpha_space = np.logspace(-4, 0, 50)\\\\nridge_scores = []\\\\nridge_scores_std = []\\\\n\\\\n# Create a ridge regressor: ridge\\\\nridge = ____\\\\n\\\\n# Compute scores over range of alphas\\\\nfor alpha in alpha_space:\\\\n\\\\n    # Specify the alpha value to use: ridge.alpha\\\\n    ridge.alpha = ____\\\\n    \\\\n    # Perform 10-fold CV: ridge_cv_scores\\\\n    ridge_cv_scores = ____\\\\n    \\\\n    # Append the mean of ridge_cv_scores to ridge_scores\\\\n    ridge_scores.append(____(____))\\\\n    \\\\n    # Append the std of ridge_cv_scores to ridge_scores_std\\\\n    ridge_scores_std.append(____(____))\\\\n\\\\n# Display the plot\\\\ndisplay_plot(ridge_scores, ridge_scores_std)\\\\n&quot;,&quot;^V&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Instantiate a Ridge regressor and specify &lt;code&gt;normalize=True&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Inside the &lt;code&gt;for&lt;/code&gt; loop:&lt;ul&gt;\\\\n&lt;li&gt;Specify the alpha value for the regressor to use.&lt;/li&gt;\\\\n&lt;li&gt;Perform 10-fold cross-validation on the regressor with the specified alpha. The data is available in the arrays &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Append the average and the standard deviation of the computed cross-validated scores. NumPy has been pre-imported for you as &lt;code&gt;np&lt;/code&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;display_plot()&lt;/code&gt; function to visualize the scores and standard deviations.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^N&quot;,13,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.Ridge&#39;,\\\\n                 not_imported_msg=&#39;Did you import `Ridge` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `Ridge` from `sklearn.linear_model`?&#39;)\\\\n\\\\nEx().test_import(&#39;sklearn.model_selection.cross_val_score&#39;,\\\\n                 not_imported_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;)\\\\n\\\\nEx().test_function(&#39;numpy.logspace&#39;, incorrect_msg=&#39;The array of alphas has been setup for you. You don\\\\\\\\&#39;t have to modify this.&#39;)\\\\nEx().test_object(&#39;alpha_space&#39;, incorrect_msg=&#39;The array of alphas has been setup for you. You don\\\\\\\\&#39;t have to modify this.&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.Ridge&#39;)\\\\n\\\\nEx().check_for_loop(0).check_iter().has_equal_value()\\\\n\\\\nEx().check_for_loop(0).check_body().test_function(&#39;numpy.mean&#39;, incorrect_msg=&#39;Did you correctly compute the mean of `ridge_cv_scores`? Ensure that you have first computed `ridge_cv_scores` correctly by performing 10-fold CV, and then that you computed its mean using `np.mean()`.&#39;)\\\\nEx().check_for_loop(0).check_body().test_function(&#39;ridge_scores.append&#39;)\\\\n\\\\nEx().check_for_loop(0).check_body().test_function(&#39;numpy.std&#39;)\\\\nEx().check_for_loop(0).check_body().test_function(&#39;ridge_scores_std.append&#39;)\\\\n\\\\n\\\\nsuccess_msg(\\\\&quot;Great work! Notice how the cross-validation scores change with different alphas. Which alpha should you pick? How can you fine-tune your model? You&#39;ll learn all about this in the next chapter!\\\\&quot;)\\\\n&quot;,&quot;^W&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values\\\\n\\\\ndef display_plot(cv_scores, cv_scores_std):\\\\n    fig = plt.figure()\\\\n    ax = fig.add_subplot(1,1,1)\\\\n    ax.plot(alpha_space, cv_scores)\\\\n\\\\n    std_error = cv_scores_std / np.sqrt(10)\\\\n\\\\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\\\\n    ax.set_ylabel(&#39;CV Score +/- Std Error&#39;)\\\\n    ax.set_xlabel(&#39;Alpha&#39;)\\\\n    ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;)\\\\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\\\\n    ax.set_xscale(&#39;log&#39;)\\\\n    plt.show()\\\\n&quot;,&quot;^X&quot;,&quot;# Import necessary modules\\\\nfrom sklearn.linear_model import Ridge\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Setup the array of alphas and lists to store scores\\\\nalpha_space = np.logspace(-4, 0, 50)\\\\nridge_scores = []\\\\nridge_scores_std = []\\\\n\\\\n# Create a ridge regressor: ridge\\\\nridge = Ridge(normalize=True)\\\\n\\\\n# Compute scores over range of alphas\\\\nfor alpha in alpha_space:\\\\n\\\\n    # Specify the alpha value to use: ridge.alpha\\\\n    ridge.alpha = alpha\\\\n    \\\\n    # Perform 10-fold CV: ridge_cv_scores\\\\n    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\\\\n    \\\\n    # Append the mean of ridge_cv_scores to ridge_scores\\\\n    ridge_scores.append(np.mean(ridge_cv_scores))\\\\n    \\\\n    # Append the std of ridge_cv_scores to ridge_scores_std\\\\n    ridge_scores_std.append(np.std(ridge_cv_scores))\\\\n\\\\n# Display the plot\\\\ndisplay_plot(ridge_scores, ridge_scores_std)\\\\n&quot;,&quot;^Y&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use &lt;code&gt;Ridge()&lt;/code&gt; to create the ridge regressor, and specify &lt;code&gt;normalize=True&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Inside the &lt;code&gt;for&lt;/code&gt; loop:&lt;ul&gt;\\\\n&lt;li&gt;Specify the alpha value by assigning &lt;code&gt;alpha&lt;/code&gt; to &lt;code&gt;ridge.alpha&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;cross_val_score()&lt;/code&gt; function with &lt;code&gt;ridge&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;cv=10&lt;/code&gt; as arguments.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;np.mean()&lt;/code&gt; and &lt;code&gt;np.std()&lt;/code&gt; functions to compute the mean and standard deviation of &lt;code&gt;ridge_cv_scores&lt;/code&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;^Z&quot;,null,&quot;xp&quot;,100,&quot;^[&quot;,[],&quot;^10&quot;,[],&quot;^11&quot;,&quot;&quot;,&quot;^16&quot;,&quot;python&quot;,&quot;^17&quot;,0.4314734773824245]]]],&quot;activeImage&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,&quot;course-1939-master:2af9ff2d1bb45aeb54d95b0aabe4903f-20180906121314280&quot;]],&quot;sharedImage&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,&quot;shared-python:a7cf0466b33c3d2a8eca9255d200220e-20181123091351381&quot;]]]],&quot;systemStatus&quot;,[&quot;^0&quot;,[&quot;indicator&quot;,&quot;none&quot;,&quot;description&quot;,&quot;No status has been fetched from the Status Page.&quot;]],&quot;backendSession&quot;,[&quot;^0&quot;,[&quot;status&quot;,[&quot;^0&quot;,[&quot;code&quot;,&quot;none&quot;,&quot;text&quot;,&quot;&quot;]],&quot;isInitSession&quot;,false,&quot;message&quot;,null]],&quot;settings&quot;,[&quot;^0&quot;,[&quot;uiTheme&quot;,&quot;LIGHT&quot;,&quot;isOnboarding&quot;,false]],&quot;autocomplete&quot;,[&quot;^0&quot;,[]],&quot;user&quot;,[&quot;^0&quot;,[&quot;status&quot;,null,&quot;settings&quot;,[&quot;^0&quot;,[]]]],&quot;fileBrowser&quot;,[&quot;^0&quot;,[&quot;isVisible&quot;,true,&quot;fileSelected&quot;,null]],&quot;chapter&quot;,[&quot;^0&quot;,[&quot;current&quot;,[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,2,&quot;slug&quot;,&quot;regression-2&quot;,&quot;last_updated_on&quot;,&quot;17/07/2018&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,13,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_4172/slides/ch2_slides.pdf&quot;,&quot;title&quot;,&quot;Regression&quot;,&quot;xp&quot;,1000,&quot;id&quot;,5233,&quot;description&quot;,&quot;In the previous chapter, you made use of image and political datasets to predict binary as well as multiclass outcomes. But what if your problem requires a continuous outcome? Regression, which is the focus of this chapter, is best suited to solving such problems. You will learn about fundamental concepts in regression and apply them to predict the life expectancy in a given country using Gapminder data. &quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;boot&quot;,[&quot;^0&quot;,[&quot;bootState&quot;,&quot;PRE_BOOTED&quot;,&quot;error&quot;,null]],&quot;location&quot;,[&quot;^0&quot;,[&quot;current&quot;,[&quot;^0&quot;,[&quot;pathname&quot;,&quot;/courses/supervised-learning-with-scikit-learn/regression-2&quot;,&quot;query&quot;,[&quot;^0&quot;,[&quot;ex&quot;,&quot;8&quot;]]]],&quot;canonical&quot;,null]],&quot;course&quot;,[&quot;^0&quot;,[&quot;difficulty_level&quot;,1,&quot;reduced_outline&quot;,null,&quot;shared_image&quot;,&quot;shared-python:a7cf0466b33c3d2a8eca9255d200220e-20181123091351381&quot;,&quot;active_image&quot;,&quot;course-1939-master:2af9ff2d1bb45aeb54d95b0aabe4903f-20180906121314280&quot;,&quot;author_field&quot;,null,&quot;chapters&quot;,[&quot;~#iL&quot;,[[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;slug&quot;,&quot;classification&quot;,&quot;last_updated_on&quot;,&quot;17/07/2018&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_4172/slides/ch1_slides.pdf&quot;,&quot;title&quot;,&quot;Classification&quot;,&quot;xp&quot;,900,&quot;id&quot;,5232,&quot;description&quot;,&quot;In this chapter, you will be introduced to classification problems and learn how to solve them using supervised learning techniques. Classification problems are prevalent in a variety of domains, ranging from finance to healthcare. Here, you will have the chance to apply what you are learning to a political dataset, where you classify the party affiliation of United States Congressmen based on their voting records. &quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,2,&quot;slug&quot;,&quot;regression-2&quot;,&quot;last_updated_on&quot;,&quot;17/07/2018&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,13,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_4172/slides/ch2_slides.pdf&quot;,&quot;title&quot;,&quot;Regression&quot;,&quot;xp&quot;,1000,&quot;id&quot;,5233,&quot;description&quot;,&quot;In the previous chapter, you made use of image and political datasets to predict binary as well as multiclass outcomes. But what if your problem requires a continuous outcome? Regression, which is the focus of this chapter, is best suited to solving such problems. You will learn about fundamental concepts in regression and apply them to predict the life expectancy in a given country using Gapminder data. &quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,3,&quot;slug&quot;,&quot;fine-tuning-your-model&quot;,&quot;last_updated_on&quot;,&quot;17/07/2018&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,15,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_3894/slides/ch3_slides.pdf&quot;,&quot;title&quot;,&quot;Fine-tuning your model&quot;,&quot;xp&quot;,1200,&quot;id&quot;,5234,&quot;description&quot;,&quot;Having trained your model, your next task is to evaluate its performance. What metrics can you use to gauge how good your model is? So far, you have used accuracy for classification and R-squared for regression. In this chapter, you will learn about some of the other metrics available in  scikit-learn that will allow you to assess your model&#39;s performance in a more nuanced manner. You will then learn to optimize both your classification as well as regression models using hyperparameter tuning. &quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,4,&quot;slug&quot;,&quot;preprocessing-and-pipelines&quot;,&quot;last_updated_on&quot;,&quot;17/07/2018&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,14,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_3327/slides/ch4_slides.pdf&quot;,&quot;title&quot;,&quot;Preprocessing and pipelines&quot;,&quot;xp&quot;,1200,&quot;id&quot;,5235,&quot;description&quot;,&quot;This chapter will introduce the notion of pipelines and how scikit-learn allows for transformers and estimators to be chained together and used as a single unit. Pre-processing techniques will be then be introduced as a way to enhance model performance and pipelines will be  the glue that ties together concepts in the prior chapters.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;time_needed&quot;,null,&quot;author_image&quot;,&quot;placeholder.png&quot;,&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1939/shields/thumb/shield_image_course_1939_20180618-12-158gfc9?1529338815&quot;,&quot;topic_id&quot;,6,&quot;slug&quot;,&quot;supervised-learning-with-scikit-learn&quot;,&quot;last_updated_on&quot;,&quot;06/09/2018&quot;,&quot;paid&quot;,true,&quot;university&quot;,null,&quot;state&quot;,&quot;live&quot;,&quot;author_bio&quot;,null,&quot;should_cache&quot;,true,&quot;sharing_links&quot;,[&quot;^0&quot;,[&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;]],&quot;title&quot;,&quot;Supervised Learning with scikit-learn&quot;,&quot;xp&quot;,4300,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1939/shields/thumb_home/shield_image_course_1939_20180618-12-158gfc9?1529338815&quot;,&quot;short_description&quot;,&quot;Learn how to build and tune predictive models and evaluate how well they will perform on unseen data.&quot;,&quot;nb_of_subscriptions&quot;,82755,&quot;type&quot;,&quot;datacamp&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/supervised-learning-with-scikit-learn&quot;,&quot;id&quot;,1939,&quot;description&quot;,&quot;At the end of day, the value of Data Scientists rests on their ability to describe the world and to make predictions. Machine Learning is the field of teaching machines and computers to learn from existing data to make predictions on new data - will a given tumor be benign or malignant? Which of your customers will take their business elsewhere? Is a particular email spam or not? In this course, you&#39;ll learn how to use Python to perform supervised learning, an essential component of Machine Learning. You&#39;ll learn how to build predictive models, how to tune their parameters and how to tell how well they will perform on unseen data, all the while using real world datasets. You&#39;ll do so using scikit-learn, one of the most popular and user-friendly machine learning libraries for Python.&quot;,&quot;programming_language&quot;,&quot;python&quot;]],&quot;exercises&quot;,[&quot;^0&quot;,[&quot;current&quot;,7,&quot;all&quot;,[&quot;^18&quot;,[[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,1,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1939_supervised_learning_w_scikit_learn/v8/hls-ch2_1.master.m3u8&quot;,&quot;randomNumber&quot;,0.06627007370500415,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Introduction to regression&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,55928,&quot;projector_key&quot;,&quot;course_1939_c4f0a9bf726ed46d3a5baff1ba8c64e7&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;p&gt;In regression problems, the target variable takes on continuous values (such as the price of a house), unlike in classification problems, where the target variable falls into discrete categories (such as the integers 0 through 9 in the MNIST dataset).&lt;/p&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[&quot;An e-commerce company using labeled customer data to predict whether or not a customer will purchase a particular item.&quot;,&quot;A healthcare company using data about cancer tumors (such as their geometric measurements) to predict whether a new tumor is benign or malignant.&quot;,&quot;A restaurant using review data to ascribe positive or negative sentiment to a given review.&quot;,&quot;[A bike share company using time and weather data to predict the number of bikes being rented at any given hour.]&quot;]],&quot;number&quot;,2,&quot;randomNumber&quot;,0.7121569409537483,&quot;assignment&quot;,&quot;&lt;p&gt;Andy introduced regression to you using the Boston housing dataset. But regression models can be used in a variety of contexts to solve a variety of different problems.&lt;/p&gt;\\\\n&lt;p&gt;Given below are four example applications of machine learning. Your job is to pick the one that is &lt;em&gt;best&lt;/em&gt; framed as a &lt;strong&gt;regression&lt;/strong&gt; problem.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[&quot;Incorrect. There are only two outcomes here: Either the customer will purchase the item, or they will not. This is a classification task.&quot;,&quot;Incorrect. There are only two outcomes here: Either the tumor is benign, or it is malignant. This is a classification task.&quot;,&quot;Incorrect. The target variable here is the sentiment of a review: It can be either positive or negative. This is not a task suited to regression.&quot;,&quot;Great work! The target variable here - the number of bike rentals at any given hour - is quantitative, so this is best framed as a regression problem.&quot;]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Which of the following is a regression problem?&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;id&quot;,55929]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import numpy and pandas\\\\nimport numpy as np\\\\nimport pandas as pd\\\\n\\\\n# Read the CSV file into a DataFrame: df\\\\ndf = ____\\\\n\\\\n# Create arrays for features and target variable\\\\ny = ____\\\\nX = ____\\\\n\\\\n# Print the dimensions of X and y before reshaping\\\\nprint(\\\\&quot;Dimensions of y before reshaping: {}\\\\&quot;.format(y.shape))\\\\nprint(\\\\&quot;Dimensions of X before reshaping: {}\\\\&quot;.format(X.shape))\\\\n\\\\n# Reshape X and y\\\\ny = ____\\\\nX = ____\\\\n\\\\n# Print the dimensions of X and y after reshaping\\\\nprint(\\\\&quot;Dimensions of y after reshaping: {}\\\\&quot;.format(y.shape))\\\\nprint(\\\\&quot;Dimensions of X after reshaping: {}\\\\&quot;.format(X.shape))\\\\n&quot;,&quot;sct&quot;,&quot;Ex().test_import(&#39;numpy&#39;)\\\\nEx().test_import(&#39;pandas&#39;)\\\\n\\\\nEx().test_function(&#39;pandas.read_csv&#39;)\\\\nEx().test_object(&#39;df&#39;)\\\\n\\\\nEx().test_object(&#39;y&#39;)\\\\nEx().test_object(&#39;X&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\nEx().test_function(&#39;print&#39;, index=2)\\\\n\\\\nEx().test_function(&#39;y.reshape&#39;)\\\\nEx().test_function(&#39;X.reshape&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=3)\\\\nEx().test_function(&#39;print&#39;, index=4)\\\\n\\\\nsuccess_msg(\\\\&quot;Great work! Notice the differences in shape before and after applying the `.reshape()` method. Getting the feature and target variable arrays into the right format for scikit-learn is an important precursor to model building.\\\\&quot;)\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;pandas&lt;/code&gt; as their standard aliases.&lt;/li&gt;\\\\n&lt;li&gt;Read the file &lt;code&gt;&#39;gapminder.csv&#39;&lt;/code&gt; into a DataFrame &lt;code&gt;df&lt;/code&gt; using the &lt;code&gt;read_csv()&lt;/code&gt; function.&lt;/li&gt;\\\\n&lt;li&gt;Create array &lt;code&gt;X&lt;/code&gt; for the &lt;code&gt;&#39;fertility&#39;&lt;/code&gt; feature  and array &lt;code&gt;y&lt;/code&gt; for the &lt;code&gt;&#39;life&#39;&lt;/code&gt; target variable.&lt;/li&gt;\\\\n&lt;li&gt;Reshape the arrays by using the &lt;code&gt;.reshape()&lt;/code&gt; method and passing in &lt;code&gt;-1&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use &lt;code&gt;import x as y&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; as the alias &lt;code&gt;y&lt;/code&gt;. &lt;/li&gt;\\\\n&lt;li&gt;To read in the CSV file &lt;code&gt;&#39;gapminder.csv&#39;&lt;/code&gt;, pass it in as an argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;To create the arrays, first select the column using &lt;code&gt;df[&#39;column_name&#39;]&lt;/code&gt; and then access its &lt;code&gt;.values&lt;/code&gt; attribute.&lt;/li&gt;\\\\n&lt;li&gt;Use &lt;code&gt;.reshape(-1, 1)&lt;/code&gt; on &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; to reshape them.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,3,&quot;randomNumber&quot;,0.8386097187169055,&quot;assignment&quot;,&quot;&lt;p&gt;In this chapter, you will work with &lt;a href=\\\\&quot;https://www.gapminder.org/data/\\\\&quot;&gt;Gapminder&lt;/a&gt; data that we have consolidated into one CSV file available in the workspace as &lt;code&gt;&#39;gapminder.csv&#39;&lt;/code&gt;. Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country&#39;s GDP, fertility rate, and population. As in Chapter 1, the dataset has been preprocessed.&lt;/p&gt;\\\\n&lt;p&gt;Since the target variable here is quantitative, this is a regression problem. To begin, you will fit a linear regression with just one feature: &lt;code&gt;&#39;fertility&#39;&lt;/code&gt;, which is the average number of children a woman in a given country gives birth to. In later exercises, you will use all the features to build regression models.&lt;/p&gt;\\\\n&lt;p&gt;Before that, however, you need to import the data and get it into the form needed by scikit-learn. This involves creating feature and target variable arrays. Furthermore, since you are going to use only one feature to begin with, you need to do some reshaping using NumPy&#39;s &lt;code&gt;.reshape()&lt;/code&gt; method. Don&#39;t worry too much about this reshaping right now, but it is something you will have to do occasionally when working with scikit-learn so it is useful to practice.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Importing data for supervised learning&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;fn = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;\\\\n\\\\nfrom urllib.request import urlretrieve\\\\nurlretrieve(fn, &#39;gapminder.csv&#39;)&quot;,&quot;solution&quot;,&quot;# Import numpy and pandas\\\\nimport numpy as np\\\\nimport pandas as pd\\\\n\\\\n# Read the CSV file into a DataFrame: df\\\\ndf = pd.read_csv(&#39;gapminder.csv&#39;)\\\\n\\\\n# Create arrays for features and target variable\\\\ny = df[&#39;life&#39;].values\\\\nX = df[&#39;fertility&#39;].values\\\\n\\\\n# Print the dimensions of X and y before reshaping\\\\nprint(\\\\&quot;Dimensions of y before reshaping: {}\\\\&quot;.format(y.shape))\\\\nprint(\\\\&quot;Dimensions of X before reshaping: {}\\\\&quot;.format(X.shape))\\\\n\\\\n# Reshape X and y\\\\ny = y.reshape(-1, 1)\\\\nX = X.reshape(-1, 1)\\\\n\\\\n# Print the dimensions of X and y after reshaping\\\\nprint(\\\\&quot;Dimensions of y after reshaping: {}\\\\&quot;.format(y.shape))\\\\nprint(\\\\&quot;Dimensions of X after reshaping: {}\\\\&quot;.format(X.shape))\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,55930]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;msg1 = \\\\&quot;The DataFrame does indeed have `139` rows and 9 `columns`, as seen by using `df.info()`. Remember, `life` is a column as well, even though we are using it as our target variable.\\\\&quot;\\\\nmsg2 = \\\\&quot;This is a true statement. Look at the heatmap: the cell corresponding to `life` and `fertility` is red, indicating a negative correlation.\\\\&quot;\\\\nmsg3 = \\\\&quot;Using `df.describe()` shows that the mean of `life` is indeed `69.602878`.\\\\&quot;\\\\nmsg_success = \\\\&quot;Good job! As seen by using `df.info()`, `fertility`, along with all the other columns, is of type `float64`, **not** `int64`.\\\\&quot;\\\\nmsg5 = \\\\&quot;This is a true statement. Look at the heatmap: the cell corresponding to `GDP` and `life` is green, indicating a positive correlation.\\\\&quot;\\\\n\\\\ntest_mc(4, [msg1, msg2, msg3, msg_success, msg5]) #Option 4 is correct&quot;,&quot;instructions&quot;,[&quot;^18&quot;,[&quot;The DataFrame has &lt;code&gt;139&lt;/code&gt; samples (or rows) and &lt;code&gt;9&lt;/code&gt; columns.&quot;,&quot;&lt;code&gt;life&lt;/code&gt; and &lt;code&gt;fertility&lt;/code&gt; are negatively correlated.&quot;,&quot;The mean of &lt;code&gt;life&lt;/code&gt; is &lt;code&gt;69.602878&lt;/code&gt;.&quot;,&quot;&lt;code&gt;fertility&lt;/code&gt; is of type &lt;code&gt;int64&lt;/code&gt;.&quot;,&quot;&lt;code&gt;GDP&lt;/code&gt; and &lt;code&gt;life&lt;/code&gt; are positively correlated.&quot;]],&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;In the IPython Shell, use the &lt;code&gt;.info()&lt;/code&gt;, &lt;code&gt;.describe()&lt;/code&gt;, and &lt;code&gt;.head()&lt;/code&gt; methods on &lt;code&gt;df&lt;/code&gt; to explore the DataFrame. Observe the heatmap as well. Remember, cells in red show negative correlation, while cells in green show positive correlation.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,4,&quot;randomNumber&quot;,0.13925421161774,&quot;assignment&quot;,&quot;&lt;p&gt;As always, it is important to explore your data before building models. On the right, we have constructed a heatmap showing the correlation between the different features of the Gapminder dataset, which has been pre-loaded into a DataFrame as &lt;code&gt;df&lt;/code&gt; and is available for exploration in the IPython Shell. Cells that are in green show positive correlation, while cells that are in red show negative correlation. Take a moment to explore this: Which features are positively correlated with &lt;code&gt;life&lt;/code&gt;, and which ones are negatively correlated? Does this match your intuition? &lt;/p&gt;\\\\n&lt;p&gt;Then, in the IPython Shell, explore the DataFrame using pandas methods such as &lt;code&gt;.info()&lt;/code&gt;, &lt;code&gt;.describe()&lt;/code&gt;, &lt;code&gt;.head()&lt;/code&gt;. &lt;/p&gt;\\\\n&lt;p&gt;In case you are curious, the heatmap was generated using &lt;a href=\\\\&quot;http://seaborn.pydata.org/generated/seaborn.heatmap.html\\\\&quot;&gt;Seaborn&#39;s heatmap function&lt;/a&gt; and the following line of code, where &lt;code&gt;df.corr()&lt;/code&gt; computes the pairwise correlation between columns:&lt;/p&gt;\\\\n&lt;p&gt;&lt;code&gt;sns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;)&lt;/code&gt;&lt;/p&gt;\\\\n&lt;p&gt;Once you have a feel for the data, consider the statements below and select the one that is &lt;strong&gt;not&lt;/strong&gt; true. After this, Hugo will explain the mechanics of linear regression in the next video and you will be on your way building regression models!&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Exploring the Gapminder data&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nimport seaborn as sns\\\\nfrom urllib.request import urlretrieve\\\\n\\\\nfn = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;\\\\nurlretrieve(fn, &#39;gapminder.csv&#39;)\\\\n\\\\ndf = pd.read_csv(&#39;gapminder.csv&#39;)\\\\n\\\\nsns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;)\\\\n\\\\nplt.xticks(rotation=90) \\\\nplt.yticks(rotation=0)\\\\n\\\\nplt.show()&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;id&quot;,101796]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,5,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1939_supervised_learning_w_scikit_learn/v7/hls-ch2_2.master.m3u8&quot;,&quot;randomNumber&quot;,0.18733165875262814,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;The basics of linear regression&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,55931,&quot;projector_key&quot;,&quot;course_1939_c2c0d781d764cf5b8310a165fe711cfe&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import LinearRegression\\\\n____\\\\n\\\\n# Create the regressor: reg\\\\nreg = ____\\\\n\\\\n# Create the prediction space\\\\nprediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\\\\n\\\\n# Fit the model to the data\\\\n____\\\\n\\\\n# Compute predictions over the prediction space: y_pred\\\\ny_pred = ____\\\\n\\\\n# Print R^2 \\\\nprint(reg.score(____, ____))\\\\n\\\\n# Plot regression line\\\\nplt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3)\\\\nplt.show()\\\\n&quot;,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.LinearRegression&#39;,\\\\n                 not_imported_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.LinearRegression&#39;)\\\\nEx().test_object(&#39;reg&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;numpy.linspace&#39;)\\\\nEx().test_function(&#39;numpy.linspace.reshape&#39;)\\\\nEx().test_object(&#39;prediction_space&#39;)\\\\n\\\\nEx().test_function(&#39;reg.fit&#39;)\\\\nEx().check_function(&#39;reg.predict&#39;, index=0).check_args(0).has_equal_value(&#39;Did you use `.predict()` to compute the predictions of `reg` over `prediction_space`?&#39;, error_msg=&#39;Did you use `.predict()` compute the predictions of `reg` over `prediction_space`?&#39;)\\\\nEx().test_object(&#39;y_pred&#39;)\\\\n\\\\nEx().test_function(&#39;reg.score&#39;)\\\\nEx().test_function(&#39;print&#39;)\\\\n\\\\nEx().test_function(&#39;matplotlib.pyplot.plot&#39;, incorrect_msg=&#39;The code to plot the regression line has been provided for you. You don\\\\\\\\&#39;t have to change this.&#39;, not_called_msg=&#39;The code to plot the regression line has been provided for you. You don\\\\\\\\&#39;t have to change this.&#39;)\\\\nEx().test_function(&#39;matplotlib.pyplot.show&#39;, incorrect_msg=&#39;The code to plot the regression line has been provided for you. You don\\\\\\\\&#39;t have to change this.&#39;, not_called_msg=&#39;The code to plot the regression line has been provided for you. You don\\\\\\\\&#39;t have to change this.&#39;)\\\\n\\\\nsuccess_msg(\\\\&quot;Fantastic! Notice how the line captures the underlying trend in the data. And the performance is quite decent for this basic regression model with only one feature!\\\\&quot;)\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;LinearRegression&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Create a &lt;code&gt;LinearRegression&lt;/code&gt; regressor called &lt;code&gt;reg&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Set up the prediction space to range from the minimum to the maximum of &lt;code&gt;X_fertility&lt;/code&gt;. This has been done for you.&lt;/li&gt;\\\\n&lt;li&gt;Fit the regressor to the data (&lt;code&gt;X_fertility&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;) and compute its predictions using the &lt;code&gt;.predict()&lt;/code&gt; method and the &lt;code&gt;prediction_space&lt;/code&gt; array.&lt;/li&gt;\\\\n&lt;li&gt;Compute and print the \\\\\\\\(R^2\\\\\\\\) score using the &lt;code&gt;.score()&lt;/code&gt; method.&lt;/li&gt;\\\\n&lt;li&gt;Overlay the plot with your linear regression line. This has been done for you, so hit &#39;Submit Answer&#39; to see the result!&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;You can import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt; using the command &lt;code&gt;from y import x&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the function &lt;code&gt;LinearRegression()&lt;/code&gt; to create the regressor.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;.fit()&lt;/code&gt; method on &lt;code&gt;reg&lt;/code&gt; with &lt;code&gt;X_fertility&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; as arguments to fit the model.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;.predict()&lt;/code&gt; method on &lt;code&gt;reg&lt;/code&gt; with &lt;code&gt;prediction_space&lt;/code&gt; as the argument to compute the predictions.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;.score()&lt;/code&gt; method with &lt;code&gt;X_fertility&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; as arguments to compute the \\\\\\\\(R^2\\\\\\\\) score.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,6,&quot;randomNumber&quot;,0.28675428230407807,&quot;assignment&quot;,&quot;&lt;p&gt;Now, you will fit a linear regression and predict life expectancy using just one feature. You saw Andy do this earlier using the &lt;code&gt;&#39;RM&#39;&lt;/code&gt; feature of the Boston housing dataset. In this exercise, you will use the &lt;code&gt;&#39;fertility&#39;&lt;/code&gt; feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is &lt;code&gt;&#39;life&#39;&lt;/code&gt;. The array for the target variable has been pre-loaded as &lt;code&gt;y&lt;/code&gt; and the array for &lt;code&gt;&#39;fertility&#39;&lt;/code&gt; has been pre-loaded as &lt;code&gt;X_fertility&lt;/code&gt;.&lt;/p&gt;\\\\n&lt;p&gt;A scatter plot with &lt;code&gt;&#39;fertility&#39;&lt;/code&gt; on the x-axis and &lt;code&gt;&#39;life&#39;&lt;/code&gt; on the y-axis has been generated. As you can see, there is a strongly negative correlation, so a linear regression should be able to capture this trend. Your job is to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. You will also compute and print the \\\\\\\\(R^2\\\\\\\\) score using sckit-learn&#39;s &lt;code&gt;.score()&lt;/code&gt; method.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Fit &amp; predict for regression&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\n\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1)\\\\n\\\\n# Reshape to 1-D\\\\ny = y.reshape(-1, 1)\\\\nX_fertility = X[&#39;fertility&#39;].reshape(-1, 1) \\\\n\\\\n_ = plt.scatter(X[&#39;fertility&#39;], y, color=&#39;blue&#39;)\\\\n_ = plt.ylabel(&#39;Life Expectancy&#39;)\\\\n_ = plt.xlabel(&#39;Fertility&#39;)\\\\nplt.show()\\\\n&quot;,&quot;solution&quot;,&quot;# Import LinearRegression\\\\nfrom sklearn.linear_model import LinearRegression\\\\n\\\\n# Create the regressor: reg\\\\nreg = LinearRegression()\\\\n\\\\n# Create the prediction space\\\\nprediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\\\\n\\\\n# Fit the model to the data\\\\nreg.fit(X_fertility, y)\\\\n\\\\n# Compute predictions over the prediction space: y_pred\\\\ny_pred = reg.predict(prediction_space)\\\\n\\\\n# Print R^2 \\\\nprint(reg.score(X_fertility, y))\\\\n\\\\n# Plot regression line\\\\nplt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3)\\\\nplt.show()\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,55932]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import necessary modules\\\\n____\\\\n____\\\\n____\\\\n\\\\n# Create training and test sets\\\\nX_train, X_test, y_train, y_test = ____(____, ____, test_size = ____, random_state=____)\\\\n\\\\n# Create the regressor: reg_all\\\\nreg_all = ____\\\\n\\\\n# Fit the regressor to the training data\\\\n____\\\\n\\\\n# Predict on the test data: y_pred\\\\ny_pred = ____\\\\n\\\\n# Compute and print R^2 and RMSE\\\\nprint(\\\\&quot;R^2: {}\\\\&quot;.format(reg_all.score(X_test, y_test)))\\\\nrmse = np.sqrt(____)\\\\nprint(\\\\&quot;Root Mean Squared Error: {}\\\\&quot;.format(rmse))\\\\n&quot;,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.LinearRegression&#39;,\\\\n                 not_imported_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;)\\\\n                 \\\\nEx().test_import(&#39;sklearn.metrics.mean_squared_error&#39;,\\\\n                 not_imported_msg=&#39;Did you import `mean_squared_error` from `sklearn.metrics`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `mean_squared_error` from `sklearn.metrics`?&#39;)\\\\n                 \\\\nEx().test_import(&#39;sklearn.model_selection.train_test_split&#39;,\\\\n                 not_imported_msg=&#39;Did you import `train_test_split` from `sklearn.model_selection`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `train_test_split` from `sklearn.model_selection`?&#39;)\\\\n\\\\nEx().test_correct(test_object(&#39;X_train&#39;), test_function(&#39;sklearn.model_selection.train_test_split&#39;, args_not_specified_msg=&#39;Have you specified all required arguments inside `train_test_split()`? After passing in `X` and `y`, you need to specify the `train_size` **or** `test_size` and the `random_state`.&#39;))\\\\nEx().test_correct(test_object(&#39;X_test&#39;), test_function(&#39;sklearn.model_selection.train_test_split&#39;, args_not_specified_msg=&#39;Have you specified all required arguments inside `train_test_split()`? After passing in `X` and `y`, you need to specify the `train_size` **or** `test_size` and the `random_state`.&#39;))\\\\nEx().test_correct(test_object(&#39;y_train&#39;), test_function(&#39;sklearn.model_selection.train_test_split&#39;, args_not_specified_msg=&#39;Have you specified all required arguments inside `train_test_split()`? After passing in `X` and `y`, you need to specify the `train_size` **or** `test_size` and the `random_state`.&#39;))\\\\nEx().test_correct(test_object(&#39;y_test&#39;), test_function(&#39;sklearn.model_selection.train_test_split&#39;, args_not_specified_msg=&#39;Have you specified all required arguments inside `train_test_split()`? After passing in `X` and `y`, you need to specify the `train_size` **or** `test_size` and the `random_state`.&#39;))\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.LinearRegression&#39;)\\\\nEx().test_object(&#39;reg_all&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;reg_all.predict&#39;)\\\\n#Ex().test_object(&#39;y_pred&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\nEx().test_function(&#39;sklearn.metrics.mean_squared_error&#39;)\\\\nEx().test_function(&#39;numpy.sqrt&#39;)\\\\nEx().test_object(&#39;rmse&#39;)\\\\nEx().test_function(&#39;print&#39;, index=2)\\\\n\\\\nsuccess_msg(\\\\&quot;Excellent! Using all features has improved the model score. This makes sense, as the model has more information to learn from. However, there is one potential pitfall to this process. Can you spot it? You&#39;ll learn about this as well how to better validate your models in the next video!\\\\&quot;)\\\\n\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;LinearRegression&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt;, &lt;code&gt;mean_squared_error&lt;/code&gt; from &lt;code&gt;sklearn.metrics&lt;/code&gt;, and &lt;code&gt;train_test_split&lt;/code&gt; from &lt;code&gt;sklearn.model_selection&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Using &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, create training and test sets such that 30% is used for testing and 70% for training. Use a random state of &lt;code&gt;42&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Create a linear regression regressor called &lt;code&gt;reg_all&lt;/code&gt;, fit it to the training set, and evaluate it on the test set.&lt;/li&gt;\\\\n&lt;li&gt;Compute and print the \\\\\\\\(R^2\\\\\\\\) score using the &lt;code&gt;.score()&lt;/code&gt; method on the test set.&lt;/li&gt;\\\\n&lt;li&gt;Compute and print the RMSE. To do this, first compute the Mean Squared Error using the &lt;code&gt;mean_squared_error()&lt;/code&gt; function with the arguments &lt;code&gt;y_test&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt;, and then take its square root using &lt;code&gt;np.sqrt()&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use the command &lt;code&gt;from y import x&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use &lt;code&gt;train_test_split()&lt;/code&gt; to create training and test sets. Pass in the arguments &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, and specify the test set size using &lt;code&gt;test_size&lt;/code&gt; (&lt;strong&gt;or&lt;/strong&gt; the training set size using &lt;code&gt;train_size&lt;/code&gt;) and the random state using &lt;code&gt;random_state&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;To create the regressor, use &lt;code&gt;LinearRegression()&lt;/code&gt;. Then use &lt;code&gt;.fit()&lt;/code&gt; to fit it to &lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt;, and &lt;code&gt;.predict()&lt;/code&gt; to evaluate it over &lt;code&gt;X_test&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;To compute the RMSE, compute the mean squared error inside the provided &lt;code&gt;np.sqrt()&lt;/code&gt; function using the &lt;code&gt;mean_squared_error()&lt;/code&gt; function. &lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,7,&quot;randomNumber&quot;,0.6935809439079497,&quot;assignment&quot;,&quot;&lt;p&gt;As you learned in Chapter 1, train and test sets are vital to ensure that your supervised learning model is able to generalize well to new data. This was true for classification models, and is equally true for linear regression models. &lt;/p&gt;\\\\n&lt;p&gt;In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over &lt;strong&gt;all&lt;/strong&gt; features. In addition to computing the \\\\\\\\(R^2\\\\\\\\) score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. The feature array &lt;code&gt;X&lt;/code&gt; and target variable array &lt;code&gt;y&lt;/code&gt; have been pre-loaded for you from the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Train/test split for regression&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values\\\\n&quot;,&quot;solution&quot;,&quot;# Import necessary modules\\\\nfrom sklearn.linear_model import LinearRegression\\\\nfrom sklearn.metrics import mean_squared_error\\\\nfrom sklearn.model_selection import train_test_split\\\\n\\\\n# Create training and test sets\\\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\\\\n\\\\n# Create the regressor: reg_all\\\\nreg_all = LinearRegression()\\\\n\\\\n# Fit the regressor to the training data\\\\nreg_all.fit(X_train, y_train)\\\\n\\\\n# Predict on the test data: y_pred\\\\ny_pred = reg_all.predict(X_test)\\\\n\\\\n# Compute and print R^2 and RMSE\\\\nprint(\\\\&quot;R^2: {}\\\\&quot;.format(reg_all.score(X_test, y_test)))\\\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\\\\nprint(\\\\&quot;Root Mean Squared Error: {}\\\\&quot;.format(rmse))\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,68998]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,8,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1939_supervised_learning_w_scikit_learn/v5/hls-ch2_3.master.m3u8&quot;,&quot;user&quot;,[&quot;^0&quot;,[&quot;isHintShown&quot;,false,&quot;rstudio&quot;,[&quot;^0&quot;,[&quot;isReady&quot;,false,&quot;settings&quot;,[&quot;^0&quot;,[]],&quot;showHistory&quot;,false,&quot;cards&quot;,[&quot;^0&quot;,[&quot;messages&quot;,[&quot;^18&quot;,[]],&quot;currentRow&quot;,0]]]],&quot;editorTabs&quot;,[&quot;^0&quot;,[&quot;files/script.py&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;script.py&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true,&quot;isClosable&quot;,false,&quot;code&quot;,&quot;&quot;,&quot;extra&quot;,[&quot;^0&quot;,[]],&quot;resetCode&quot;,&quot;&quot;]]]]]],&quot;fileBrowser&quot;,[&quot;^0&quot;,[&quot;sampleCode&quot;,[&quot;^0&quot;,[&quot;fileSelected&quot;,null,&quot;files&quot;,[&quot;^0&quot;,[&quot;name&quot;,&quot;files&quot;,&quot;isOpen&quot;,true,&quot;children&quot;,[&quot;^18&quot;,[[&quot;^0&quot;,[&quot;name&quot;,&quot;script.py&quot;,&quot;initialContent&quot;,&quot;&quot;,&quot;content&quot;,&quot;&quot;,&quot;isClosable&quot;,false]]]]]]]],&quot;solution&quot;,[&quot;^0&quot;,[&quot;fileSelected&quot;,null,&quot;files&quot;,[&quot;^0&quot;,[&quot;name&quot;,&quot;solution&quot;,&quot;isOpen&quot;,true,&quot;children&quot;,[&quot;^18&quot;,[[&quot;^0&quot;,[&quot;name&quot;,&quot;solution.py&quot;,&quot;initialContent&quot;,&quot;&quot;,&quot;content&quot;,&quot;&quot;,&quot;isClosable&quot;,false]]]]]]]]]],&quot;outputMarkdownTabs&quot;,[&quot;^0&quot;,[]],&quot;markdown&quot;,[&quot;^0&quot;,[&quot;titles&quot;,[&quot;^18&quot;,[&quot;Knit PDF&quot;,&quot;Knit HTML&quot;]],&quot;activeTitle&quot;,&quot;Knit HTML&quot;]],&quot;currentXp&quot;,50,&quot;graphicalTabs&quot;,[&quot;^0&quot;,[&quot;plot&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;Plots&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;sources&quot;,[&quot;^18&quot;,[]],&quot;currentIndex&quot;,0]],&quot;dimension&quot;,[&quot;^0&quot;,[&quot;isRealSize&quot;,false,&quot;width&quot;,1,&quot;height&quot;,1]]]],&quot;html&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;HTML Viewer&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;sources&quot;,[&quot;^18&quot;,[]],&quot;currentIndex&quot;,0]]]]]],&quot;feedbackMessages&quot;,[&quot;^18&quot;,[]],&quot;lastSubmittedCode&quot;,null,&quot;ltiStatus&quot;,[&quot;^0&quot;,[]],&quot;lastSubmitActiveEditorTab&quot;,null,&quot;consoleSqlTabs&quot;,[&quot;^0&quot;,[&quot;query_result&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;&quot;,&quot;title&quot;,&quot;query result&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true,&quot;isNotView&quot;,true,&quot;message&quot;,&quot;No query executed yet...&quot;]]]]]],&quot;consoleTabs&quot;,[&quot;^0&quot;,[&quot;console&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;IPython Shell&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true]]]],&quot;slides&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;Slides&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,false]]]]]],&quot;inputMarkdownTabs&quot;,[&quot;^0&quot;,[]]]],&quot;randomNumber&quot;,0.7864841993659184,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Cross-validation&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,68999,&quot;projector_key&quot;,&quot;course_1939_958a0c679de5c7d3b0b0a5acec08ba22&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import the necessary modules\\\\n____\\\\n____\\\\n\\\\n# Create a linear regression object: reg\\\\nreg = ____\\\\n\\\\n# Compute 5-fold cross-validation scores: cv_scores\\\\ncv_scores = ____\\\\n\\\\n# Print the 5-fold cross-validation scores\\\\nprint(____)\\\\n\\\\nprint(\\\\&quot;Average 5-Fold CV Score: {}\\\\&quot;.format(____(____)))\\\\n&quot;,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.LinearRegression&#39;,\\\\n                 not_imported_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;)\\\\n\\\\nEx().test_import(&#39;sklearn.model_selection.cross_val_score&#39;,\\\\n                 not_imported_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.LinearRegression&#39;)\\\\nEx().test_object(&#39;reg&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;sklearn.model_selection.cross_val_score&#39;, do_eval=False)\\\\nEx().test_object(&#39;cv_scores&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\nEx().test_function(&#39;print&#39;, index=2)\\\\n\\\\nsuccess_msg(\\\\&quot;Great work! Now that you have cross-validated your model, you can more confidently evaluate its predictions.\\\\&quot;)\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;LinearRegression&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt; and &lt;code&gt;cross_val_score&lt;/code&gt; from &lt;code&gt;sklearn.model_selection&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Create a linear regression regressor called &lt;code&gt;reg&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;cross_val_score()&lt;/code&gt; function to perform 5-fold cross-validation on &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Compute and print the average cross-validation score. You can use NumPy&#39;s &lt;code&gt;mean()&lt;/code&gt; function to compute the average.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use the command &lt;code&gt;from y import x&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;LinearRegression()&lt;/code&gt; function to create the regressor.&lt;/li&gt;\\\\n&lt;li&gt;Inside &lt;code&gt;cross_val_score()&lt;/code&gt;, specify the arguments &lt;code&gt;reg&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;cv=5&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Pass &lt;code&gt;cv_scores&lt;/code&gt; to the first function, and &lt;code&gt;np.mean(cv_scores)&lt;/code&gt; to the &lt;code&gt;format()&lt;/code&gt; function of the second &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,9,&quot;randomNumber&quot;,0.015515917094008236,&quot;assignment&quot;,&quot;&lt;p&gt;Cross-validation is a vital step in evaluating a model. It maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data.&lt;/p&gt;\\\\n&lt;p&gt;In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn&#39;s &lt;code&gt;cross_val_score()&lt;/code&gt; function uses \\\\\\\\(R^2\\\\\\\\) as the metric of choice for regression. Since you are performing 5-fold cross-validation, the function will return 5 scores. Your job is to compute these 5 scores and then take their average.&lt;/p&gt;\\\\n&lt;p&gt;The DataFrame has been loaded as &lt;code&gt;df&lt;/code&gt; and split into the feature/target variable arrays &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. The modules &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; have been imported as &lt;code&gt;pd&lt;/code&gt; and &lt;code&gt;np&lt;/code&gt;, respectively.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;5-fold cross-validation&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values&quot;,&quot;solution&quot;,&quot;# Import the necessary modules\\\\nfrom sklearn.linear_model import LinearRegression\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Create a linear regression object: reg\\\\nreg = LinearRegression()\\\\n\\\\n# Compute 5-fold cross-validation scores: cv_scores\\\\ncv_scores = cross_val_score(reg, X, y, cv=5)\\\\n\\\\n# Print the 5-fold cross-validation scores\\\\nprint(cv_scores)\\\\n\\\\n# Print the average 5-fold cross-validation score\\\\nprint(\\\\&quot;Average 5-Fold CV Score: {}\\\\&quot;.format(np.mean(cv_scores)))\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,62360]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import necessary modules\\\\n____\\\\n____\\\\n\\\\n# Create a linear regression object: reg\\\\nreg = ____\\\\n\\\\n# Perform 3-fold CV\\\\ncvscores_3 = ____\\\\nprint(np.mean(cvscores_3))\\\\n\\\\n# Perform 10-fold CV\\\\ncvscores_10 = ____\\\\nprint(np.mean(cvscores_10))\\\\n&quot;,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.LinearRegression&#39;,\\\\n                 not_imported_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `LinearRegression` from `sklearn.linear_model`?&#39;)\\\\n                 \\\\nEx().test_import(&#39;sklearn.model_selection.cross_val_score&#39;,\\\\n                 not_imported_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.LinearRegression&#39;, do_eval=False)\\\\nEx().test_object(&#39;reg&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;sklearn.model_selection.cross_val_score&#39;, do_eval=False, index=1)\\\\nEx().test_object(&#39;cvscores_3&#39;)\\\\n\\\\nEx().test_function(&#39;numpy.mean&#39;, index=1)\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\n\\\\nEx().test_function(&#39;sklearn.model_selection.cross_val_score&#39;, do_eval=False, index=2)\\\\nEx().test_object(&#39;cvscores_10&#39;)\\\\nEx().test_function(&#39;numpy.mean&#39;, index=2)\\\\nEx().test_function(&#39;print&#39;, index=2)\\\\n\\\\nsuccess_msg(\\\\&quot;Excellent! Did you use `%timeit` in the IPython Shell to see how much longer it takes 10-fold cross-validation to run compared to 3-fold cross-validation?\\\\&quot;)\\\\n\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;LinearRegression&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt; and &lt;code&gt;cross_val_score&lt;/code&gt; from &lt;code&gt;sklearn.model_selection&lt;/code&gt;. &lt;/li&gt;\\\\n&lt;li&gt;Create a linear regression regressor called &lt;code&gt;reg&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Perform 3-fold CV and then 10-fold CV. Compare the resulting mean scores.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use the command &lt;code&gt;from y import x&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;LinearRegression()&lt;/code&gt; function to create the regressor.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;cross_val_score()&lt;/code&gt; function with &lt;code&gt;reg&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; as arguments. For 3-fold CV, specify &lt;code&gt;cv=3&lt;/code&gt;, and for 10-fold CV, specify &lt;code&gt;cv=10&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,10,&quot;randomNumber&quot;,0.3140520886777385,&quot;assignment&quot;,&quot;&lt;p&gt;Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset.&lt;/p&gt;\\\\n&lt;p&gt;In the IPython Shell, you can use &lt;code&gt;%timeit&lt;/code&gt; to see how long each 3-fold CV takes compared to 10-fold CV by executing the following &lt;code&gt;cv=3&lt;/code&gt; and &lt;code&gt;cv=10&lt;/code&gt;:&lt;/p&gt;\\\\n&lt;pre&gt;&lt;code&gt;%timeit cross_val_score(reg, X, y, cv = ____)\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\n&lt;p&gt;&lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; are available in the workspace as &lt;code&gt;pd&lt;/code&gt; and &lt;code&gt;np&lt;/code&gt;. The DataFrame has been loaded as &lt;code&gt;df&lt;/code&gt; and the feature/target variable arrays &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; have been created.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;K-Fold CV comparison&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values\\\\n\\\\n# Import necessary modules\\\\nfrom sklearn import linear_model\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Create a linear regression object\\\\nreg = linear_model.LinearRegression()\\\\n\\\\n&quot;,&quot;solution&quot;,&quot;# Import necessary modules\\\\nfrom sklearn.linear_model import LinearRegression\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Create a linear regression object: reg\\\\nreg = LinearRegression()\\\\n\\\\n# Perform 3-fold CV\\\\ncvscores_3 = cross_val_score(reg, X, y, cv = 3)\\\\nprint(np.mean(cvscores_3))\\\\n\\\\n# Perform 10-fold CV\\\\ncvscores_10 = cross_val_score(reg, X, y, cv = 10)\\\\nprint(np.mean(cvscores_10))\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,69000]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,11,&quot;video_hls&quot;,&quot;//videos.datacamp.com/transcoded/1939_supervised_learning_w_scikit_learn/v4/hls-ch2_4.master.m3u8&quot;,&quot;randomNumber&quot;,0.760492612055184,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Regularized regression&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,62363,&quot;projector_key&quot;,&quot;course_1939_f9e49441a9f1a28282ee5ba0c965cf9d&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import Lasso\\\\nfrom sklearn.linear_model import Lasso\\\\n\\\\n# Instantiate a lasso regressor: lasso\\\\nlasso = ____\\\\n\\\\n# Fit the regressor to the data\\\\n____\\\\n\\\\n# Compute and print the coefficients\\\\nlasso_coef = ____\\\\nprint(lasso_coef)\\\\n\\\\n# Plot the coefficients\\\\nplt.plot(range(len(df_columns)), lasso_coef)\\\\nplt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\\\\nplt.margins(0.02)\\\\nplt.show()\\\\n&quot;,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.Lasso&#39;,\\\\n                 not_imported_msg=&#39;Did you import `Lasso` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `Lasso` from `sklearn.linear_model`?&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.Lasso&#39;)\\\\nEx().test_object(&#39;lasso&#39;, do_eval=False)\\\\n\\\\nEx().test_function(&#39;lasso.fit&#39;)\\\\nEx().test_object_accessed(&#39;lasso.coef_&#39;)\\\\nEx().test_object(&#39;lasso_coef&#39;)\\\\n\\\\nEx().test_function(&#39;print&#39;, index=1)\\\\n\\\\nEx().test_function(&#39;matplotlib.pyplot.plot&#39;)\\\\nEx().test_function(&#39;matplotlib.pyplot.xticks&#39;)\\\\nEx().test_function(&#39;matplotlib.pyplot.margins&#39;)\\\\nEx().test_function(&#39;matplotlib.pyplot.show&#39;)\\\\n\\\\nsuccess_msg(\\\\&quot;Great work! According to the lasso algorithm, it seems like `&#39;child_mortality&#39;` is the most important feature when predicting life expectancy.\\\\&quot;)\\\\n\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Import &lt;code&gt;Lasso&lt;/code&gt; from &lt;code&gt;sklearn.linear_model&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Instantiate a Lasso regressor with an alpha of &lt;code&gt;0.4&lt;/code&gt; and specify &lt;code&gt;normalize=True&lt;/code&gt;. &lt;/li&gt;\\\\n&lt;li&gt;Fit the regressor to the data and compute the coefficients using the &lt;code&gt;coef_&lt;/code&gt; attribute.&lt;/li&gt;\\\\n&lt;li&gt;Plot the coefficients on the y-axis and column names on the x-axis. This has been done for you, so hit &#39;Submit Answer&#39; to view the plot!&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use the command &lt;code&gt;from y import x&lt;/code&gt; to import &lt;code&gt;x&lt;/code&gt; from &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;Lasso()&lt;/code&gt; function with &lt;code&gt;alpha=0.4&lt;/code&gt; and &lt;code&gt;normalize=True&lt;/code&gt; to instantiate the regressor.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;.fit()&lt;/code&gt; method of the regressor on &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; to fit it to the data.&lt;/li&gt;\\\\n&lt;li&gt;To compute the coefficients, access the &lt;code&gt;.coef_&lt;/code&gt; attribute of the lasso regressor.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,12,&quot;randomNumber&quot;,0.05695502255452389,&quot;assignment&quot;,&quot;&lt;p&gt;In the video, you saw how Lasso selected out the &lt;code&gt;&#39;RM&#39;&lt;/code&gt; feature as being the most important for predicting Boston house prices, while shrinking the coefficients of certain other features to 0. Its ability to perform feature selection in this way becomes even more useful when you are dealing with data involving thousands of features. &lt;/p&gt;\\\\n&lt;p&gt;In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining.&lt;/p&gt;\\\\n&lt;p&gt;The feature and target variable arrays have been pre-loaded as &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Regularization I: Lasso&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values\\\\n\\\\ndf_columns = df.drop(&#39;life&#39;, axis=1).columns\\\\n&quot;,&quot;solution&quot;,&quot;# Import Lasso\\\\nfrom sklearn.linear_model import Lasso\\\\n\\\\n# Instantiate a lasso regressor: lasso\\\\nlasso = Lasso(alpha=0.4, normalize=True)\\\\n\\\\n# Fit the regressor to the data\\\\nlasso.fit(X, y)\\\\n\\\\n# Compute and print the coefficients\\\\nlasso_coef = lasso.coef_\\\\nprint(lasso_coef)\\\\n\\\\n# Plot the coefficients\\\\nplt.plot(range(len(df_columns)), lasso_coef)\\\\nplt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\\\\nplt.margins(0.02)\\\\nplt.show()\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,69001]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import necessary modules\\\\nfrom sklearn.linear_model import Ridge\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Setup the array of alphas and lists to store scores\\\\nalpha_space = np.logspace(-4, 0, 50)\\\\nridge_scores = []\\\\nridge_scores_std = []\\\\n\\\\n# Create a ridge regressor: ridge\\\\nridge = ____\\\\n\\\\n# Compute scores over range of alphas\\\\nfor alpha in alpha_space:\\\\n\\\\n    # Specify the alpha value to use: ridge.alpha\\\\n    ridge.alpha = ____\\\\n    \\\\n    # Perform 10-fold CV: ridge_cv_scores\\\\n    ridge_cv_scores = ____\\\\n    \\\\n    # Append the mean of ridge_cv_scores to ridge_scores\\\\n    ridge_scores.append(____(____))\\\\n    \\\\n    # Append the std of ridge_cv_scores to ridge_scores_std\\\\n    ridge_scores_std.append(____(____))\\\\n\\\\n# Display the plot\\\\ndisplay_plot(ridge_scores, ridge_scores_std)\\\\n&quot;,&quot;sct&quot;,&quot;Ex().test_import(&#39;sklearn.linear_model.Ridge&#39;,\\\\n                 not_imported_msg=&#39;Did you import `Ridge` from `sklearn.linear_model`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `Ridge` from `sklearn.linear_model`?&#39;)\\\\n\\\\nEx().test_import(&#39;sklearn.model_selection.cross_val_score&#39;,\\\\n                 not_imported_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;,\\\\n                 incorrect_as_msg=&#39;Did you import `cross_val_score` from `sklearn.model_selection`?&#39;)\\\\n\\\\nEx().test_function(&#39;numpy.logspace&#39;, incorrect_msg=&#39;The array of alphas has been setup for you. You don\\\\\\\\&#39;t have to modify this.&#39;)\\\\nEx().test_object(&#39;alpha_space&#39;, incorrect_msg=&#39;The array of alphas has been setup for you. You don\\\\\\\\&#39;t have to modify this.&#39;)\\\\n\\\\nEx().test_function(&#39;sklearn.linear_model.Ridge&#39;)\\\\n\\\\nEx().check_for_loop(0).check_iter().has_equal_value()\\\\n\\\\nEx().check_for_loop(0).check_body().test_function(&#39;numpy.mean&#39;, incorrect_msg=&#39;Did you correctly compute the mean of `ridge_cv_scores`? Ensure that you have first computed `ridge_cv_scores` correctly by performing 10-fold CV, and then that you computed its mean using `np.mean()`.&#39;)\\\\nEx().check_for_loop(0).check_body().test_function(&#39;ridge_scores.append&#39;)\\\\n\\\\nEx().check_for_loop(0).check_body().test_function(&#39;numpy.std&#39;)\\\\nEx().check_for_loop(0).check_body().test_function(&#39;ridge_scores_std.append&#39;)\\\\n\\\\n\\\\nsuccess_msg(\\\\&quot;Great work! Notice how the cross-validation scores change with different alphas. Which alpha should you pick? How can you fine-tune your model? You&#39;ll learn all about this in the next chapter!\\\\&quot;)\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Instantiate a Ridge regressor and specify &lt;code&gt;normalize=True&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Inside the &lt;code&gt;for&lt;/code&gt; loop:&lt;ul&gt;\\\\n&lt;li&gt;Specify the alpha value for the regressor to use.&lt;/li&gt;\\\\n&lt;li&gt;Perform 10-fold cross-validation on the regressor with the specified alpha. The data is available in the arrays &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Append the average and the standard deviation of the computed cross-validated scores. NumPy has been pre-imported for you as &lt;code&gt;np&lt;/code&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;display_plot()&lt;/code&gt; function to visualize the scores and standard deviations.&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\n&lt;li&gt;Use &lt;code&gt;Ridge()&lt;/code&gt; to create the ridge regressor, and specify &lt;code&gt;normalize=True&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Inside the &lt;code&gt;for&lt;/code&gt; loop:&lt;ul&gt;\\\\n&lt;li&gt;Specify the alpha value by assigning &lt;code&gt;alpha&lt;/code&gt; to &lt;code&gt;ridge.alpha&lt;/code&gt;.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;cross_val_score()&lt;/code&gt; function with &lt;code&gt;ridge&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;cv=10&lt;/code&gt; as arguments.&lt;/li&gt;\\\\n&lt;li&gt;Use the &lt;code&gt;np.mean()&lt;/code&gt; and &lt;code&gt;np.std()&lt;/code&gt; functions to compute the mean and standard deviation of &lt;code&gt;ridge_cv_scores&lt;/code&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^18&quot;,[]],&quot;number&quot;,13,&quot;randomNumber&quot;,0.4314734773824245,&quot;assignment&quot;,&quot;&lt;p&gt;Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice.&lt;/p&gt;\\\\n&lt;p&gt;Recall that lasso performs regularization by adding to the loss function a penalty term of the &lt;em&gt;absolute&lt;/em&gt; value of each coefficient multiplied by some alpha. This is also known as \\\\\\\\(L1\\\\\\\\) regularization because the regularization term is the \\\\\\\\(L1\\\\\\\\) norm of the coefficients. This is not the only way to regularize, however. &lt;/p&gt;\\\\n&lt;p&gt;If instead you took the sum of the &lt;em&gt;squared&lt;/em&gt; values of the coefficients multiplied by some alpha - like in Ridge regression - you would be computing the \\\\\\\\(L2\\\\\\\\) norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated \\\\\\\\(R^2\\\\\\\\) scores for each, using this function that we have defined for you, which plots the \\\\\\\\(R^2\\\\\\\\) score as well as standard error for each alpha:&lt;/p&gt;\\\\n&lt;pre&gt;&lt;code&gt;def display_plot(cv_scores, cv_scores_std):\\\\n    fig = plt.figure()\\\\n    ax = fig.add_subplot(1,1,1)\\\\n    ax.plot(alpha_space, cv_scores)\\\\n\\\\n    std_error = cv_scores_std / np.sqrt(10)\\\\n\\\\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\\\\n    ax.set_ylabel(&#39;CV Score +/- Std Error&#39;)\\\\n    ax.set_xlabel(&#39;Alpha&#39;)\\\\n    ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;)\\\\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\\\\n    ax.set_xscale(&#39;log&#39;)\\\\n    plt.show()\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\n&lt;p&gt;Don&#39;t worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the \\\\\\\\(R^2\\\\\\\\) score varies with different alphas, and to understand the importance of selecting the right value for alpha. You&#39;ll learn how to tune alpha in the next chapter.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^18&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Regularization II: Ridge&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nplt.style.use(&#39;ggplot&#39;)\\\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_2433/datasets/gapminder-clean.csv&#39;)\\\\n\\\\ny = df[&#39;life&#39;].values\\\\nX = df.drop(&#39;life&#39;, axis=1).values\\\\n\\\\ndef display_plot(cv_scores, cv_scores_std):\\\\n    fig = plt.figure()\\\\n    ax = fig.add_subplot(1,1,1)\\\\n    ax.plot(alpha_space, cv_scores)\\\\n\\\\n    std_error = cv_scores_std / np.sqrt(10)\\\\n\\\\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\\\\n    ax.set_ylabel(&#39;CV Score +/- Std Error&#39;)\\\\n    ax.set_xlabel(&#39;Alpha&#39;)\\\\n    ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;)\\\\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\\\\n    ax.set_xscale(&#39;log&#39;)\\\\n    plt.show()\\\\n&quot;,&quot;solution&quot;,&quot;# Import necessary modules\\\\nfrom sklearn.linear_model import Ridge\\\\nfrom sklearn.model_selection import cross_val_score\\\\n\\\\n# Setup the array of alphas and lists to store scores\\\\nalpha_space = np.logspace(-4, 0, 50)\\\\nridge_scores = []\\\\nridge_scores_std = []\\\\n\\\\n# Create a ridge regressor: ridge\\\\nridge = Ridge(normalize=True)\\\\n\\\\n# Compute scores over range of alphas\\\\nfor alpha in alpha_space:\\\\n\\\\n    # Specify the alpha value to use: ridge.alpha\\\\n    ridge.alpha = alpha\\\\n    \\\\n    # Perform 10-fold CV: ridge_cv_scores\\\\n    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\\\\n    \\\\n    # Append the mean of ridge_cv_scores to ridge_scores\\\\n    ridge_scores.append(np.mean(ridge_cv_scores))\\\\n    \\\\n    # Append the std of ridge_cv_scores to ridge_scores_std\\\\n    ridge_scores_std.append(np.std(ridge_cv_scores))\\\\n\\\\n# Display the plot\\\\ndisplay_plot(ridge_scores, ridge_scores_std)\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,69002]]]]]],&quot;onboardingSettings&quot;,[&quot;^0&quot;,[&quot;numberOfCompletedExercises&quot;,0,&quot;showScreen&quot;,true,&quot;activated&quot;,null]]]]\";</script><div id=\"root\"><div class=\"theme progress-indicator--visible theme--light\" data-reactroot=\"\"><header class=\"dc-header-campus\" data-cy=\"header-container\"><a class=\"dc-header-campus__home\" data-cy=\"header-logo\" href=\"https://www.datacamp.com\"><img alt=\"datacamp-logo\" src=\"/static/media/logo-full-color.018b48cc.svg\" style=\"max-height:29px\"/></a><div class=\"dc-nav-course__container\"><div class=\"__react_component_tooltip place-top type-dark \" data-id=\"tooltip\"></div><div class=\"__react_component_tooltip place-top type-dark \" data-id=\"tooltip\"></div><div class=\"__react_component_tooltip place-top type-dark \" data-id=\"tooltip\"></div><nav class=\"dc-nav-course\" data-onboarding=\"course-navigation\"><a class=\"dc-nav-course__backward\" data-cy=\"header-previous\" data-for=\"nav-tp-prev\" data-tip=\"true\" href=\"/courses/supervised-learning-with-scikit-learn/regression-2?ex=7\"><svg aria-label=\"arrow_2_left icon\" class=\"dc-icon-arrow_2_left dc-nav-course__icon\" fill=\"currentColor\" height=\"12\" role=\"Img\" width=\"12\"><use xlink:href=\"/static/media/symbols.cace91b1.svg#arrow_2_left\"></use></svg></a><a class=\"dc-nav-course__outline\" data-cy=\"header-outline\" data-for=\"nav-tp-outline\" data-tip=\"true\" href=\"javascript:void(0)\"><svg aria-label=\"bars icon\" class=\"dc-icon-bars dc-nav-course__icon\" fill=\"currentColor\" height=\"12\" role=\"Img\" width=\"12\"><use xlink:href=\"/static/media/symbols.cace91b1.svg#bars\"></use></svg>Course Outline</a><a class=\"dc-nav-course__forward\" data-cy=\"header-next\" data-for=\"nav-tp-next\" data-tip=\"true\" href=\"/courses/supervised-learning-with-scikit-learn/regression-2?ex=9\"><svg aria-label=\"arrow_2_right icon\" class=\"dc-icon-arrow_2_right dc-nav-course__icon\" fill=\"currentColor\" height=\"12\" role=\"Img\" width=\"12\"><use xlink:href=\"/static/media/symbols.cace91b1.svg#arrow_2_right\"></use></svg></a></nav></div><nav class=\"dc-u-fx dc-u-fx-ais dc-u-fx-jcfe dc-u-w-96\"><div data-for=\"tp-notifications\" data-tip=\"true\"><div class=\"__react_component_tooltip place-top type-dark \" data-id=\"tooltip\"></div><a class=\"ds-icon-action dc-u-fx\" data-cy=\"header-notifications\" href=\"javascript:void(0)\"><svg aria-label=\"notification icon\" class=\"dc-icon-notification\" fill=\"currentColor\" height=\"18\" role=\"Img\" width=\"18\"><use xlink:href=\"/static/media/symbols.cace91b1.svg#notification\"></use></svg></a></div><a class=\"ds-icon-action dc-u-fx dc-u-ml-8\" data-cy=\"header-slides\" href=\"javascript:void(0)\"><div data-for=\"tp-slides\" data-tip=\"true\"><div class=\"__react_component_tooltip place-top type-dark \" data-id=\"tooltip\"></div><svg aria-label=\"pdf icon\" class=\"dc-icon-pdf dc-u-fx\" fill=\"currentColor\" height=\"18\" role=\"Img\" width=\"18\"><use xlink:href=\"/static/media/symbols.cace91b1.svg#pdf\"></use></svg></div></a><a class=\"ds-icon-action dc-u-fx dc-u-ml-8\" data-cy=\"header-issue\" data-test-id=\"header-report-issue-button\" href=\"javascript:void(0)\"><div data-for=\"tp-issue\" data-tip=\"true\"><div class=\"__react_component_tooltip place-top type-dark \" data-id=\"tooltip\"></div><svg aria-label=\"attention icon\" class=\"dc-icon-attention dc-u-fx\" fill=\"currentColor\" height=\"18\" role=\"Img\" width=\"18\"><use xlink:href=\"/static/media/symbols.cace91b1.svg#attention\"></use></svg></div></a></nav></header><div class=\"exercise-area \"><div><div class=\"vex-scroller\" data-cy=\"video-exercise-container\"><section class=\"vex-body\"><header class=\"dc-u-mb-16\"><div class=\"dc-u-fx dc-u-fx-jcsb\"><div><h1 class=\"dc-u-fs-h5\">Cross-validation</h1></div><div class=\"dc-u-fx dc-u-fx-aic\"><div class=\"dc-tag dc-tag--hue dc-tag--white\">50<!-- --> XP</div></div></div></header><div class=\"dc-u-pos-relative dc-u-wh-0 dc-u-brad-all dc-u-bs-md dc-u-of-hidden\" style=\"padding-bottom:56.25%\"><iframe allowfullscreen=\"\" class=\"dc-u-stretch dc-u-b-none\" style=\"width:100%;height:100%\" title=\"projector video\"></iframe></div><p></p><div class=\"dc-u-fx dc-u-fx-jcfe\"><button aria-label=\"button\" class=\"dc-btn dc-btn--secondary dc-btn--sm\" data-cy=\"submit-button\" type=\"submit\">Got it!</button></div></section></div></div><div class=\"Toastify\"></div></div><div class=\"exercise-footer\"><ul class=\"dc-progress-indicator\" data-cy=\"progress-container\"><li class=\"dc-progress-indicator__item\"><a class=\"dc-progress-indicator__bar\" href=\"javascript:void(0)\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a class=\"dc-progress-indicator__bar\" href=\"javascript:void(0)\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a class=\"dc-progress-indicator__bar\" href=\"javascript:void(0)\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a class=\"dc-progress-indicator__bar\" href=\"javascript:void(0)\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li></ul></div></div></div><script type=\"text/x-mathjax-config\">MathJax && MathJax.Hub && MathJax.Hub.Config && MathJax.Hub.Config({\n",
      "        messageStyle: \"none\"\n",
      "      });</script><script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script><script src=\"/static/js/main.b58f9274.js\" type=\"text/javascript\"></script></body></html>\n",
      "<class 'bs4.element.ResultSet'>\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmatch = soup.findAll(\"video\")\\nif len(match) > 0:\\n    for m in match:\\n        imagelist.append(str(m))\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML\n",
    "from lxml import html\n",
    "from requests import get\n",
    "\n",
    "# specify the url\n",
    "# https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/regression-2?ex=8\n",
    "quote_page = 'https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/regression-2?ex=8'\n",
    "\n",
    "# query the website and return the html to the variable ‘page’\n",
    "page = urlopen(quote_page)\n",
    "# tree = html.fromstring(page.content)\n",
    "\n",
    "# parse the html using beautiful soup and store in variable `soup`\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "print(soup)\n",
    "response = get(quote_page)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# type(html_soup) https://www.dataquest.io/blog/web-scraping-beautifulsoup/\n",
    "\n",
    "# Take out the <div> of name and get its value\n",
    "movie_containers = html_soup.find_all('div', class_ = 'video__container')\n",
    "print(type(movie_containers))\n",
    "print(len(movie_containers))\n",
    "\n",
    "name_box = html_soup.find('div', attrs={'class': 'video__container'})\n",
    "video = soup.find_all('body')\n",
    "'''\n",
    "match = soup.findAll(\"video\")\n",
    "if len(match) > 0:\n",
    "    for m in match:\n",
    "        imagelist.append(str(m))\n",
    "'''        \n",
    "        \n",
    "# Youtube\n",
    "\n",
    "# HTML('<iframe width=\"560\" height=\"315\" src=\"https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/regression-2?ex=8?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')\n",
    "# display(HTML('<iframe width=\"560\" height=\"315\" src=\"{name}?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>'.format(name=soup)))\n",
    "\n",
    "\n",
    "# display(HTML(\"<a href='{name}.html' target='_blank'> {name}.html </a>\".format(name=soup)))\n",
    "# iframe = '<iframe src=' + soup + ' width=700 height=350></iframe>'\n",
    "# IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
